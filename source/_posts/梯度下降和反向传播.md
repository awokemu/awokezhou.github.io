---
title: 梯度下降和反向传播
date: 2019-09-30 14:37:52
tags: [Deep Learning]
categories:
- ML
- 理论
- DeepLearning
mathjax: true
---

梯度下降和反向传播是理解神经网络学习原理的核心，本文试图从数学原理、实例来分析梯度下降和反向传播在神经网络中是如何工作的

## 数学原理

为了理解梯度下降算法和反向传播算法的工作原理，需要理解以下数学知识
* 方向导数和梯度
* 链式法则

### 方向导数
梯度在数学中的概念，源于方向导数。在一元函数中，我们用导数来描述和分析函数的变化趋势，二元函数或多元函数我们用偏导数来分析函数变化情况。但是一旦变成多元，偏导数只能反应在坐标轴方向的变化情况，如果想要知道函数在任意方向上的变化情况，偏导数是无法做到的。因此引入了方向导数的概念

$l$ 是 $xOy$ 平面上以 $P_{0}(x_{0},y_{0})$ 为起点的一条射线，$e_{l}=(\cos{\alpha},\cos{\beta})$ 是与 $l$ 同方向的单位向量，$P$ 为射线上另一点。如果函数 $z = f(x,y)$ 的增量 $f(x_{0}+t\cos{\alpha}, y_{0}+t\cos{\beta})$ 与 $P$ 到 $P_{0}$ 的距离 $\lvert{PP_{0}}\rVert = t$ 比值 
$$
\frac{f(x_{0}+t\cos{\alpha}, y_{0}+t\cos{\beta}) - f(x_{0}, y_{0})}{t}
$$

当 $P$ 趋于 $P_{0}$ 时的极限存在，则称此极限为函数在点 $P_{0}$ 沿着射线 $l$ 的方向导数。简单来说，方向导数就是函数在某一点上(当然这一点必须是在函数上的)，沿着某一个方向的变化率，但是方向导数并不是在任何方向上都存在，有定理：
* 如果函数在某一点可微，函数在该点的任何方向的方向导数存在 

### 梯度
函数在很多方向上都有方向导数，那么在哪个方向上变化率最大呢？

如果函数在点 $P_{0}$ 可微，$e_{l} = (cos\alpha, cos\beta)$ 是与方向 $l$ 同方向的单位向量，则方向导数可以写成如下形式

$$
\frac{\partial f}{\partial l} = \nabla f\left ( x_{0}, y_{0} \right ) \cdot e_{l} = \left | \nabla f\left ( x_{0}, y_{0} \right ) \right |cos\theta
$$

其中 $\nabla f\left ( x_{0},y_{0} \right )$ 就是梯度，$\theta$ 是梯度与单位向量的夹角。反过来，可以将方向导数看做梯度在任意方向上的投影，当这个夹角 $\theta$ 为0时，表示方向导数与梯度同方向，函数增长速度最快，当夹角 $\theta$ 为180度时，方向导数与梯度方向相反，函数减小最快 

梯度的表达式为

$$
\nabla{f} = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}]
$$

本质上是函数的各个偏导数合成的向量

### 链式法则
用如下函数来说明

$$
f(x,y,z) = (x+y)z
$$

我们用以下函数来变换以下形式

$$
\begin{align}
q &= x + y\\
f &= qz
\end{align}
$$

对函数 $f$ 求导可用链式法则表示为

$$
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x}
$$

## 实例

## 参考
* [知乎-梯度的方向为什么是函数值增加最快的方向？](https://zhuanlan.zhihu.com/p/38525412)
* [知乎-如何直观形象的理解方向导数与梯度以及它们之间的关系？](https://www.zhihu.com/question/36301367/answer/156102040)
* [博客园-刘建平Pinard-梯度下降（Gradient Descent）小结](https://www.cnblogs.com/pinard/p/5970503.html)
* [cs231n-Backpropagation](http://cs231n.github.io/optimization-2/#backprop)