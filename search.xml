<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SharedMemocry优化矩阵乘法]]></title>
    <url>%2F2023%2F01%2F16%2FSharedMemocry%E4%BC%98%E5%8C%96%E7%9F%A9%E9%98%B5%E4%B9%98%E6%B3%95%2F</url>
    <content type="text"><![CDATA[巧妙的使用共享内存Shared memory，能够减少线程对全局内存Global memory的访问，提升CUDA程序在访存方面的性能。本文以矩阵乘法为例，通过对比不使用共享内存的普通矩阵乘法实现和使用共享内存的矩阵乘法优化版本，展示共享内存对程序性能的提升，并分析使用共享内存的条件和注意点 普通矩阵乘法矩阵乘法的基本原理如下图所示 矩阵A与矩阵B相乘，得到矩阵C，相乘的一个重要条件是矩阵A的列数或者宽度与矩阵B的行数或者高度要相等，相乘得到的矩阵C的列数(宽度)与矩阵B的列数(宽度)相等，行数(高度)与矩阵A的行数(高度)相等，假设A为2x3的矩阵，B为3x4的矩阵，A和B相乘结果是C为2x4，注意这里的表述是行在前列在后。乘法中单个元素的对应关系是，C中的每个元素是A中对应行与B中对应列的逐元素相乘的累加和，如图中黄色部分，整个乘法过程可以抽象成一个两层循环，外层循环是C中每个元素位置的循环，内层循环是A的行与B的列的逐元素循环 普通矩阵乘法的示例代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100#include "cuda_runtime.h"#include "device_launch_parameters.h"#include &lt;iostream&gt;#include &lt;memory&gt;#include &lt;string&gt;using namespace std;#define MATRIX_A_WIDTH 1024#define MATRIX_A_HEIGHT 1920#define MATRIX_B_WIDTH 1280#define MATRIX_B_HEIGHT MATRIX_A_WIDTH#define BLOCK_SIZE 16#define MAT_ELEM_NUM(m) (m.width * m.height)#define MAT_SIZE(m) (MAT_ELEM_NUM(m) * sizeof(float))typedef struct &#123; int width; int height; float *data;&#125; Matrix;__global__ void mat_mul_kernel_v1(Matrix A, Matrix B, Matrix C)&#123; /* get row/col index */ int row = blockDim.x * blockIdx.x + threadIdx.x; int col = blockDim.y + blockIdx.y + threadIdx.y; float value = 0.0; /* foreach element in A's row and B's col */ for (int i=0; i&lt;A.width; i++) &#123; value += A.data[row * A.width + i] * B.data[i * B.height + col]; &#125; /* write sum value to C */ c.data[row * C.width + col] = value;&#125;static void MatMulKernel(Matrix A, Matrix B, Matrix C)&#123; Matrix d_A, d_B, d_C; d_A.width = A.width; d_A.height = A.height; d_B.width = B.width; d_B.height = B.height; d_C.width = C.width; d_C.height = C.height; /* Alloc cuda mem for A, B, C */ cudaMalloc(&amp;d_A.data, MAT_SIZE(A)); cudaMalloc(&amp;d_B.data, MAT_SIZE(B)); cudaMalloc(&amp;d_C.data, MAT_SIZE(C)); /* Copy mem from host to device */ cudaMemcpy(d_A.data, A.data, MAT_SIZE(A), cudaMemcpyHostToDevice); cudaMemcpy(d_B.data, B.data, MAT_SIZE(B), cudaMemcpyHostToDevice); /* Create and invoke cuda kernel */ dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE); dim3 numBlocks(C.height / BLOCK_SIZE, C.width / BLOCK_SIZE); printf("call kernel with blocks&#123;%d,%d&#125; threads&#123;%d,%d&#125;\n", B.width / BLOCK_SIZE, A.height / BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE); mat_mul_kernel_v1&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C); /* Copy mem from device to host*/ cudaMemcpy(C.data, d_C.data, MAT_SIZE(C), cudaMemcpyDeviceToHost); /* Free cuda mem */ cudaFree(d_A.data); cudaFree(d_B.data); cudaFree(d_C.data);&#125;int main(int argc, char *argv)&#123; Matrix A, B, C; /* Initialize A,B,C and alloc memory for them */ A.width = MATRIX_A_WIDTH; A.height = MATRIX_A_HEIGHT; A.data = (float *)malloc(MAT_SIZE(A)); B.width = MATRIX_B_WIDTH; B.height = MATRIX_B_HEIGHT; B.data = (float*)malloc(MAT_SIZE(B)); C.width = MATRIX_B_WIDTH; C.height = MATRIX_A_HEIGHT; C.data = (float*)malloc(MAT_SIZE(C)); MatMulKernel(A, B, C); free(A.data); free(B.data); free(C.data); exit(EXIT_SUCCESS);&#125; 程序中为每个block分配了16x16的二维线程数，block的大小由数据大小决定，每个线程计算C中的一个元素，通过线程的二维ID来确定当前线程计算C中的哪个元素 使用共享内存优化矩阵乘法在普通矩阵乘法中，每个线程负责计算C中的一个元素，每个元素都会读取A中的一行和B中的一列。例如在计算C[2][1]时，线程1要读取一次A的第2行和B的第1列，在计算C[2][2]时，线程2要读取一次A的第2行和B的第2列，可以看到两个线程各进行了一次对A第二行的读取操作，由于从全局内存中读取数据是非常缓慢的，这种重复的读取如果可以被避免的话，能够有效提升程序性能。如下图所示，在优化的程序中，将最终要计算的结果矩阵C拆分成一个一个大小为 BLOCK_SIZE*BLOCK_SIZE的子矩阵$C{sub}$，每个线程块负责计算一个子矩阵$C{sub}$，而每个线程负责计算子矩阵中的每个元素。对A、B的读取也是以块为单位的，每次将读取的数据放入SharedMemory中，这样在计算完一个子矩阵$C_{sub}$时，对A的行读取、B的列读取能够从SharedMemory中获得，而不是每次都从全局内存中获取 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146#include "cuda_runtime.h"#include "device_launch_parameters.h"#include &lt;iostream&gt;#include &lt;memory&gt;#include &lt;string&gt;using namespace std;#define MATRIX_A_WIDTH 1024#define MATRIX_A_HEIGHT 1920#define MATRIX_B_WIDTH 1280#define MATRIX_B_HEIGHT MATRIX_A_WIDTH#define BLOCK_SIZE 16#define MAT_ELEM_NUM(m) (m.width * m.height)#define MAT_SIZE(m) (MAT_ELEM_NUM(m) * sizeof(float))typedef struct &#123; int width; int height; int stride; float *data;&#125; Matrix;__device__ void DevMatSetElement(Matrix m, int row, int col, float value)&#123; m.data[m.stride * row + col] = value;&#125;__device__ float DevMatGetElement(Matrix m, int row, int col)&#123; return m.data[m.stride * row + col];&#125;__device__ Matrix DevMatGetSub(Matrix m, int row, int col)&#123; Matrix sub; sub.width = BLOCK_SIZE; sub.height = BLOCK_SIZE; sub.stride = m.stride; sub.data = &amp;m.data[m.stride * BLOCK_SIZE * row + BLOCK_SIZE * col]; return sub;&#125;__global__ void mat_mul_kernel_v2(Matrix A, Matrix B, Matrix C)&#123; int blockRow = blockIdx.y; int blockCol = blockIdx.x; int row = threadIdx.y; int col = threadIdx.x; Matrix CSub = DevMatGetSub(C, blockRow, blockCol); float Cvalue = 0.0; /* foreach sub */ for (int m=0; m&lt;(A.width/BLOCK_SIZE); m++) &#123; Matrix ASub = DevMatGetSub(A, blockRow, m); Matrix BSub = DevMatGetSub(B, m, blockCol); __shared__ float As[BLOCK_SIZE][BLOCK_SIZE]; __shared__ float Bs[BLOCK_SIZE][BLOCK_SIZE]; As[row][col] = DevMatGetElement(ASub, row, col); Bs[row][col] = DevMatGetElement(BSub, row, col); __syncthreads(); /* foreach elem */ for (int e=0; e&lt;BLOCK_SIZE; e++) &#123; Cvalue += As[row][e] * Bs[e][col]; &#125; __syncthreads(); &#125; DevMatSetElement(CSub, row, col, Cvalue);&#125;static void MatMulKernel(Matrix A, Matrix B, Matrix C)&#123; Matrix d_A, d_B, d_C; d_A.width = A.width; d_A.height = A.height; d_A.stride = A.stride; d_B.width = B.width; d_B.height = B.height; d_B.stride = B.stride; d_C.width = C.width; d_C.height = C.height; d_C.stride = C.stride; /* Alloc cuda mem for A, B, C */ cudaMalloc(&amp;d_A.data, MAT_SIZE(A)); cudaMalloc(&amp;d_B.data, MAT_SIZE(B)); cudaMalloc(&amp;d_C.data, MAT_SIZE(C)); /* Copy mem from host to device */ cudaMemcpy(d_A.data, A.data, MAT_SIZE(A), cudaMemcpyHostToDevice); cudaMemcpy(d_B.data, B.data, MAT_SIZE(B), cudaMemcpyHostToDevice); /* Create and invoke cuda kernel */ dim3 threadsPerBlock(BLOCK_SIZE, BLOCK_SIZE); dim3 numBlocks(C.height / BLOCK_SIZE, C.width / BLOCK_SIZE); printf("call kernel with blocks&#123;%d,%d&#125; threads&#123;%d,%d&#125;\n", B.width / BLOCK_SIZE, A.height / BLOCK_SIZE, BLOCK_SIZE, BLOCK_SIZE); mat_mul_kernel_v2&lt;&lt;&lt;numBlocks, threadsPerBlock&gt;&gt;&gt;(d_A, d_B, d_C); /* Copy mem from device to host*/ cudaMemcpy(C.data, d_C.data, MAT_SIZE(C), cudaMemcpyDeviceToHost); /* Free cuda mem */ cudaFree(d_A.data); cudaFree(d_B.data); cudaFree(d_C.data);&#125;int main(int argc, char *argv)&#123; Matrix A, B, C; /* Initialize A,B,C and alloc memory for them */ A.width = MATRIX_A_WIDTH; A.height = MATRIX_A_HEIGHT; A.stride = A.width; A.data = (float *)malloc(MAT_SIZE(A)); B.width = MATRIX_B_WIDTH; B.height = MATRIX_B_HEIGHT; B.stride = B.width; B.data = (float*)malloc(MAT_SIZE(B)); C.width = MATRIX_B_WIDTH; C.height = MATRIX_A_HEIGHT; C.stride = C.width; C.data = (float*)malloc(MAT_SIZE(C)); MatMulKernel(A, B, C); free(A.data); free(B.data); free(C.data); exit(EXIT_SUCCESS);&#125; Performance 显卡 KernelVersion KernelLaunchTime(us) GTX 730(sm_35) V1 2849372.820 GTX 730(sm_35) v2 2154518.019]]></content>
      <categories>
        <category>CUDA</category>
      </categories>
      <tags>
        <tag>Nvidia</tag>
        <tag>CUDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT部署YOLOv5-03-TensorRT]]></title>
    <url>%2F2023%2F01%2F03%2FTensorRT%E9%83%A8%E7%BD%B2YOLOv5-03-TensorRT%2F</url>
    <content type="text"><![CDATA[TensorRT是本专栏中最重要的内容，绝大多数内容将围绕TensorRT来展开，本文对TensorRT进行一个基本的介绍，让不熟悉TensorRT的读者能够对TensorRT是什么，如何使用它有一个较为全面的认识 Nvidia TensorRT是一个用于Nvidia GPU上高性能机器学习推理的SDK，对开发者屏蔽了模型在GPU上推理运行的CUDA计算细节，用户只需要通过一套简介易用的Python/C++ API接口，即可方便的将模型在GPU上进行加速推理。另外TensorRT还提供了模型转换、性能评估的工具，方便用户将各训练框架生成的模型转换到TensorRT能够识别的形式，以及在未进行业务开发之前快速评估模型推理的性能 Build Phase and Runtime PhaseTensorRT的主要工作内容可分为构建期(Build Phase)和运行期(Runtime Phase) 构建期的目标是生成一个能够被运行期加载并运行的TensorRT推理引擎(Engine)，模型的来源可以是从各种深度学习框架(TensorFlow、Pytorch、Caffe等)训练好的模型文件，也可以是利用TensorRT API原生搭建的网络，从深度学习框架导出的权重文件中加载权重参数。推理引擎是构建期的输出，推理引擎可以以本地二进制文件的形式存在，也可以是程序中的一个类实例。构建期在引擎的创建过程中，除了进行模型结构的解析以及生成引擎之外，还有很多中间的优化工作，例如计算图优化，经典的操作Conv+Add+ReLU层融合，节点消除，节点变换(Pad、Slice、Concat、Shuffle)，并对算子在GPU上的实现进行本地运行评估和选择，所有这些优化都是工具自动进行的，但是用户可以通过一些额外的参数来调整优化过程。经过构建期生成的推理引擎，其内部的网络结构和原模型已经完全不同 运行期的作用是加载推理引擎，并在GPU上进行执行，在这个阶段，用户需要为引擎提供输入数据，并准备好输出内存，通过TensorRT提供的运行期API进行模型执行 构建期和运行期可以处于同一个程序中，也可以分开独立进行。例如可以在同一个程序中，先进行构建期将模型转换为推理引擎，然后进行执行期将推理引擎的类实例直接运行推理计算；也可以在一个单独的构建程序中进行模型转换，生成推理引擎，通过API导出为序列化的引擎文件，然后在另一个推理程序中加载引擎文件，生成引擎类实例，进行推理计算 Workflow构建一个推理引擎有3种方式，这些方式各有优劣 框架自带TRT接口 英伟达与部分深度学习框架有合作，例如Tensorflow内置了TF-TRT，Pytorch有Torch-TensorRT，使用这些框架内置的TensorRT接口，可以将训练好的模型无缝衔接，直接在原有框架上调用对应接口进行推理，这种方式的优势在于非常方便，环境统一，开发效率较高，遇到不支持的算子会返回到原框架的实现。缺点是性能较差，不能最大限度的利用TensorRT的优化加速能力，另外在很多资源有限的嵌入式设备上安装Tensorflow这种较大的深度学习工具也不现实，因此这种方式通常是应用于服务端推理，本文不对这种方式进行介绍 使用Parser 将深度学习框架导出的模型文件，经过一个中间表示，转换到TensorRT引擎，中间表示主要是使用ONNX，这种方式的优点是TensorRT能够在构建期尽可能多的操作网络结构和算子优化，因此推理性能较高。缺点也是非常明显的，由于使用了ONNX，原框架中支持的算子与ONNX支持的算子以及TensorRT支持的算子，这三者之间并不是完全覆盖的，在网络模型使用了较多较新层结构时，在算子支持方面可能会存在较多问题，并且由于ONNX本身对不支持的算子也会进行模型结构的变换，这部分并不可控，对性能也有一些损失，这种情况下要么修改网络结构适配ONNX，要么以插件的形式通过TensorRT提供的API手写自定义算子，比较复杂 TensorRT API搭建 TensorRT API本身提供了构建网络结构的API，这种方式的优点是网络细节完全由用户控制，某些情况下TensorRT转换出来的网络结构可能并非最佳方案，手工设置的网络结构性能更佳，这种情况下由用户自己搭建TensorRT的原生网络性能能够达到最大化，但是由于TensorRT API搭建网络也存在算子支持的问题，并且从网络整体结构层面进行性能优化本身是一个难度很高的事情，需要用户对计算图优化有较为深入的理解，因此这种方式开发难度最大 总的来说，无论是哪种方式，特殊算子适配是一个绕不开的问题，必须研究插件写法以及CUDA计算细节。本文由于重点是介绍TensorRT上运行YOLOv5模型的全流程，重点将会放在整个流程的完整性上，且YOLOv5模型本身使用到的层和算子比较常规，不存在算子适配问题，因此后文主要以“使用Parser”方式进行介绍，对插件写法和TensorRT API搭建网络等方面不进行介绍]]></content>
      <categories>
        <category>AI</category>
        <category>Nvidia</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>nvidia</tag>
        <tag>TensorRT</tag>
        <tag>YOLOv5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT部署YOLOv5-02-环境介绍]]></title>
    <url>%2F2022%2F12%2F19%2FTensorRT%E9%83%A8%E7%BD%B2YOLOv5-02-%E7%8E%AF%E5%A2%83%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[本文对TensorRT部署YOLOv5模型的整体环境配置及软件包进行介绍。实验环境主要从主机和JestonNano两方面进行介绍，在主机端完成模型训练并转换为onnx中间模型表示，在JestonNano进行onnx模型转换为TensorRT引擎、图片/视频加载、编解码处理、模型推理、后处理等工作 主机环境主机是一台Windows11的台式机，使用Tensorflow的GPU版进行模型训练，生成模型文件，由于在windows操作系统上安装onnx存在一些问题，比较麻烦，不想折腾，因此我选择在Ubuntu虚拟机上进行Tensorflow模型到onnx模型的转换 主机端主要使用的软件及版本如下 Windows11 tensorflow-gpu 2.5.0 CUDA 11.0 Ubuntu20.0.4虚拟机 tensorflow-gpu 2.2.0：没啥用处，主要是为了安装tf2onnx tf2onnx 1.12.0：用于将tensorflow模型转换为onnx sdkmanager 1.8.1：Nvidia官方提供的镜像及软件包下载烧写工具，用于向JestonNano烧写Linux镜像和软件包 JestonNano环境JestonNano环境的配置主要包括两方面，一方面是通过sdkmanager烧写的官方镜像所携带的软件包以及官方额外提供的软件包，另一方面是自己下载并安装到JestonNano的第三方软件和库 官方提供，列举一些常用到的 bin trtexec：TensorRT的命令行工具，可以进行推理引擎生成及性能评估 nsys：CUDA性能分析工具，生成Profile文件 python包 numpy：张量计算，前后处理都会用到 pycuda：与nvinfer配合进行数据的拷贝(devToHost/hostToDev)，以及部分计算加速 opencv：图像预处理、图像视频加载及显示 nvinfer：TensorRT的Python包，可以进行推理引擎生成以及推理计算 C++ cmake：构建C++程序 opencv：图像预处理、图像视频加载及显示 nvinfer：TensorRT C++库 私有安装 Python numba：张量计算加速，较难安装 cupy：张量计算加速，容易安装 lbtorch pytorch的C++库，用于替代numpy，处理C++程序的张量计算]]></content>
      <categories>
        <category>AI</category>
        <category>Nvidia</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>nvidia</tag>
        <tag>TensorRT</tag>
        <tag>YOLOv5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TensorRT部署YOLOv5-01-Overview]]></title>
    <url>%2F2022%2F12%2F14%2FTensorRT%E9%83%A8%E7%BD%B2YOLOv5-01-Overview%2F</url>
    <content type="text"><![CDATA[本系列对在Nvidia边缘计算平台进行深度学习模型部署进行一个全面的介绍，主要围绕TensorRT深度学习推理框架，以YOLOv5目标检测任务为例，以Jeston Nano为目标计算平台，对环境搭建、模型量化、模型推理、性能评估、后处理优化等细节进行详细说明，并给出C++和Python分别进行模型部署推理的代码实例 阅读本系列，您可以了解到 Jeston Nano计算平台以及相关软件资源介绍 什么是TensorRT？什么是TensorRT引擎？如何将Tensorflow等深度学习框架训练好的模型转换为TensorRT引擎？转换过程需要注意什么？TensorRT对模型做了哪些优化？ 如何使用TensorRT的Python/C++ API进行模型推理？如何构造输入数据并传递给推理引擎？输出数据又是什么格式，如何获取模型输出结果？ 如何利用Nsight对TensoRT推理过程进行抓取，推理过程内部细节和耗时分布是什么样的？ YOLOv5这种需要前处理和后处理解码的任务如何在TensorRT上进行推理？如何使用Python/C++进行图像/视频预处理、模型推理、模型解码、非极大值抑制，并最终在输出图像上绘制预测框？ 如何使用Gstreamer并利用Nvidia提供的加速插件，对视频源(摄像头/文件)进行编解码及格式转换的加速，提高视频编解码速度 Python版本视频推理性能瓶颈分析，如何提速？利用pycuda、cupy、numba等加速包进行加速，可行吗？ C++版本如何利用libtorch进行后处理解码 影响性能的因素 Performance目前最快的方案是使用Python程序，利用pycuda进行sigmoid运算加速，fp16量化，yolov5n模型，最快fps为11 以下表格是目前不同模型规模、不同精度和不同类别数量情况下，推理引擎大小、吞吐量、延迟和进行视频检测的帧率，重点关注帧率 Model Classes Params EngineSize Precision Throughput(qps) Latency(ms) Program FPS YOLOv5m 20 146M fp32 3.5738 279.722 Python 2.77 YOLOv5m 20 70M fp16 Python YOLOv5s 20 44M fp32 8.67145 115.22 Python 5.21 YOLOv5s 20 20M fp16 72.2289 72.2289 Python 6.66 YOLOv5n 20 1800481 13M fp32 21.8709 45.4301 Python 9.85 YOLOv5n 20 1800481 7M fp16 28.4845 35.0919 Python+CUDA 10.88 YOLOv5n 80 1881661 14M fp32 19.8624 50.3083 Python 5.62 YOLOv5n 80 1881661 7M fp16 26.0428 38.2188 Python 5.85 YOLOv5n 20 1800481 7M fp16 28.4845 35.0919 C++&amp;libtorch 8.9 Model：表示YOLOv5的模型规模 Classes：模型支持的分类类别数，训练时确定 Params：模型在Tensorflow summary中输出的总参数数量 EngineSize：模型转换为TensorRT引擎后的引擎文件大小 Precision：模型转换为TensorRT引擎的转换精度 Throughput：模型转换时TensorRT评估推理吞吐量，每秒可执行的推理次数 Latency：模型转换时TensorRT评估推理延迟，即推理一次需要的时长 Program：分别用Python实现还是C++实现的检测程序 FPS：检测程序统计的平均帧率]]></content>
      <categories>
        <category>AI</category>
        <category>Nvidia</category>
      </categories>
      <tags>
        <tag>CUDA</tag>
        <tag>nvidia</tag>
        <tag>TensorRT</tag>
        <tag>YOLOv5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202211]]></title>
    <url>%2F2022%2F11%2F11%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202211%2F</url>
    <content type="text"><![CDATA[vscode注释插件Better Comments该插件能够帮助生成易于阅读的高亮形式代码注释 插件需要通过settings.json添加配置项，在settings.json中输入”better-comments.tags”，会自动生成默认配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546"better-comments.tags": [ &#123; "tag": "!", "color": "#FF2D00", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false &#125;, &#123; "tag": "?", "color": "#3498DB", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false &#125;, &#123; "tag": "//", "color": "#474747", "strikethrough": true, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false &#125;, &#123; "tag": "todo", "color": "#FF8C00", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false &#125;, &#123; "tag": "*", "color": "#98C379", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false &#125; 插件支持自定义关键字和颜色，例如我定义的自定义关键字 123456789101112131415161718192021222324252627 &#123; "tag": "&lt;todo&gt;", "color": "#FF8C00", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false&#125;,&#123; "tag": "&lt;test&gt;", "color": "#32CD32", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false&#125;,&#123; "tag": "&lt;note&gt;", "color": "#007FFF", "strikethrough": false, "underline": false, "backgroundColor": "transparent", "bold": false, "italic": false&#125; tag就是匹配的关键字 color字段填写你想要的颜色编码 strikethrough是否用删除线 underline 是否用下划线 backgroundColor 背景色 bold 是否加粗 italic 是否为斜体 在C代码中显示的效果如下 koroFileHeader该插件用于自动生成文件头部注释和函数注释 详细配置参见 koro1FileHeade wiki 主要功能 文件头部注释 自动更新创建时间和最后编辑时间 配置好模板后，通过快捷键自动添加 支持自定义属性 函数注释 支持自定义属性 Z-Library没了怎么办环球最大的数字图书馆Z-Library因版权问题被美国邮政检查局查封，所有DNS服务器全部封禁 在Z-Library上下载了好多电子书，没有了Z-Library怎么办呢。我找到了一个Z-Library的替代方案，那就是Library Genesis 虽然没有Z-Library的图书那么全那么多，但是很多比较新的英文书籍也是可以找到的，例如最近正在研究的PyCUDA]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Mosica数据增强]]></title>
    <url>%2F2022%2F11%2F11%2FMosica%E6%95%B0%E6%8D%AE%E5%A2%9E%E5%BC%BA%2F</url>
    <content type="text"><![CDATA[Yolov4中使用了Mosica数据增强方法，能够在有限数据集情况下极大程度的增加增强样本量。本文对Mosica数据增强方法进行Python代码实现介绍 数据准备本实现需要以下相关数据和文件准备 VOC数据集(本实验使用的是VOC2007) classes文件 将VOC数据集预先经过处理后，得到一个合并所有标注信息的文件’annotations.txt’，其中每一行是图像绝对路径和BoundingBox和标签信息 1234567VOCdevkit/VOC2007/JPEGImages/000005.jpg 263,211,324,339,8 165,264,253,372,8 5,244,67,374,8 241,194,295,299,8 277,186,312,220,8VOCdevkit/VOC2007/JPEGImages/000007.jpg 141,50,500,330,6VOCdevkit/VOC2007/JPEGImages/000009.jpg 69,172,270,330,12 150,141,229,284,14 285,201,327,331,14 258,198,297,329,14VOCdevkit/VOC2007/JPEGImages/000012.jpg 156,97,351,270,6VOCdevkit/VOC2007/JPEGImages/000016.jpg 92,72,305,473,1VOCdevkit/VOC2007/JPEGImages/000017.jpg 185,62,279,199,14 90,78,403,336,12... 导入依赖包1234import randomimport numpy as npfrom PIL import Image, ImageDraw, ImageFontimport matplotlib.pyplot as plt random用于生成随机数，以进行随机参数的变换 numpy用于图像和numpy的转换以及数据处理 Image用于图像加载和变换 ImageDraw和ImageFont用于在图像上画BoundingBox pyplot用于绘图显示图像 定义一些工具函数123# 用于生成指定范围的随机数def random_rate(a=0, b=1): return np.random.rand()*(b-a) + a 123456# 用于从classes文件得到classes数组def get_classes(classes_path): with open(classes_path, encoding='utf-8') as f: class_names = f.readlines() class_names = [c.strip() for c in class_names] return class_names 1234567891011121314# 在图像上绘制box矩形框以及标签文字def draw_bndbox(image, bboxes, classes): for box in bboxes: label = classes[int(box[-1])] font = ImageFont.truetype(font='SIMYOU.TTF', size=np.floor(1.5e-2 * np.shape(image)[1] + 15).astype('int32')) draw = ImageDraw.Draw(image) label_size = draw.textsize(label, font) text_origin = np.array([box[0], box[1] - label_size[1]]) draw.rectangle([box[0], box[1], box[2], box[3]], outline='red',width=3) draw.rectangle([tuple(text_origin), tuple(text_origin + label_size)], fill='red') draw.text(text_origin, str(label), fill=(255, 255, 255), font=font) del draw return image 图像样本显示12345678# 变量预先设置input_shape = [640, 640]max_boxes = 100annotation_path = 'annotations.txt'with open(annotation_path, encoding='utf-8') as f: train_indexes = f.readlines()classes_path = 'voc_classes.txt'classes = get_classes(classes_path) Mosica数据增强是以4副图像合并为一副图像，首先获取4副图像进行显示 123456789101112plt.figure(figsize=(16, 12))samples = train_indexes[:4]for i, sample in enumerate(samples): line_content = sample.split() bboxes = np.array([np.array(list(map(int,box.split(',')))) for box in line_content[1:]]) image = Image.open(line_content[0]) image = draw_bndbox(image, bboxes, classes) plt.subplot(2,2,i+1) plt.xticks([]) plt.yticks([]) plt.title('sample&#123;&#125;'.format(i)) plt.imshow(image) Mosica处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112index = 0image_datas = []boxes_datas = []# 生成4副图像的分割比例和坐标min_offset_x = random_rate(0.3, 0.7)min_offset_y = random_rate(0.3, 0.7)cutx = int(w * min_offset_x)cuty = int(h * min_offset_y)plt.figure(figsize=(16, 12))for sample in samples: # 读入图像和box line_content = sample.split() image = Image.open(line_content[0]) iw, ih = image.size box = np.array([np.array(list(map(int,box.split(',')))) for box in line_content[1:]]) # 随机翻转 flip = random_rate() &lt; 0.5 if flip and len(box)&gt;0: image = image.transpose(Image.FLIP_LEFT_RIGHT) # 图像进行翻转，box也需要翻转 box[:, [0,2]] = iw - box[:, [2,0]] # 图像长宽比例和大小随机变换 beta1 = random_rate(1-jitter, 1+jitter) beta2 = random_rate(1-jitter, 1+jitter) rate = iw/ih*beta1 / beta2 scale = random_rate(.4, 1) if rate &lt; 1: nh = int(scale*h) nw = int(nh*rate) else: nw = int(scale*w) nh = int(nw/rate) image = image.resize((nw, nh), Image.BICUBIC) # 计算每幅图像的左上角相对最后生成图的偏移 if index == 0: dx = cutx - nw dy = cut - nh elif index == 1: dx = icutx- nw dy = cutx elif index == 2: dx = cutx dy = cuty elif index == 3: dx = cutx dy = cuty - nh # 创建与最终合成图大小一致的灰色背景图，用于将变换图粘贴到其中 new_image = Image.new('RGB', (w,h), (128,128,128)) new_image.paste(image, (dx, dy)) image_data = np.array(new_image) bboxs_data = [] # 图像进行了长宽比例和大小随机变换，box也需要进行尺度变换，并限制边界 if len(box)&gt;0: np.random.shuffle(box) box[:, [0,2]] = box[:, [0,2]]*nw/iw + dx box[:, [1,3]] = box[:, [1,3]]*nh/ih + dy box[:, 0:2][box[:, 0:2]&lt;0] = 0 box[:, 2][box[:, 2]&gt;w] = w box[:, 3][box[:, 3]&gt;h] = h box_w = box[:, 2] - box[:, 0] box_h = box[:, 3] - box[:, 1] box = box[np.logical_and(box_w&gt;1, box_h&gt;1)] bboxs_data = np.zeros((len(box),5)) bboxs_data[:len(box)] = box plt.subplot(2,2,index+1) plt.xticks([]) plt.yticks([]) plt.title('sample&#123;&#125;'.format(index)) new_image = draw_bndbox(new_image, box, classes) plt.imshow(new_image) index = index + 1 image_datas.append(image_data) boxes_datas.append(bboxs_data)# 生成最终合成图，并将4副图像粘贴到对应位置new_image = np.zeros([h, w, 3])new_image[:cuty, :cutx, :] = image_datas[0][:cuty, :cutx, :]new_image[cuty:, :cutx, :] = image_datas[1][cuty:, :cutx, :]new_image[cuty:, cutx:, :] = image_datas[2][cuty:, cutx:, :]new_image[:cuty, cutx:, :] = image_datas[3][:cuty, cutx:, :]new_image = np.array(new_image, np.uint8)new_image = Image.fromarray(new_image)'''这里可以进行一些色域变换'''# box处理new_boxes = merge_bboxes(boxes_datas, cutx, cuty)box_data = np.zeros((max_boxes, 5))if len(new_boxes)&gt;0: if len(new_boxes)&gt;max_boxes: new_boxes = new_boxes[:max_boxes] box_data[:len(new_boxes)] = new_boxesplt.figure(figsize=(12, 8))plt.xticks([])plt.yticks([])plt.title('merge')new_image = draw_bndbox(new_image, new_boxes, classes)plt.imshow(new_image)plt.show() box合并处理的函数 1234567891011121314151617181920212223242526272829303132333435363738394041424344def merge_bboxes(bboxes, cutx, cuty): merge_bbox = [] for i in range(len(bboxes)): for box in bboxes[i]: tmp_box = [] x1, y1, x2, y2 = box[0], box[1], box[2], box[3] if i == 0: if y1 &gt; cuty or x1 &gt; cutx: continue if y2 &gt;= cuty and y1 &lt;= cuty: y2 = cuty if x2 &gt;= cutx and x1 &lt;= cutx: x2 = cutx if i == 1: if y2 &lt; cuty or x1 &gt; cutx: continue if y2 &gt;= cuty and y1 &lt;= cuty: y1 = cuty if x2 &gt;= cutx and x1 &lt;= cutx: x2 = cutx if i == 2: if y2 &lt; cuty or x2 &lt; cutx: continue if y2 &gt;= cuty and y1 &lt;= cuty: y1 = cuty if x2 &gt;= cutx and x1 &lt;= cutx: x1 = cutx if i == 3: if y1 &gt; cuty or x2 &lt; cutx: continue if y2 &gt;= cuty and y1 &lt;= cuty: y2 = cuty if x2 &gt;= cutx and x1 &lt;= cutx: x1 = cutx tmp_box.append(x1) tmp_box.append(y1) tmp_box.append(x2) tmp_box.append(y2) tmp_box.append(box[-1]) merge_bbox.append(tmp_box) return merge_bbox 将4副样本进行随机翻转、尺度变换后放置在背景图中的效果 最终合成图的效果]]></content>
      <categories>
        <category>AI</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-07-模型部署]]></title>
    <url>%2F2022%2F08%2F27%2FVitisAI-07-%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[本文以自定义模型为例，对使用VitisAI进行模型量化部署的流程进行介绍 Workflow 数据集为fashion_mnist 使用Tensorflow2搭建一个简单分类网络并进行训练，导出模型文件 使用VitsiAI docker中的vai_q_tensorflow2工具进行模型量化和校准，得到校准模型文件 使用VitisAI docker中的vai_c_tensorflow2工具进行模型编译，生成能够部署在DPU上的模型文件 编写模型推理程序(Python)，并将推理程序、编译后的模型文件以及测试图片导入设备中，运行推理程序进行图片分类 Trainkeras内置了fashion_mnist数据集，该数据集是小尺寸商品分类数据集，由28x28的单通道灰度图构成，训练集为60000张图片，测试集为10000张图片 导入包并加载数据集 123456789import cv2 as cvimport numpy as npfrom PIL import Imageimport tensorflow as tffrom tensorflow import kerasimport matplotlib.pyplot as pltfashion_mnist = keras.datasets.fashion_mnist(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() 查看数据集 12print(train_images.shape)print(test_images.shape) 12(60000, 28, 28)(10000, 28, 28) 保存一些测试数据为图片 12345def test_image_save(images, num=32): for i in range(num): img = Image.fromarray(images[i]) img.save('images/&#123;&#125;.jpg'.format(i), quality=100)test_image_save(test_images, num=32) 构建模型并打印summary 12345678910111213train_images = train_images.reshape((60000, 28,28,1))test_images = test_images.reshape((10000, 28,28,1))np.save('train_images', train_images)np.save('test_images', test_images)np.save('test_labels', test_labels)model = keras.Sequential([ keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(28,28,1)), keras.layers.Flatten(), keras.layers.Dense(128, activation='relu'), keras.layers.Dense(10)])model.summary() 12345678910111213141516Model: &quot;sequential&quot;_________________________________________________________________Layer (type) Output Shape Param # =================================================================conv2d (Conv2D) (None, 26, 26, 16) 160 _________________________________________________________________flatten (Flatten) (None, 10816) 0 _________________________________________________________________dense (Dense) (None, 128) 1384576 _________________________________________________________________dense_1 (Dense) (None, 10) 1290 =================================================================Total params: 1,386,026Trainable params: 1,386,026Non-trainable params: 0_________________________________________________________________ 编译并训练模型 1234model.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])model.fit(train_images, train_labels, epochs=10) 123456789101112131415161718192021222324252627282930Epoch 1/201875/1875 [==============================] - 13s 7ms/step - loss: 1.9238 - accuracy: 0.8173Epoch 2/201875/1875 [==============================] - 13s 7ms/step - loss: 0.3347 - accuracy: 0.8827Epoch 3/201875/1875 [==============================] - 13s 7ms/step - loss: 0.2755 - accuracy: 0.8995Epoch 4/201875/1875 [==============================] - 13s 7ms/step - loss: 0.2399 - accuracy: 0.9123 0s -Epoch 5/201875/1875 [==============================] - 14s 7ms/step - loss: 0.2122 - accuracy: 0.9227Epoch 6/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1852 - accuracy: 0.9323 0s -Epoch 7/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1617 - accuracy: 0.9408Epoch 8/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1429 - accuracy: 0.9478Epoch 9/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1321 - accuracy: 0.9518Epoch 10/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1153 - accuracy: 0.9586Epoch 11/201875/1875 [==============================] - 13s 7ms/step - loss: 0.1039 - accuracy: 0.9626Epoch 12/201875/1875 [==============================] - 13s 7ms/step - loss: 0.0935 - accuracy: 0.9658Epoch 13/20...Epoch 19/201875/1875 [==============================] - 13s 7ms/step - loss: 0.0579 - accuracy: 0.9800Epoch 20/201875/1875 [==============================] - 13s 7ms/step - loss: 0.0550 - accuracy: 0.9805 在测试集验证精度并保存模型为.h5文件 123test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)print('\nTest accuracy:', test_acc)model.save('tf2_fmnist_model.h5') 精度还可以吧，毕竟只是随便搭个模型验证流程用的 12313/313 - 1s - loss: 1.0434 - accuracy: 0.8792Test accuracy: 0.8791999816894531 Quantization and CompileVitisAI的量化和编译工具在docker中。VAI(VitisAi)量化器以一个浮点模型作为输入，这个浮点模型可以是Tensorflow1.x/2.x、Caffe或者PyTorch生成的模型文件，在量化器中会进行一些预处理，例如折叠BN、删除不需要的节点等，然后对权重、偏置和激活函数量化到指定的位宽(目前仅支持INT8)为了抓取激活函数的统计信息并提高量化模型的准确性，VAI量化器必须运行多次推理来进行校准。校准需要一个100~1000个样本的数据作为输入，这个过程只需要unlabel的图片就可以校准完成后生成量化过的定点模型，该模型还必须经过编译，以生成可以部署在DPU上的模型 Quantization the model由于训练模型使用的是Tensorflow2，因此这里必须使用docker的vai_q_tensorflow2来进行量化，读者如果使用的是其他框架，请使用对应的量化工具。请注意这里的vai_q_tensorflow2等量化工具并非是Tensorflow提供的，而是VitisAI为不同的深度学习框架定制的量化工具，请勿混淆 VitisAI中共支持以下4种量化方式 vai_q_tensorflow：用于量化Tensorflow1.x训练的模型 vai_q_tensorflow2：用于量化Tensorflow2.x训练的模型 vai_q_pytorch：用于量化Pytorch训练的模型 vai_q_caffe：用于量化caffe训练的模型 这四种量化工具都在docker中，但是在不同的conda虚拟环境中，因此启动进入docker后需要先激活对应的conda环境 12./docker_run.sh xilinx/vitis-aiconda activate vitis-ai-tensorflow2 vai_q_tensorflow2支持两种不同的方法来量化模型 Post-training quantization (PTQ)：PTQ是一种将预先训练好的浮点模型转换为量化模型的技术，模型精度几乎没有降低。需要一个具有代表性的数据集对浮点模型进行batch推断，以获得激活函数的分布。这个过程也叫量化校准 Quantization aware training(QAT)：QAT在模型量化期间对前向和后向过程中的量化误差进行建模，对于QAT，建议从精度良好的浮点预训练模型开始，而不是从头开始 本文选用的是PTQ方式。在VitisAI/models路径下创建文件夹mymodel，将以下文件放入其中 训练生成的tf2_fmnist_model.h5文件 训练集train_images.npy用于校准 测试集test_images.npy、测试标签test_labels用于校准后的评估 编写quantization.py量化脚本 12345678910111213141516171819202122232425262728293031323334353637import osimport numpy as npimport tensorflow as tffrom tensorflow import kerasfrom tensorflow_model_optimization.quantization.keras import vitis_quantizetrain_images = np.load('train_images.npy')test_images = np.load('test_images.npy')test_labels = np.load('test_labels.npy')# load float modelfloat_model = tf.keras.models.load_model('tf2_fmnist_model.h5')quantizer = vitis_quantize.VitisQuantizer(float_model)# Set calibration dataset to train_images, save quantized model to quantized.h5quantized_model = quantizer.quantize_model( calib_dataset=train_images[0:10], include_cle=True, cle_steps=10, include_fast_ft=True)quantized_model.save('quantized.h5')# Load quantized modelquantized_model = keras.models.load_model('quantized.h5')# Evaluate quantized modelquantized_model.compile( loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['sparse_categorical_accuracy'])quantized_model.evaluate(test_images, test_labels, batch_size=500)# Dump quantized modelquantizer.dump_model( quantized_model, dataset=train_images[0:1], dump_float=True) 运行该脚本 在目录下生成了quantized.h5量化模型文件，可以看到量化后的模型在测试集上评估精度为0.8797 Compile the model编译模型需要准备以下文件 quantized.h5：量化生成的模型文件 arch.json：DPU描述文件，在vitis项目中，dpu_trd_system_hw_link/Hardware/dpu.build/link/vivado/vpl/prj/prj.gen/sources_1/bd/design_1/ip/design_1_DPUCZDX8G_1_0/arch.json将arch.json拷贝到本目录下，运行以下命令进行编译 1vai_c_tensorflow2 -m quantized.h5 -a arch.json -o ./ -n tf2_cnn_fmnist 在目录下生成了tf2_cnn_fmnist.xmodel Inference首先在设备上需要安装VART相关工具，安装脚本在VitisAI仓库的setup/mpsoc/VART中，将该目录下的target_vart_setup.sh拷贝到设备上运行 编写推理Python程序 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import sysimport timeimport cv2import xirimport vartimport mathimport numpy as npdef get_graph_from_model(path): graph = xir.Graph.deserialize(path) root = graph.get_root_subgraph() if root.is_leaf: return [] cs = root.toposort_child_subgraph() return [ c for c in cs if cc.has_attr("device") and c.get_attr("device").upper() == "DPU" ]def image_preprocess(path, input_scale): print('input_scale:&#123;&#125;'.format(input_scale)) image = cv2.imread(path, 0) image = image / 2 image = image.astype(np.int8) return imagedef model_run(runner, img): input_tensors = runner.get_input_tensors() output_tensors = runner.get_output_tensors() print('input tensor type:&#123;&#125;'.format(input_tensors)) input_ndim = tuple(input_tensors[0].dims) print('input_ndim:&#123;&#125;'.format(input_ndim)) output_ndim = tuple(output_tensors[0].dims) print('output_ndim:&#123;&#125;'.format(output_ndim)) input_data = [np.empty(input_ndim, dtype=np.int8, order="C")] output_data = [np.empty(output_ndim, dtype=np.int8, order="C")] print('image shape:&#123;&#125;'.format(img.shape)) #input_data[0] = img.reshape(input_ndim[1:]) job_id = runner.execute_async(img.reshape((1,28,28,1)), output_data) runner.wait(job_id) print(output_data) return np.argmax(output_data[0])def main(argv): image_path = argv[1] model_path = argv[2] print('image path:&#123;&#125;\nmodel path:&#123;&#125;'.format( image_path, model_path)) graph = get_graph_from_model(model_path) dpu_runner = vart.Runner.create_runner(graph[0], "run") input_fixpos = dpu_runner.get_input_tensors()[0].get_attr("fix_point") input_scale = 2**input_fixpos image_data = image_preprocess(image_path, input_scale) print('image shape:&#123;&#125;'.format(image_data.shape)) print('&#123;&#125;'.format(image_data.reshape((28,28,1)))) time_start = time.time() pred = model_run(dpu_runner, image_data) print('pred:&#123;&#125;'.format(pred)) time_end = time.time() timetotal = time_end - time_start print('total time:&#123;&#125;'.format(timetotal))if __name__ == "__main__": main(sys.argv) 将推理程序、编译后的xmodel模型文件以及测试图片拷贝到设备中，运行推理程序，传入测试图片路径和模型文件路径进行推理，会打印出预测结果 推理代码主要使用了VART API，主要步骤为 加载模型并转换为计算图graph 根据计算图生成DPU Runner 加载输入图片预处理，并构建输入输出Tensor，需要注意各Tensor的shape和dtype 输入输出Tensor传入DPU Runner，异步执行，并同步等待结果 输出Tensor转换为期望的预测数据格式]]></content>
      <categories>
        <category>AI</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-06-DPU-Configuration]]></title>
    <url>%2F2022%2F07%2F31%2FVitisAI-06-DPU-Configuration%2F</url>
    <content type="text"><![CDATA[本文对DPU的一些配置选项进行介绍。主要参考文档为pg338，”DPUCZDX8G for Zynq UltraScale+MPSoCs Product Guide” DPU配置文件对DPU配置的描述在Vitis工程中，以本系列使用的Vitis工程路径为例，在dpu_trd_system/dpu_trd_kernels/src/prj/Vitis/dpu_conf.vh文件中 该文件中对DPU的配置项主要有以下6个部分 Architecture Options URAM Enable/Disable DRAM Enable/Disable RAM Usage Configuration Channel Augmentation Configuration DepthWiseConv Configuration Pool Average Configuration Multiplication Feature Maps RELU Type Configuration DSP48 Usage Configuration Power Configuration DEVICE Configuration Architecture OptionsDPU可以被配置为多种多样的卷积架构，这些架构与卷积单元的并行度有关。DPUCZDX8G的可选架构有：B512、B800、B1024、B1152、B1600、B2304、B3136和B4096。不同的架构对逻辑资源的要求不同，更大的架构能够获得更高性能但是代价是占用更多资源 以下是不同架构在LUT、Register、Block RAM、DSP方面的资源占用情况 Architecture LUT Register Block RAM DSP B512 27893 35435 73.5 78 B800 30468 42773 91.5 117 B1024 34471 50763 105.5 154 B1152 33238 49040 123 164 B1600 38716 63033 127.5 232 B2304 42842 73326 167 326 B3136 47667 85778 210 436 B4096 53540 105008 257 562 在DPUCZDX8G的卷积操作有3个维度的并行度：像素级并行(Pixel Parallelism，PP)、输入通道并行(Input Channel Parallelism，ICP)和输出通道并行(Output Channel Parallelism， OCP)，以下是不同架构的并行度情况 Architecture PP ICP OCP Peak Ops B512 4 8 8 512 B800 4 10 10 800 B1024 8 8 8 1024 B1152 4 12 12 1152 B1600 8 10 10 1600 B2304 8 12 12 2304 B3136 8 14 14 3136 B4096 8 16 16 4096 可以看到，架构的命名方式是以每个时钟周期的操作量而定，因为卷积操作是一个乘法跟一个加法，因此单周期操作数为PP*ICP*OCP*2 dpu_config.vh文件中，可以通过如下部分设置所选用的架构 1234567891011121314151617181920212223/*====== Architecture Options ======*/// |------------------------------------------------------|// | Support 8 DPU size// | It relates to model. if change, must update model// +------------------------------------------------------+// | `define B512 // +------------------------------------------------------+// | `define B800 // +------------------------------------------------------+// | `define B1024 // +------------------------------------------------------+// | `define B1152 // +------------------------------------------------------+// | `define B1600 // +------------------------------------------------------+// | `define B2304 // +------------------------------------------------------+// | `define B3136 // +------------------------------------------------------+// | `define B4096 // |------------------------------------------------------|`define B1024 这里选用的是B1024，如果选用其他架构，直接修改B1024替换为所选用架构的名称即可 RAM Usage权重、偏置和运算过程的一些立即数据是存储在片上内存中的，片上内存包括Block RAM和UltraRAM，不同架构的RAM占用情况不同，高内存占用(High Usage)意味着DPU能够更加灵活的处理中间数据，更高的性能 dpu_config.vh中与RAM相关的配置有URAM、DRAM和RAM 以下配置用于开启或关闭URAM的使用，请注意你所使用的FPGA芯片是否有支持URAM 12345678910// |------------------------------------------------------|// | If the FPGA has Uram. You can define URAM_EN parameter // | if change, Don't need update model// +------------------------------------------------------+// | for zcu104 : `define URAM_ENABLE // +------------------------------------------------------+// | for zcu102 : `define URAM_DISABLE // |------------------------------------------------------|`define URAM_DISABLE 以下配置用于开启或关闭DRAM，请注意你所使用的FPGA芯片是否有支持DRAM 12345678910// |------------------------------------------------------|// | You can use DRAM if FPGA has extra LUTs // | if change, Don't need update model// +------------------------------------------------------+// | Enable DRAM : `define DRAM_ENABLE // +------------------------------------------------------+// | Disable DRAM : `define DRAM_DISABLE // |------------------------------------------------------|`define DRAM_DISABLE 以下配置用于选择Block RAM的使用是高占用还是低占用 12345678910// |------------------------------------------------------|// | RAM Usage Configuration // | It relates to model. if change, must update model// +------------------------------------------------------+// | RAM Usage High : `define RAM_USAGE_HIGH // +------------------------------------------------------+// | RAM Usage Low : `define RAM_USAGE_LOW // |------------------------------------------------------|`define RAM_USAGE_LOW Channel Augmentation通道增强可以用于提升DPU效率，不同架构DPU输入通道最大可以到8的并行度，而一般的卷积运算尤其是图像领域，输入通道通常为3，这样没有充分利用所有输入通道的并行性，而启用通道增强，可以最大程度利用输入通道的并行性 以下是对通道增强的配置，只能选择开启或关闭 12345678910// |------------------------------------------------------|// | Channel Augmentation Configuration// | It relates to model. if change, must update model// +------------------------------------------------------+// | Enable : `define CHANNEL_AUGMENTATION_ENABLE // +------------------------------------------------------+// | Disable : `define CHANNEL_AUGMENTATION_DISABLE // |------------------------------------------------------|`define CHANNEL_AUGMENTATION_ENABLE DepthwiseConv 在标准的卷积操作中，每个输入通道对应一个kernel，将所有通道的运算结果结合得到最终卷积结果。而在深度可分离卷积中，运算分为两个步骤：深度卷积和点卷积。深度卷积是指对每个特征图进行单独计算。点卷积是进行1x1的标准卷积。深度可分卷积的并行度只有标准卷积的一半，可以大大提升卷积效率 以下是深度可分卷积的配置 12345678910// |------------------------------------------------------|// | DepthWiseConv Configuration// | It relates to model. if change, must update model// +------------------------------------------------------+// | Enable : `define DWCV_ENABLE // +------------------------------------------------------+// | Disable : `define DWCV_DISABLE // |------------------------------------------------------|`define DWCV_ENABLE ElementWise Multiply元素点积计算两个特征图的Hadamard乘积 12345678910// |------------------------------------------------------|// | support multiplication of two feature maps// | It relates to model. if change, must update model// +------------------------------------------------------+// | Enable : `define ELEW_MULT_ENABLE // +------------------------------------------------------+// | Disable : `define ELEW_MULT_DISABLE // |------------------------------------------------------|`define ELEW_MULT_DISABLE AveragePool该选项决定DPU是否支持平均池化，大小可选2x2、3x3到8x8 12345678910// |------------------------------------------------------|// | Pool Average Configuration// | It relates to model. if change, must update model// +------------------------------------------------------+// | Enable : `define POOL_AVG_ENABLE // +------------------------------------------------------+// | Disable : `define POOL_AVG_DISABLE // |------------------------------------------------------|`define POOL_AVG_ENABLE ReLU TypeReLU类型决定DPU可用哪种ReLU函数，默认支持ReLU和ReLU6 以下是ReLU的配置，最多只能选择ReLU+LeakyReLU+ReLU6 12345678910// +------------------------------------------------------+// | RELU Type Configuration// | It relates to model. if change, must update model// +------------------------------------------------------+// | `define RELU_RELU6// +------------------------------------------------------+// | `define RELU_LEAKYRELU_RELU6// |------------------------------------------------------|`define RELU_LEAKYRELU_RELU6 Softmax该选项可以在硬件实现Softmax操作，硬件实现的Softmax是一个独立的加速器，能够支持INT8输入和浮点输出。硬件实现的Softmax比MPSoC上软件实现的Softmax快160倍。硬件实现的Softmax最大只支持1023个分类的任务，如果类别数超过1023则只能使用软件实现 硬件Softmax并非在dpu_config.vh中配置，而是在vitis的UI界面中添加]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>DPU</tag>
        <tag>PetaLinux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202207]]></title>
    <url>%2F2022%2F07%2F16%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202207%2F</url>
    <content type="text"><![CDATA[Label Studio在研究目标检测任务时，必定需要了解如何对自定义的数据集做标注，网络上提到最多的就是labelme，然而我觉得labelme并没有那么好用，这是由于标注这块目前还是挺乱的，一方面目标检测数据集的格式各不相同，例如VOC数据集使用的是xml格式的标注images+annotations，标注坐标为xmin、ymin、xmax、ymax，而YOLO又有自己的标注格式images+labels，标注文件是txt格式，很多开源的目标检测源码支持的格式也各不相同，有些是直接将xml、yolo格式的数据集读进来做训练，有些又是转换成AI框架的数据集格式例如tfrecord，再读取进行训练。在了解标注工具时，我了解到Label Studio这个好用的工具，能够对语音识别、目标检测、实例分割等多种任务的数据进行标注 在目标检测的标注任务中，Label Studio能够方便添加label，而且可以对不同类别的Bounding Box自由选择颜色 将标注好的数据支持YOLO、VOC、COCO等格式的导出，非常方便 可以直接在pip中安装label studio，注意不要安装最新版本，访问有些问题 1pip install -U label-studio==1.4]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-05-Vitis Flow]]></title>
    <url>%2F2022%2F07%2F06%2FVitisAI-05-Vitis-Flow%2F</url>
    <content type="text"><![CDATA[本文承接VitisAI-04-PetaLinux Flow，介绍使用Xilinx的Vitis工具利用Vivado生成的design_1_wrapper.xsa文件以及PetaLinux编译的rootfs和内核镜像，生成制作好的SD卡镜像文件sd_card.img PetaLinux工程准备编译好的各文件PetaLinux编译成功后的输出文件在PetaLinux工程的image/linux下，进入此目录，并在其中创建pfm文件夹，并创建boot和sd_dir两个子目录，将Vitis工程需要的PetaLinux输出文件拷贝到新创建的文件夹下 1234cd image/linuxmkdir pfmmkdir pfm/bootmkdir pfm/sd_dir 拷贝以下文件到pfm/boot bl31.elf pmufw.elf zynqmp_fsbl.elf u-boot.elf system.dtb 1cp bl31.elf pmufw.elf zynqmp_fsbl.elf u-boot.elf system.dtb pfm/boot 拷贝以下文件到pfm/sd_dir boot.scr system.dtb 1cp boot.scr system.dtb pfm/sd_dir 创建Vitis Platform Project运行Vitis环境变量脚本 1source ~/opt/pkg/xilinx/Vitis/2021.1/settings64.sh 在dpu_custom下创建dpu_vitis文件夹，并进入，打开vitis 123mkdir dpu_vitiscd dpu_vitisvitis 此时的各项目文件夹路径关系为 1234|--dpu_custom |--dpu_vivado |--dpu_plnx |--dpu_vitis 选择workspace为dpu_vitis，选择File-&gt;New-&gt;Platform Project，创建一个平台项目，设置项目名为dpu_base，点击Next，选择dpu_vivado中的.xsa硬件描述文件，Operating system选linux，取消勾选Generate boot components，点击Finish 选中linux on psu_cortexa53，在Bif File处点击下三角生成Bif File文件，Boot Components Directory选择PetaLinux项目中创建的pfm/boot，FAT32 Partition Directory选择pfm/sd_dir 右键工程编译工程，该编译过程很快(1分钟以内) Vitis安装Vitis AI克隆Vitis AI仓库到本地在Vitis中添加DPU需要预先将Vitis AI仓库克隆到本地，使用如下命令 1git clone https://github.com/Xilinx/Vitis-AI 安装Vitis AI到Vitis在Vitis中选择Windows→Preferences 点击Add添加一个库 ID设置为vitis ai，Name设置为Vitis AI，Location设置为Vitis AI仓库路径 点击Apply and Close 安装交叉编译环境sdk点击sdk-2021.2.0.0.sh下载该sdk，运行以下命令，将其安装到PetaLinux路径下 1./sdk-2021.2.0.0.sh 安装完成后PetaLinux安装路径下会出现environment-setup-cortexa72-cortexa53-xilinx-linux的文件 创建Vitis Application 工程File→New→Application Project，点击Next，上文创建的platform工程自动出现在选项中，选中并点击Next 设置项目名为dpu_trd，点击Next 设置sysroot path为安装的交叉编译链位置：~/opt/pkg/petalinux/2021.2/sysroots/cortexa72-cortexa53-xilinx-linux，Root FS为dpu_plnx/image/linux/rootfs.ext4，Kernel Image为dpu_plnx/image/linux/image 点击Next，在左侧选中dsa→DPU Kernel(RTL Kernel)，点击Finish(必须在Vitis中成功安装了VitisAi仓库，这一步中才会出现DPU Kernel选项) 将Emulation-SW修改为Hardware 选择dpu_trd_system→dpu_trd_kernel→src→prj→Vitsi→dpi_conf.vh，将B4096改为B1024，之所以改为B1024是因为本文实验环境使用的FPGA芯片资源有限，只能选择B1024的DPU，这里的B1024和B4096是DPU不同架构配置，越大的数字代表了越高的并行度和计算性能，但同时占用更多片上资源(LUT、RAM、DSP)，关于DPU相关的配置参数和资源占用，包括dpi_conf.vh文件中可选的参数含义，后续会专门出一片文章来介绍 选择dpu_trd_system→dpu_trd_system_hw_link→dpu_trd_system_hw_link.prj，去除sfm_xrt_top，减少DPU数量为1(将DPU数量减小到1也是由于本文使用的FPGA资源受限) 在Assistant窗口双击dpu_trd_system，弹出窗口 选择dpu_trd_system→dpu_trd_system_hw_link→Hardware→dpu 点击V++configuration settings的省略号按钮，为其添加时钟配置 添加如下代码 12345678[clock]id=1:DPUCZDX8G_1.aclkid=2:DPUCZDX8G_1.ap_clk_2[connectivity]sp=DPUCZDX8G_1.M_AXI_GP0:HPC0sp=DPUCZDX8G_1.M_AXI_HP0:HP0sp=DPUCZDX8G_1.M_AXI_HP2:HP1 这里的时钟接口和vivado中配置的时钟一一对应 有些版本Vitis编译过程会出现找不到opencv库的error，点击Apply and Close，在dpu_trd_system→dpu_trd上右键选择C/C+= Build Settings 在includes中添加 1$&#123;SYSROOT&#125;/usr/include/opencv4 点击Apply 右键dpu_trd_system进行编译，该过程耗时较长，本机环境编译时长在20~30分钟。编译成功后，在vitis工程路径下的dpu_trd_system/Hardware/package路径下会生成sd_card.img 该文件是合并了linux内核镜像、uboot、dpu,xclbin二进制文件以及设备树文件的SD卡镜像文件，SD卡分区已经做好的，直接使用balenaEtcher将该文件烧写到SD卡中即可]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>DPU</tag>
        <tag>PetaLinux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-04-PetaLinux Flow]]></title>
    <url>%2F2022%2F06%2F30%2FVitisAI-04-PetaLinux-Flow%2F</url>
    <content type="text"><![CDATA[本文承接VitisAI-03-Vivado Flow，介绍使用Xilinx的PetaLinux工具将Vivado生成的design_1_wrapper.xsa文件创建PetaLinux并编译生成Linux镜像和rootfs的过程 创建PetaLinux工程首先运行PetaLinux的环境变量脚本 1source ~/opt/pkg/Xilinx/PetaLinux/2021.2/settings.sh 在dpu_vivado同级目录下，通过以下命令创建一个PetaLinux工程”dpu_plnx” 1petalinux-create --type project --template zynqMP --name dpu_plnx 将vivado生成的.xsa文件拷贝到PetaLinux工程目录下并进行工程配置 123cp dpu_vivado/dpu_hardware/design_1_wrapper.xsa dpu_plnxcd dpu_plnxpetalinux-config --get-hw-description=. 运行配置命令后，会弹出类似配置内核时的menuconfig界面 设置离线编译由于PetaLinux的编译过程中需要从网络中下载很多包资源，并且很多包的源是外网，编译过程会很缓慢。Xilinx为PetaLinux的编译提供了离线下载方式，官网将PetaLinux编译依赖的包资源进行了打包处理，预先将依赖包下载到本地，再进行PetaLinux编译时，可以极大的加快编译速度，非常建议使用。离线编译包的下载在VitisAI-02-环境与资源文章中已经介绍，离线编译的缺点是需要占用100G+的磁盘容量 本文环境，已将downloads_2021.2.tar.gz和sstate_aarch64_2021.2.tar.gz两个文件下载并解压至~/opt/sstate-cache/downloads_2021.2和~/opt/sstate-cache/sstate_aarch64_2021.2路径下 关闭Enable Network sstate feeds petalinux-config中，进入Yocto Settings，取消选择Yocto Settings→Enable Network sstate feeds 开启Enable BB NO Network 选中Yocto Settings→Enable Network sstate feeds 设置local sstate feeds url Yocto Settings→local sstate feeds url设置为”/home/username/opt/sstate-cache/sstate_aarch64_2021.2/aarch64” 设置Add pre-mirror url path Yocto Settings→Add pre-mirror url path设置为”file:///home/username/opt/sstate-cache/downloads_2021.2” 修改文件系统类型为ext4petalinux-config退回到根目录，选择Image Packaging Configuration-&gt;Root filesystem type为EXT4 由于本文所使用FPGA开发板有EMMC和SD卡两个外部存储，根文件系统是放在SD卡中，SD卡为分区1，因此还需要修改SD卡的分区为/dev/mmcblk1p2 至此，在petalinux-config需要修改的内容完毕 去掉dnndkdnndk是vitisai老版本中使用的开发工具，新版本中已经废除了，需要在vitisai.bb中去掉dnndk 1vim components/yocto/layers/meta-petalinux/recipes-core/packagegroups/packagegroup-petalinux-vitisai.bb 修改设备树文件1vim project-spec/meta-user/recipes-bsp/device-tree/files/system-user.dtsi 修改为 1234567891011121314151617/include/ "system-conf.dtsi"/ &#123; chosen &#123; bootargs = "earlycon console=ttyPS0,115200 clk_ignore_unused root=/dev/mmcblk1p2 rw rootwait cma=512M"; &#125;;&#125;;&amp;sdhci1 &#123; no-1-8-v; disable-wp;&#125;;/* USB */&amp;dwc3_0 &#123; status="okay"; dr_mode="host";&#125;; 配置kernel1petalinux-config -c kernel 配置rootfsrootfs添加user packagevim打开user-rootfsconfig文件 1vim project-spec/meta-user/conf/user-rootfsconfig 添加以下内容 123456789101112131415161718CONFIG_xrtCONFIG_dnfCONFIG_e2fsprogs-resize2fsCONFIG_partedCONFIG_resize-partCONFIG_packagegroup-petalinux-vitisaiCONFIG_packagegroup-petalinux-self-hostedCONFIG_cmakeCONFIG_packagegroup-petalinux-vitisai-devCONFIG_xrt-devCONFIG_opencl-clhpp-devCONFIG_opencl-headers-devCONFIG_packagegroup-petalinux-opencvCONFIG_packagegroup-petalinux-opencv-devCONFIG_mesa-megadriverCONFIG_packagegroup-petalinux-x11CONFIG_packagegroup-petalinux-v4lutilsCONFIG_packagegroup-petalinux-matchbox 配置rootfs 1petalinux-config -c rootfs 选择user packages，选中所有内容 编译1petalinux-build 编译后在工程目录下的images/linux路径下会生成u-boot、rootfs、linux image等文件]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>DPU</tag>
        <tag>PetaLinux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Arm Cortex-M 高效神经网络计算]]></title>
    <url>%2F2022%2F05%2F31%2FArm-Cortex-M-%E9%AB%98%E6%95%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E8%AE%A1%E7%AE%97%2F</url>
    <content type="text"><![CDATA[本文源自ARM CMSIS-NN项目的一篇论文”CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs”。CMSIS-NN是一种高效的内核，用于将最大化性能和最小化内存占用的神经网络应用于Arm Cortex-M处理器上。基于CMSIS-NN核的神经网络在推理运行时间上提高4.6倍，在能效上提高4.9倍 随着物联网的发展，各种边缘设备的规模迅速发展，各种边缘设备收集的数据需要经由无线网络传送到云端处理。随着节点数量的增加，对网络带宽造成了很大的负担，并增加了延迟，另一方面是数据的安全问题。一个好的解决方案是边缘计算 另一方面，深度神经网络在很多复杂任务中的表现已经超过人类，例如图像分类、语音识别、自然语言处理等。但是由于计算复杂度和资源要求较高，NN的执行主要局限于高性能服务器或专用硬件加速的云计算服务，这增加了网络负担。如果能在数据收集的边缘设备上部署一些小型的神经网络进行简单轻量化任务，这将减少整个网络的延迟和能源消耗 CMSIS-NN的神经网络内核结构如图，内核代码包括两部分：NNFunctions和NNSupportFunctions。NNFunctions包括一些神经网络层函数，如卷积、深度可分离卷积、全连接、池化和激活等，这些函数可被应用程序代码调用以实现神经网络推理。NNSupportFunctions包括一些实用的计算函数，如数据转换、一些查找表 定点量化神经网络模型使用32位浮点数据进行训练，然而在推理过程中并不需要这么高的精度。研究表明，即使在低精读定点表示下，神经网络也能很好的工作。定点量化有助于避免昂贵的浮点计算，并减少存储权重和激活函数的内存占用，这对于资源受限的平台至关重要。CMSIS-NN同时支持8位和16位的定点 CMSIS-NN中使用q7_t和q15_t来表示int8和int16类型。量化方式使用的是”Power of 2”，即将数据表示为$A \times 2^{n}$的形式，其中$A$为整数值，$n$为缩放因子，内核在进行运算时使用移位来量化和反量化。这种量化方式的好处是不需要额外的FPU来进行浮点数据运算，很多微控制器没有FPU 优化的内核Arm Cortex-M处理器支持SIMD指令，特别是对神经网络计算非常有效的16位乘积指令 Support Functions大多数NNFunctions使用16位的乘积指令，因此需要将q7_t转为q15_t。CMSIS提供了一个实用的函数arm_q7_to_q15。数据转换分两步完成，第一步是通过使用符号扩展指令__SXTB16将8位数据扩展为16位，第二步是重新排列数据，使输出数据与输入数据顺序相同 因为在NN计算过程中，需要调用很多次数据转换，因此它的性能至关重要。如果两个操作数遵循相同的的顺序，则可以省略第二步的重新排序。为了更好的利用这一点，创建了另一个版本的数据转换，不需要重新排序 矩阵乘法矩阵乘法是神经网络中最重要的计算核，CMSIS中使用2x2的核来实现。乘积结果是32位数据，乘积使用__SMLAD 正常的操作下，Sum11是由A的第一行与B的第一列对应元素乘积的和构成，每次取一个元素，则需要循环4次 __SIMD32指令一次取32位数据，由于数据是q15_t的，因此一次可以读入2个数据，只需循环两次即可完成 上文中提到q7_t扩展到q15_t去掉重新排列的问题，在矩阵乘法中，可以不用进行重排来提高性能。但是当元素数量不是4倍数时，数据对齐会比较棘手 如下图，q7_t扩展到q15_t时，A和B的数据元素位置都交错了，但是由于相乘时元素的对应关系没有改变，因此结果不变 另一种场景是q7_t的权重和q15_t的激活函数。这种情况下，权重可以通过交换每32位的第二和第三字节进行预处理，即将[1,2,3,4]转换为[1,3,2,4]，经过预处理后，不需要重新排序，扩展后的权重数据顺序会回到[1,2,3,4] 全连接层的向量与矩阵乘法也可以利用1x2的核来提高性能 由于权重一直保持不变，并在推断期间重复使用，可以重新排序权重矩阵，以便行数据交错，并且只需要一个指针访问就可以读取。这种权重调整如图所示 矩阵向量相乘时，使用相同的q7_t扩展到q15_t函数，不需要重新排列，这样可以使用可用的寄存器来容纳1x4的内核 卷积通常，一个基于CPU的卷积实现被分解位输入重新排序和展开(im2col)以及矩阵乘法。im2col是将类似图像的输入转换为表示每个卷积滤波器所需数据的列的过程 im2col的主要问题是内存占用很高，im2col矩阵中有很多重复项。为了解决内存占用问题，同时保留im2col带来的性能优势，为卷积内核实现了部分im2col。内核只会扩展为有限的列，足以从矩阵乘法内核获得最大性能提升，同时保持最小内核开销 数据格式也会影响卷积性能。当batchsize为1时，卷积操作是对3D数据的二维卷积，延两个方向移动 最常见的两种图像数据格式是CHW和HWC。在HWC格式中，沿通道的数据以步长1存储，沿宽度的数据以通道计数的步长存储，沿高度的数据以(通道计数x图像宽度)步长存储。HWC格式的数据可以实现高效的数据移动，每个像素是连续存储的，可以通过SIMD指令高效的复制。在Arm Cortex-M7上将CHW和HWC进行了比较，HWC输入固定为16x16x16，当输出通道为0时，意味着只执行im2col操作，不执行矩阵乘法。与CHW格式相比，相同矩阵乘法，HWC有更少的运行时间 Pooling与卷积类似，池化是一个基于窗口的操作，具有给定的内核大小，步长和填充。与卷积不同，池化通常在同一通道内操作，并独立于其他通道的数据。一个有效的池化替代方案是将池化操作分为x池化和y池化。这样，x方向上的池化操作(最大、平均)可以在y方向上重复使用，从而减少了操作总数，称这种方法为x-y池化。x-y池化的一个潜在问题是数据安排，因为需要额外的内存来存储中间结果。为了消除额外内存的要求，内核在原位进行池化，这样池化对输入数据进行了破坏 激活函数ReLUCMSIS中使用类似SWAR的概念来实现ReLU。关键是识别q7_t的符号位，如果数字为负数，则使其为0。实现方式如下图 将符号位提取出来构成一个字节，通过使用字节级减法指令__QSUB8将0与符号位提取生成的字节数相减，得到一个掩码，如果是负数，则提取的字节为0x01，0减去0x01为0xFF，那么原始数据与0xFF的非得到0，如果是整数，0减0依然是0，原始数据与0x00的非还是原始数据 Sigmoid and Tanhsigmoid和tanh需要使用专用的数学函数，这种计算在Cortex-M架构的CPU上计算程本很高，因此使用固定输入和固定输出的查找表来实现。有两种方式实现的查找表，第一种是对所有范围使用一个统一的表，输入粒度固定，输入的MSB用于识别查找表中的条目，LSB可以用于线性插值 另一种选择是实现两个独立的表，以覆盖函数的不同区域，这样精度更高，因为sigmoid和tanh都是高度非线性的。对于0附近的输入区域，应该有一个细粒度的表，对于绝对值较大的区域，有另一个粗粒度的表 确定sigmoid和tanh函数表的范围是很重要的，CMSIS中使用了[-8,8]，因为sigmoid(8)=0.9997，tanh(8)=0.9999 实验结果在CIFAR-10数据集上训练的CNN测试了CMSIS-NN内核，该数据集由6万张32x32的彩色图像组成，分为10个类别。网络结构如下表，3个卷积层和一个全连接层。所有权重和激活函数都量化为q7_t 整个图像分类每张约运行99.1ms，帧频约为10fps。运行该网络时，CPU计算吞吐量大约为每秒249MOps。在CIFAR-10测试集上，预量化网络准确率达到80.3%，在Arm Cortex-M7上运行的8位量化网络达到79.9%。使用CMSIS-NN内核最大内存占用为133KB，其中使用部分im2col实现卷积以节省内存。如果不使用部分im2col，内存开销将到达332KB 为了量化CMSIS-NN内核相对于现有方案的优势，使用了一维卷积函数实现了一个基线版本。下表总结了基线版本与CMSIS-NN的结果比较，CMSIS-NN在运行时间上比基线版本提高了2.6倍，吞吐量上提高了5.4倍 ReferenceCMSIS NN Software Library GitHub - CMSIS_5 arxiv - CMSIS-NN: Efficient Neural Network Kernels for Arm Cortex-M CPUs]]></content>
      <categories>
        <category>AI</category>
        <category>量化与加速</category>
      </categories>
      <tags>
        <tag>Arm Cortex-M</tag>
        <tag>神经网络</tag>
        <tag>加速计算</tag>
        <tag>神经网络量化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-boxfilter快速窗口求和]]></title>
    <url>%2F2022%2F05%2F25%2FDIP-boxfilter%E5%BF%AB%E9%80%9F%E7%AA%97%E5%8F%A3%E6%B1%82%E5%92%8C%2F</url>
    <content type="text"><![CDATA[在很多图像处理任务中，需要对局部图像数据进行窗口运算，许多运算都需要获得局部区域像素的均值和方差，这些计算都离不开求和运算。我在研究引导滤波时，发现何凯明在其提供的引导滤波matlab参考代码中，使用了boxfilter的快速计算方法，能够以$O(1)$的时间复杂度对矩阵进行局部快速求和运算 本文对以python代码为例，对boxfilter的运算细节进行可视化分析，使读者对整个计算过程有一个直观的感受，然后将正常的卷积局部求和操作与boxfilter进行性能比较 运算过程分析首先对boxfilter的输入和输出进行说明，boxfilter的输入是要计算局部窗口和的整幅图像$I$，输出是与输入图像尺寸相同的图像$F$，输出图像某个像素的值$F(x,y)$是以$I(x,y)$为中心的一个窗口内所有像素值的和，窗口在图像上滑动，与卷积操作类似，窗口大小也是一个输入参数，一般以窗口半径$r$来表示。下图是输入与输出关系的示意图 输出图像是将一个窗口的求和作用于输入图像的结果，上图中窗口半径为1，则窗口尺寸为3x3，图中输入图像中的黄色3x3区域即为一个窗口，该窗口内输入图像的像素之和对应于输出图像的黄色像素 下面将详细介绍整个快速计算的过程，引入依赖库 1import numpy as np 定义一个8x8的二维单位矩阵作为输入图像，定义窗口半径为1，则kernel size为3x3，创建一个与输入图像相同shape的0矩阵 12345r = 1imSrc = np.ones((8, 8))I = imSrc.copy()imDst = np.zeros(I.shape)h, w = I.shape 打印imSrc 1print(imSrc) 12345678[[1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.] [1. 1. 1. 1. 1. 1. 1. 1.]] 打印imDst 1print(imDst) 12345678[[0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] 对输入图像进行行方向的累加求和，并打印 12I = np.cumsum(I, 0)print(I) 12345678[[1. 1. 1. 1. 1. 1. 1. 1.] [2. 2. 2. 2. 2. 2. 2. 2.] [3. 3. 3. 3. 3. 3. 3. 3.] [4. 4. 4. 4. 4. 4. 4. 4.] [5. 5. 5. 5. 5. 5. 5. 5.] [6. 6. 6. 6. 6. 6. 6. 6.] [7. 7. 7. 7. 7. 7. 7. 7.] [8. 8. 8. 8. 8. 8. 8. 8.]] 将累加求和后的矩阵2、3两行复制到imDst的第1、2行，并打印(本文中描述第n行，n从1开始) 12imDst[:r+1, :] = I[r:2*r+1, :]print(imDst) 12345678[[2. 2. 2. 2. 2. 2. 2. 2.] [3. 3. 3. 3. 3. 3. 3. 3.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.] [0. 0. 0. 0. 0. 0. 0. 0.]] 这一步的过程如下图所示 该操作的含义是当求和窗口未完全在图像内部时的图像边缘求和处理 然后将累加矩阵的第4到8行减去1到5行的结果复制到imDst的第3到7行，并打印 12imDst[r+1:h-r, :] = I[2*r+1:, :] - I[:h-2*r-1, :]print(imDst) 12345678[[2. 2. 2. 2. 2. 2. 2. 2.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [0. 0. 0. 0. 0. 0. 0. 0.]] 示意图如下 然后将累加矩阵的最后一行与第6行相减，结果复制到imDst的第8行 12imDst[h-r:,:]=np.matlib.repmat(I[h-1,:],r,1)-I[h-2*r-1:h-r-1,:]print(imDst)) 12345678[[2. 2. 2. 2. 2. 2. 2. 2.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [3. 3. 3. 3. 3. 3. 3. 3.] [2. 2. 2. 2. 2. 2. 2. 2.]] 示意图如下 这里使用的repmat含义是将累加矩阵的最后一行复制为一个r行的矩阵，因为r为1，因此不进行任何操作。如果r大于1时，在下边缘处的窗口计算，就需要补一行 至此，行处理完成，接下来进行列处理。首先是对输入图像进行列方向的累加求和，并打印 12I = np.cumsum(imDst, 1)print(I) 注意列方向进行累加求和是在行方向累加求和基础上做的 12345678[[ 2. 4. 6. 8. 10. 12. 14. 16.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 3. 6. 9. 12. 15. 18. 21. 24.] [ 2. 4. 6. 8. 10. 12. 14. 16.]] 然后将累加和的前第2、3列拷贝到imDst的第1、2列 12imDst[:,:r+1] = I[:,r:2*r+1]print(imDst) 12345678[[4. 6. 2. 2. 2. 2. 2. 2.] [6. 9. 3. 3. 3. 3. 3. 3.] [6. 9. 3. 3. 3. 3. 3. 3.] [6. 9. 3. 3. 3. 3. 3. 3.] [6. 9. 3. 3. 3. 3. 3. 3.] [6. 9. 3. 3. 3. 3. 3. 3.] [6. 9. 3. 3. 3. 3. 3. 3.] [4. 6. 2. 2. 2. 2. 2. 2.]] 示意图如下 然后将累加和的第4到8列减去1到5列，结果拷贝到imDst的第3到7列 12imDst[:,r+1:w-r] = I[:,2*r+1:]-I[:,:w-2*r-1]print(imDst) 12345678[[4. 6. 6. 6. 6. 6. 6. 2.] [6. 9. 9. 9. 9. 9. 9. 3.] [6. 9. 9. 9. 9. 9. 9. 3.] [6. 9. 9. 9. 9. 9. 9. 3.] [6. 9. 9. 9. 9. 9. 9. 3.] [6. 9. 9. 9. 9. 9. 9. 3.] [6. 9. 9. 9. 9. 9. 9. 3.] [4. 6. 6. 6. 6. 6. 6. 2.]] 示意图如下 最后将累加和第8列减去第6列，结果复制到imDst的第8列 12imDst[:,w-r:] = np.matlib.repmat(I[:,w-1].reshape(-1,1),1,r)-I[:,w-2*r-1:w-r-1]print(imDst) 12345678[[4. 6. 6. 6. 6. 6. 6. 4.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [4. 6. 6. 6. 6. 6. 6. 4.]] 示意图如下 这个结果是否正确呢？可以选取几个局部窗口位置计算一下，可以看到，结果确实是正确的 整合之后的boxfilter函数如下 1234567891011121314151617def boxfilter(im, r): I = im.copy() imDst = np.zeros(I.shape) h, w = I.shape # row process I = np.cumsum(I, 0) imDst[:r+1, :] = I[r:2*r+1, :] imDst[r+1:h-r, :] = I[2*r+1:, :] - I[:h-2*r-1, :] imDst[h-r:,:]=np.matlib.repmat(I[h-1,:],r,1)-I[h-2*r-1:h-r-1,:] # col process I = np.cumsum(imDst, 1) imDst[:,:r+1] = I[:,r:2*r+1] imDst[:,r+1:w-r] = I[:,2*r+1:]-I[:,:w-2*r-1] imDst[:,w-r:] = np.matlib.repmat(I[:,w-1].reshape(-1,1),1,r)-I[:,w-2*r-1:w-r-1] return imDst 性能分析本节使用正常的卷积局部求和操作与boxfilter局部求和进行对比分析，看看boxfilter性能到底如何。这里使用的卷积操作与这篇文章中的代码一致 利用卷积操作来进行局部求和，只需要将卷积核定义为如下形式即可 1kernel = np.array([[1,1,1], [1,1,1], [1,1,1]]) 这样就定义了一个3x3的局部求和卷积核，验证一下卷积局部求和与boxfilter的输出结果是否一致 1234imSrc = np.ones((8, 8))kernel = np.array([[1,1,1], [1,1,1], [1,1,1]])im_new = image_conv2d(imSrc, kernel)print(im_new) 输出结果为 12345678[[4. 6. 6. 6. 6. 6. 6. 4.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [6. 9. 9. 9. 9. 9. 9. 6.] [4. 6. 6. 6. 6. 6. 6. 4.]] 可以看出与boxfilter输出结果相同。接下来以3x3的窗口大小，分别以不同大小的单位矩阵来模拟不同分辨率的图像，对比两种运算随着分辨率的提升运算效率的变化情况 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# resolution defineresList = []resList.append((32,32))resList.append((64,64))resList.append((128,128))resList.append((256,256))resList.append((512,512))resList.append((640,480))resList.append((800,600))resList.append((1024,768))resList.append((1280,960))resList.append((1600,1200))resList.append((2048,1536))resList.append((2272,1704))resList.append((2560,1920))resList.append((3000,2000))resList.append((3264,2488))resList.append((4080,2720))resList.append((4536,3024))# convolution calculateconv_stats = []for res in resList: imSrc = np.ones(res) kernel = np.array([[1,1,1], [1,1,1], [1,1,1]]) start = time.time() im_new = image_conv2d(imSrc, kernel) runtimes = time.time()-start conv_stats.append(runtimes)# boxfilter calculatebxf_stats = []for res in resList: imSrc = np.ones(res) start = time.time() im_new = boxfilter(imSrc, 1) runtimes = time.time() - start bxf_stats.append(runtimes)# plotx = ['&#123;&#125;x&#123;&#125;'.format(res[0], res[1]) for res in resList]plt.plot(x, conv_stats, label='convolution')plt.plot(x, bxf_stats, label='boxfilter')plt.title('Comparison of convolution and boxfilter')plt.xlabel('radius')plt.ylabel('runtime(sec)')plt.xticks(np.arange(0, 20, step=2))plt.legend()plt.show() 测试结果如下 Resolution Convolution Runtimes(sec) Boxfilter Runtimes(sec) 32x32 0.015528 0.008734 64x64 0.023325 0.000000 128x128 0.088296 0.000972 256x256 0.351377 0.000971 512x512 1.396757 0.006795 640x480 1.637542 0.006794 800x600 2.556762 0.010676 1024x768 4.169978 0.019385 1280x960 6.543228 0.032032 1600x1200 10.234725 0.049505 2048x1536 16.729388 0.109683 2272x1704 20.630525 0.102891 2560x1920 26.146850 0.133951 3000x2000 31.713583 0.163045 3264x2488 43.213059 0.231047 4080x2720 59.047499 0.323232 4536x3024 72.816411 0.392182 将测试结果绘制曲线如下 可以看出，在相同局部大小3x3情况下，boxfilter的性能提升是巨大的，随着分辨率的增大，普通的卷积运算耗时呈类似指数增长，而boxfilter的耗时虽然也有所增加，但是其增长程度远远小于分辨率的增长程度 3x3大小的局部窗口下，boxfilter的运算效率确实很高，那么随着半径的增大，boxfilter的效果如何呢？下面对不同分辨率下不同半径的窗口大小对boxfilter进行了对比测试 1234567891011121314151617181920212223radList = [1,2,5,10,15,20,25,30,35,40,45,50]bxf_stats = []for res in resList: imSrc = np.ones(res) stat = &#123;'res':res, 'times':[]&#125; for rad in radList: if (rad*2+1)&lt;res[0] and (rad*2+1)&lt;res[1]: start = time.time() im_new = boxfilter(imSrc, rad) runtimes = time.time() - start stat['times'].append((runtimes, rad)) bxf_stats.append(stat)for i in range(len(bxf_stats)): x = [t[1] for t in bxf_stats[i]['times']] y = [t[0] for t in bxf_stats[i]['times']] plt.plot(x,y, label='&#123;&#125;'.format(bxf_stats[i]['res']))plt.title('Compare boxfilters with different radius sizes')plt.xlabel('radius')plt.ylabel('runtime(sec)')plt.legend()plt.show() 测试结果如下 Amazing!!! 图中每条曲线代表一种分辨率下随着半径增大的运行时间变化情况，可以看出，固定分辨率的情况下，boxfilter的运算时间与半径大小几乎完全无关，只有分辨率增大，其运算时间才稍有增加。从boxfilter的计算原理可以理解，由于其计算过程只涉及到两次矩阵行方向与纵方向的累积求和运算，以及行方向、列方向的减法运算，因此当分辨率增大时，累积求和与减法运算的运算量会有所增加，但是由于其运算过程与半径大小的关系仅在考虑边界处的处理时，而且也是单次计算，因此半径大小带来的运算量增加与分辨率增加带来的运算量增加是非常小的]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-03-Vivado Flow]]></title>
    <url>%2F2022%2F05%2F24%2FVitisAI-03-Vivado-Flow%2F</url>
    <content type="text"><![CDATA[从本文开始，将正式介绍VitisAI的工作流程。第一个流程就是Vivado Flow，在Vivado开发环境中创建一个硬件平台，最终的输出是xxx.xsa硬件描述文件，为后续的PetaLinux和Vitis提供基础。Vivado中的大部分的内容都是在Block Design中完成的，核心目标是创建一个Zynq UltraScale MPSoC的运行硬件环境，以及为DPU的正常运行提供硬件支持。这里需要注意，较老版本的VitisAI教程中，需要在Vivado中导入DPU的IP核，这种做法已经成为历史，本文介绍的流程中，在Vivado中是不需要导入DPU IP核的，只是对DPU运行环境进行支持，例如中断、时钟等 创建Vivado工程打开vivado 12source /opt/pkg/xilinx/Vivado/2021.1/settings64.shvivado vivado打开后，点击Create Project-&gt;Next 输入项目名称，这里我输入的是”dpu_hardware”，点击Next 在Project Type页面中选中RTL Project，并选中Project is an extensible Vitis platform，点击Next 在Default Part页面选择Family为Zynq UltraScale+ MPSoCs，Package为sfvc784，Temperature为E，Speed为-1，并在下方的表格中选中xczu2cg-sfvc784-1-e，点击Next，再点击Finish。这里需要注意，读者的芯片类型选择需要和自己的板子匹配 Block Design IP核添加在左侧PROJECT MANAGER→IP INTEGRATOR点击Create Block Design，在弹出框中直接点击Next创建一个Block Design 在Diagram界面中点击加号，在搜索框中输入zynq，选择Zynq UltraScale+MPSoC，将其添加到Block Design中 添加后如下图所示 添加Clock添加Clocking Wizard IP核 添加后如下图所示 双击Clocking Wizard进入配置页面，在Output Clocks中选中clk_out1、clk_out2、clk_out3，并将Output Freq分别设置为100、200、400 在Reset Type中选择Active Low，并点击OK 添加Processor System Reset添加Processor System Reset 因为有3路时钟，因此要创建3个对应的Reset模块，可点击添加的Reset，Ctrl+C/Ctrl+V复制2个出来 将Clocking Wizard的3路输出分别与Reset0/1/2的slowest_sync_clk管脚相连，将Clocking Wizard的locked与3个Reset的dcm_locked都相连 点击Run Connection Automation，在弹出框中将clk_in1选择为zynq的clk0，将3个reset都选择为zynq的resetn0，点击OK 完成后Block Design如下图 点击Platform Setup，选中Clock，将3个Clock都设置为Enable，ID号分别设置为0、1、2，并将clk_out2设置为default 添加AXI Interrupt Controller添加AXI Interrupt Controller IP核 双击AXI Interrupt Controller进入配置页面，将Interrupt Output Connection选择为Single，点击OK 双击ZYNQ进入配置页面，在PS-PL Configuration→PS-PL Interfaces→Master Interface中，确保两个FPD未选中，LPD选中 在PS-PL Configuration→General→Interrupts→PL to PS中，将IRQ0[0-7]选择为1，点击OK 点击自动连接，默认都选择auto，点击OK 整个Block Design如图 将axi_intc_0的irq连接到ZYNQ的pl_ps_irq0上，再点击图标刷新画布 在Platform Setup中将中断使能 选择AXI Port，按照下图对端口进行配置 Zynq SoC配置双击Zynq UltraScale MPSoC，打开其配置界面 I/O配置在I/O Configuration中，配置Bank0~Bank2电压为LVCMOS18，Bank3电压为LVCMOS33 在下方展开Low Speed→Memory Interfaces→QSPI，勾选QSPI，设置模式为Single，Data Mode选择x4，勾选Feedback Clk 展开SD，并勾选SD0，配置Slol Type为eMMC，Data Transfer Mode为8Bit，勾选Reset，选择MIO23 勾选SD1，Slot Type选择SD 2.0，Data Transfer Mode选择4Bit，勾选CD，用于检测SD卡的插入 勾选I2C 1，用于EEPROM 勾选UART1 勾选TTC 0~TTC3 配置PS端以太网，勾选GEM3，勾选MDIO3 勾选USB0，勾选USB3.0，选择GT Lane1，勾选USB Reset→USB 0 勾选PCIe 点击左侧Switch To Advanced Mode，选择PCIe Configuration Basic Settings→Device Port Type：RootPort Devices IDs→Initial ID Values→Subsystem ID：0x7 Devices IDs→Class Code→Base Class：0x06 Devices IDs→Class Code→Sub Class：0x04 回到I/O Configuration，勾选Display Port，Lane Selection选择Dual Higher 时钟配置在Clock Configuration界面，Input Clocks配置参考时钟，PCIe选择Ref Clk0，100MHz，Display Port选择Ref Clk2，27MHz，USB0选择Ref Clk1，26MHz 在Output Clocks窗口，如果不是IOPLL，改为IOPLL，保持一致 PL的时钟保持默认 Full Power部分，其他保持默认，将DP_VIDEO改为VPLL，DP_AUDIO和DP_STC改为RPLL 最下面的Interconnect修改如下 DDR配置在DDR Configuration窗口中，Load DDR Presets选择“DDR4_MICRON_MT40A256M16GE_083E” 生成bitBLOCK DESIGN→Sources→design_1，右键选择Create HDL Wrapper生成top文件 点击Generate Bitstream图标生成bit流，等待生成完成后，点击File→Export→Export Platform，选Next、选Hardware。选Pre-synthesis+include bitstream 设置平台名称等信息，点击Next、Finsih，可看到在dpu_hardware路径下生成了design_1_wrapper.xsa文件]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>DPU</tag>
        <tag>DeepLearning</tag>
        <tag>Vivado</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Secondary Program Loader(SPL)]]></title>
    <url>%2F2022%2F05%2F23%2FSecondary-Program-Loader-SPL%2F</url>
    <content type="text"><![CDATA[SPL在uboot的启动过程中是一个非常重要的概念，在uboot的启动相关代码中，可以看到很多与SPL相关判断和处理，了解SPL对于理解CPU以及整个系统的启动过程是很有帮助的。本文主要对以下内容进行介绍 什么是SPL，为什么需要SPL？ SPL在uboot编译层面是如何设计的？ 启动过程中SPL主要做了什么？ 为什么需要SPL？SPL全程是Secondary Program Loader，叫做第二阶段引导程序，这个”Secondary”第二阶段是怎么来的呢？uboot目前将整个启动过程设计为4个阶段 ROM Boot SPL uboot kernel 第一阶段ROM Boot是固化在SoC内部的一小段启动程序，一般由芯片厂商在出厂时固化好，一般不需要对这段代码关心，需要关心的是芯片的启动模式有哪些，如何设置启动模式，启动地址是多少。ROM Boot会引导SPL的启动，按照预先设置的启动模式，看是从SD卡、emmc还是Flash加载SPL到片上RAM中运行；SPL引导uboot的加载，将uboot拷贝到DDR中，重定向到uboot运行；uboot又加载kernel，重定向到kernel运行 以上这整个过程中，为什么不直接用ROM Boot引导uboot到RAM中，然后让uboot将kernel拷贝到DDR中再切换到kernel运行，而要多出来一个SPL多此一举呢？ 关键点在于一方面uboot慢慢发展，代码量及功能越来越多，另一方面SoC厂商由于程本、面积等方面的考虑，一般不会将片内RAM设计的太大，一般不会超过100KB，而uboot编译下来一般都会超过200KB，因此在RAM空间上就产生了矛盾。uboot为了规避这种问题，引入了SPL的概念，即将原本的uboot功能一分为二，输出2个独立的程序，一个是SPL，另一个是真正的uboot，SPL程序只包含CPU底层相关的关键启动代码，体积较小，能够完全运行在片上RAM中，SPL负责对DDR进行初始化，并将uboot拷贝到DDR中，切换到uboot运行。下图显示了不同启动阶段及引导程序的运行介质 CPU上电后，首先是从片内ROM加载ROM Boot程序，进行片上系统初始化，然后将启动介质中的SPL加载到片内RAM运行，SPL对DDR进行初始化，然后将uboot拷贝到DDR中，uboot在DDR中运行，拷贝kernel到DDR中 从编译的层面理解SPLSPL与真正的uboot在源代码上是同一套代码，通过编译选项CONFIG_SPL_BUILD进行区分，在uboot代码的很多地方，通过以下形式区分了SPL处理还是uboot处理 12345#ifdef CONFIG_SPL_BUILD /* spl process */#else /* uboot process */#endif 在uboot的Makefile设计里，将最终uboot生成的二进制输出文件分离为了u-boot-spl.bin和u-boot.bin，u-boot-spl.bin就是SPL程序的二进制文件，而u-boot.bin是真正uboot的二进制文件。从u-boot-spl.lds和u-boot.lds链接文件可以看出，SPL和真正uboot的启动都是从_start符号开始，_start-&gt;lowlevel_init-&gt;_main-&gt;board_init_f的代码调用流程，SPL和uboot都会走一遍，只是其中执行的内容不同 SPL主要内容这里以u-boot-2012.10版本源码为参考，主要分析ARMv7架构的SPL处理。下图是SPL整个运行过程调用时序 start.Sarch/arm/cpu/armv7/start.S，入口符号为_start 123456789.globl _start_start: b reset ldr pc, _undefined_instruction ldr pc, _software_interrupt ldr pc, _prefetch_abort ldr pc, _data_abort ldr pc, _not_used ldr pc, _irq ldr pc, _fiq _start地址为0x00000000，设置后续的几个地址为SPL的异常向量，跳转到reset 123456789reset: bl save_boot_params /* * set the cpu to SVC32 mode */ mrs r0, cpsr bic r0, r0, #0x1f orr r0, r0, #0xd3 msr cpsr,r0 对armv7的cpsr寄存器进行设置，设置CPU的模式为超级模式 12345678910#if !(defined(CONFIG_OMAP44XX) &amp;&amp; defined(CONFIG_SPL_BUILD)) /* Set V=0 in CP15 SCTRL register - for VBAR to point to vector */ mrc p15, 0, r0, c1, c0, 0 @ Read CP15 SCTRL Register bic r0, #CR_V @ V = 0 mcr p15, 0, r0, c1, c0, 0 @ Write CP15 SCTRL Register /* Set vector address in CP15 VBAR register */ ldr r0, =_start mcr p15, 0, r0, c12, c0, 0 @Set VBAR#endif 通过armv7的CP15协处理器将异常向量设置到VBAR，关于ARMv7协处理器CP15以及VBAR的详细内容后续会出其他相关文章专门介绍。这里进行了CONFIG_SPL_BUILD宏定义判断，说明设置这里的异常向量是SPL的操作，也就是说_start后面的几个异常处理是针对SPL程序而言的，uboot应该会设置其他的异常向量 1234#ifndef CONFIG_SKIP_LOWLEVEL_INIT bl cpu_init_cp15 bl cpu_init_crit#endif 这里是调用了cpu_init_cp15和cpu_init_crit两个函数 123456789101112131415161718192021222324252627ENTRY(cpu_init_cp15) /* * Invalidate L1 I/D */ mov r0, #0 @ set up for MCR mcr p15, 0, r0, c8, c7, 0 @ invalidate TLBs mcr p15, 0, r0, c7, c5, 0 @ invalidate icache mcr p15, 0, r0, c7, c5, 6 @ invalidate BP array mcr p15, 0, r0, c7, c10, 4 @ DSB mcr p15, 0, r0, c7, c5, 4 @ ISB /* * disable MMU stuff and caches */ mrc p15, 0, r0, c1, c0, 0 bic r0, r0, #0x00002000 @ clear bits 13 (--V-) bic r0, r0, #0x00000007 @ clear bits 2:0 (-CAM) orr r0, r0, #0x00000002 @ set bit 1 (--A-) Align orr r0, r0, #0x00000800 @ set bit 11 (Z---) BTB#ifdef CONFIG_SYS_ICACHE_OFF bic r0, r0, #0x00001000 @ clear bit 12 (I) I-cache#else orr r0, r0, #0x00001000 @ set bit 12 (I) I-cache#endif mcr p15, 0, r0, c1, c0, 0 mov pc, lr @ back to my callerENDPROC(cpu_init_cp15) cpu_init_cp15的处理是通过设置CP15寄存器来禁用TLB、指令和数据cache，禁用MMU及cache 12345678910ENTRY(cpu_init_crit) /* * Jump to board specific initialization... * The Mask ROM will have already initialized * basic memory. Go here to bump up clock rate and handle * wake up conditions. */ b lowlevel_init @ go setup pll,mux,memoryENDPROC(cpu_init_crit)#endif cpu_init_crit的处理是直接跳转到lowlevel_init，这定义在lowlevel_init.S中 lowlevel_init.Sarch/arm/cpu/armv7/lowlevel_init.S中只定义了lowlevel_init一个函数 123456789101112131415161718ENTRY(lowlevel_init) /* * Setup a temporary stack */ ldr sp, =CONFIG_SYS_INIT_SP_ADDR bic sp, sp, #7 /* 8-byte alignment for ABI compliance */ /* * Save the old lr(passed in ip) and the current lr to stack */ push &#123;ip, lr&#125; /* * go setup pll, mux, memory */ bl s_init pop &#123;ip, pc&#125;ENDPROC(lowlevel_init) 这里有一个保存栈的操作，先将ip入栈，然后调用了s_init()C函数，定义在xxx/board.c中，主要是初始化系统时钟等，然后返回到start.S中这里 123456/* Set stackpointer in internal RAM to call board_init_f */call_board_init_f: ldr sp, =(CONFIG_SYS_INIT_SP_ADDR) bic sp, sp, #7 /* 8-byte alignment for ABI compliance */ ldr r0,=0x00000000 bl board_init_f 调用lib/board.c中的board_init_f函数 lib/board.carch/arm/lib/board.c中定义了board_init_f 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183void board_init_f(ulong bootflag)&#123; bd_t *bd; init_fnc_t **init_fnc_ptr; gd_t *id; ulong addr, addr_sp;#ifdef CONFIG_PRAM ulong reg;#endif bootstage_mark_name(BOOTSTAGE_ID_START_UBOOT_F, "board_init_f"); /* Pointer is writable since we allocated a register for it */ gd = (gd_t *) ((CONFIG_SYS_INIT_SP_ADDR) &amp; ~0x07); /* compiler optimization barrier needed for GCC &gt;= 3.4 */ __asm__ __volatile__("": : :"memory"); memset((void *)gd, 0, sizeof(gd_t)); gd-&gt;mon_len = _bss_end_ofs;#ifdef CONFIG_OF_EMBED /* Get a pointer to the FDT */ gd-&gt;fdt_blob = _binary_dt_dtb_start;#elif defined CONFIG_OF_SEPARATE /* FDT is at end of image */ gd-&gt;fdt_blob = (void *)(_end_ofs + _TEXT_BASE);#endif /* Allow the early environment to override the fdt address */ gd-&gt;fdt_blob = (void *)getenv_ulong("fdtcontroladdr", 16, (uintptr_t)gd-&gt;fdt_blob); for (init_fnc_ptr = init_sequence; *init_fnc_ptr; ++init_fnc_ptr) &#123; if ((*init_fnc_ptr)() != 0) &#123; hang (); &#125; &#125;#ifdef CONFIG_OF_CONTROL /* For now, put this check after the console is ready */ if (fdtdec_prepare_fdt()) &#123; panic("** CONFIG_OF_CONTROL defined but no FDT - please see " "doc/README.fdt-control"); &#125;#endif debug("monitor len: %08lX\n", gd-&gt;mon_len); /* * Ram is setup, size stored in gd !! */ debug("ramsize: %08lX\n", gd-&gt;ram_size);#if defined(CONFIG_SYS_MEM_TOP_HIDE) /* * Subtract specified amount of memory to hide so that it won't * get "touched" at all by U-Boot. By fixing up gd-&gt;ram_size * the Linux kernel should now get passed the now "corrected" * memory size and won't touch it either. This should work * for arch/ppc and arch/powerpc. Only Linux board ports in * arch/powerpc with bootwrapper support, that recalculate the * memory size from the SDRAM controller setup will have to * get fixed. */ gd-&gt;ram_size -= CONFIG_SYS_MEM_TOP_HIDE;#endif addr = CONFIG_SYS_SDRAM_BASE + gd-&gt;ram_size;#ifdef CONFIG_LOGBUFFER#ifndef CONFIG_ALT_LB_ADDR /* reserve kernel log buffer */ addr -= (LOGBUFF_RESERVE); debug("Reserving %dk for kernel logbuffer at %08lx\n", LOGBUFF_LEN, addr);#endif#endif#ifdef CONFIG_PRAM /* * reserve protected RAM */ reg = getenv_ulong("pram", 10, CONFIG_PRAM); addr -= (reg &lt;&lt; 10); /* size is in kB */ debug("Reserving %ldk for protected RAM at %08lx\n", reg, addr);#endif /* CONFIG_PRAM */#if !(defined(CONFIG_SYS_ICACHE_OFF) &amp;&amp; defined(CONFIG_SYS_DCACHE_OFF)) /* reserve TLB table */ addr -= (4096 * 4); /* round down to next 64 kB limit */ addr &amp;= ~(0x10000 - 1); gd-&gt;tlb_addr = addr; debug("TLB table at: %08lx\n", addr);#endif /* round down to next 4 kB limit */ addr &amp;= ~(4096 - 1); debug("Top of RAM usable for U-Boot at: %08lx\n", addr);#ifdef CONFIG_LCD#ifdef CONFIG_FB_ADDR gd-&gt;fb_base = CONFIG_FB_ADDR;#else /* reserve memory for LCD display (always full pages) */ addr = lcd_setmem(addr); gd-&gt;fb_base = addr;#endif /* CONFIG_FB_ADDR */#endif /* CONFIG_LCD */ /* * reserve memory for U-Boot code, data &amp; bss * round down to next 4 kB limit */ addr -= gd-&gt;mon_len; addr &amp;= ~(4096 - 1); debug("Reserving %ldk for U-Boot at: %08lx\n", gd-&gt;mon_len &gt;&gt; 10, addr);#ifndef CONFIG_SPL_BUILD /* * reserve memory for malloc() arena */ addr_sp = addr - TOTAL_MALLOC_LEN; debug("Reserving %dk for malloc() at: %08lx\n", TOTAL_MALLOC_LEN &gt;&gt; 10, addr_sp); /* * (permanently) allocate a Board Info struct * and a permanent copy of the "global" data */ addr_sp -= sizeof (bd_t); bd = (bd_t *) addr_sp; gd-&gt;bd = bd; debug("Reserving %zu Bytes for Board Info at: %08lx\n", sizeof (bd_t), addr_sp);#ifdef CONFIG_MACH_TYPE gd-&gt;bd-&gt;bi_arch_number = CONFIG_MACH_TYPE; /* board id for Linux */#endif addr_sp -= sizeof (gd_t); id = (gd_t *) addr_sp; debug("Reserving %zu Bytes for Global Data at: %08lx\n", sizeof (gd_t), addr_sp); /* setup stackpointer for exeptions */ gd-&gt;irq_sp = addr_sp;#ifdef CONFIG_USE_IRQ addr_sp -= (CONFIG_STACKSIZE_IRQ+CONFIG_STACKSIZE_FIQ); debug("Reserving %zu Bytes for IRQ stack at: %08lx\n", CONFIG_STACKSIZE_IRQ+CONFIG_STACKSIZE_FIQ, addr_sp);#endif /* leave 3 words for abort-stack */ addr_sp -= 12; /* 8-byte alignment for ABI compliance */ addr_sp &amp;= ~0x07;#else addr_sp += 128; /* leave 32 words for abort-stack */ gd-&gt;irq_sp = addr_sp;#endif debug("New Stack Pointer is: %08lx\n", addr_sp);#ifdef CONFIG_POST post_bootmode_init(); post_run(NULL, POST_ROM | post_bootmode_get(0));#endif gd-&gt;bd-&gt;bi_baudrate = gd-&gt;baudrate; /* Ram ist board specific, so move it to board code ... */ dram_init_banksize(); display_dram_config(); /* and display it */ gd-&gt;relocaddr = addr; gd-&gt;start_addr_sp = addr_sp; gd-&gt;reloc_off = addr - _TEXT_BASE; debug("relocation Offset is: %08lx\n", gd-&gt;reloc_off); memcpy(id, (void *)gd, sizeof(gd_t)); relocate_code(addr_sp, id, addr); /* NOTREACHED - relocate_code() does not return */&#125; 维护了一个global data结构，计算uboot的地址及大小，初始化DDR，将uboot拷贝到DDR中，然后再调用relocate_code将uboot拷贝到DDR中，并跳转到uboot]]></content>
      <categories>
        <category>CS</category>
        <category>uboot</category>
      </categories>
      <tags>
        <tag>嵌入式</tag>
        <tag>uboot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VS+CUDA开发环境搭建]]></title>
    <url>%2F2022%2F05%2F19%2FVS-CUDA%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[本文介绍在Windows环境下，如何搭建Visual Studio + CUDA的开发环境 首先需要安装好Visual Studio 2015/2017/2019 CUDA版本确定在选择cuda安装时，cuda的版本需要和显卡型号对应算力进行匹配，例如GeForce GT 730算力为3.5。如何查看自己显卡的算力呢？安装了CUDA后可通过CUDA内置的deviceQuery工具输出显卡相关信息，”CUDA Capability Major/Minor version number”这一行的数字就是显卡算力的版本 cudav8.0、cudav9.0到cudav10.0都可以兼容最高3.5算力，可以在以下链接中选择不同的cuda版本 https://developer.nvidia.com/cuda-toolkit-archive 阅读在线文档中的”xxx Compatibility Guide”文档，其中显示的编译选项中显示了算力能力 compute_20表示算力2.0，compute_35表示算力3.5 下载安装对应cuda后，在cmd中输入nvcc -V可查看版本 hello world打开vs，文件→新建→NVIDIA→CUDA 8.0，新建一个test工程 创建项目后，工程会自动生成一个kernel.cu源码文件 点击项目→test属性 选中VC++目录→包含目录，为其添加CUDA的头文件路径 “$(CUDA_PATH)\include” 选中库目录，为工程添加CUDA库目录 “$(CUDA_PATH)\lib\x64” 选择Debug模式为x64 在main函数最后添加如下函数，暂停程序退出 1getc(stdin); 点击本地Windows调试器，运行程序 Nsight可视化调试手动运行Nsight Monitor，在电脑右下脚图标处，右键Nsight→options 设置以下选项为True 再次右击Nsight图标点击Exit 在vs中打开Nsight，也在option中选择以下内容为True 点击vs中的Nsight→Start Performance Analysis 出现如下界面 选中Trace Settings→System下面的几个选项和CUDA 点击下方中间状态灯的Launch 运行完毕后关闭程序，Nsight会自动显示Session Overview 显示了整个GPU运行的概要信息，左上角下拉框选择Timeline可显示时间线]]></content>
      <categories>
        <category>CS</category>
        <category>cuda</category>
      </categories>
      <tags>
        <tag>cuda</tag>
        <tag>gpu</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[HDR-曝光融合(Exposure Fusion)]]></title>
    <url>%2F2022%2F05%2F19%2FHDR-%E6%9B%9D%E5%85%89%E8%9E%8D%E5%90%88-Exposure-Fusion%2F</url>
    <content type="text"><![CDATA[Tom Mertens、Jan Kautz等人在”Exposure Fusion”这篇paper中提出了一种简单有效的曝光融合方法，通过将一组不同曝光的图像序列进行直接融合得到一副高质量图像，而不需要先将这些图像序列转化为HDR。传统的HDR做法采用多重曝光获得图像序列，需要将其转换为辐照度估计的radiance map，也就是HDR格式的高动态范围图像，然后再进行tone mapping，将高动态范围图像映射为低动态范围的显示设备能够显示的图像。本文对paper的方法进行介绍，并以python代码进行实作 paper介绍结果展示论文中以威尼斯水城的3张不同曝光图像序列作为输入，如下图所示 处理后的图像如下 输入序列中的3幅图像，第1幅曝光时间最短，天空和水面的纹理细节得以保留，而建筑物和船只的很多细节都丢失了。第2幅图像曝光时间适中，天空和水面仍有部分细节，建筑物细节和船只的细节增多。第3幅图曝光时间最长，天空过曝，水面有部分细节，建筑物和船只的细节最为充分。可以看出，经过融合之后的图像结合了3副图像各自的优势，每幅图像不同区域的细节都得以保留，而信息丢失的部分已经尽可能丢弃掉了，效果非常好 该方法的优势 pipeline得以简化：通常的HDR做法需要从多张曝光图像序列中合成高动态范围图像，需要恢复相机特定的响应曲线。另外，由于大多数显示设备动态范围有限，不能直接显示HDR图像，需要通过tone mapping压缩动态范围以适应显示设备的动态范围。而本方法则简单很多 不需要担心相机的校准问题，也不需要记录每张照片的曝光时间(响应曲线的校准需要知道每张照片的曝光时间) 曝光序列中可以允许过度曝光的部分，传统HDR方法由于要恢复响应曲线，存在过曝的图像不能使用 Exposure Fusion曝光融合通过只保留多次曝光图像序列中的“最佳”部分来计算所需的图像。什么是“最佳”是由一组质量度量指标来确定的，通过这些指标来对图像进行衡量并分配权重。最后合成的图像是利用拉普拉斯金字塔进行融合的。该paper中假定所有图像序列都是对齐的 质量衡量标准由于曝光不足和过度曝光，序列中会包含很多平坦和无色的区域。这样的区域权重应该尽可能少，而颜色鲜艳和细节丰富的区域应该得到充分保留 paper中提出了以下3种指标 对比度(Contrast)：将拉普拉斯滤波器应用到每张图像，并取滤波器响应的绝对值，将其作为对比度的简单指标C。对比度越高表明纹理和边缘越多，这部分像素的权重应该更高 饱和度(Saturation)：在每个像素位置上进行饱和度测量S，计算方式为R、G、B三通道的标准差 良好曝光(Well-exposedness)：希望每个像素位置良好曝光(既不要曝光不足也不要曝光过度)，即认为像素值接近中间为最好，例如归一化的像素值为0~1，那么0.5为最好，使用一个均值为0.5的高斯曲线来衡量像素值离0.5的距离，越近代表越好，得到指标E 每个像素不同度量信息相乘，得到每张图像的权重，使用一个幂函数来控制每个指标的影响程度 \begin{equation}W{ij,k} = (C{IJ,K})^{\omega {C}} \times (S{ij,k})^{\omega{S}} \times (E{ij,k})^{\omega_{E}}\end{equation} 其中$k$表示曝光序列中的第$k$次曝光，$i,j$表示像素位置，$W_{ij,k}$表示第$k$次曝光图像的像素$i,j$处的权重 融合计算每个像素的加权平均值来融合N个图像。需要对权重进行正则化 \begin{equation}\hat(W{ij,k}) = [\sum{\acute{k}=1}^{N}W{ij,\acute{k}}]^{-1}W{ij,k}\end{equation} paper中提到，如果使用简单的权重和图像进行加权平均，得到的融合图像并不让人满意，会在边界产生光晕和接缝问题。最终paper中选择拉普拉斯金字塔来进行多个图像的融合 普拉斯金字塔的详细介绍见另一篇文章DIP-图像金字塔 python实作首先导入依赖库 1234import cv2import copyimport numpy as npfrom matplotlib import pyplot as plt 读入图像并显示 1234567891011121314151617image_list = [ 'images/Balloon-01.jpg', 'images/Balloon-02.jpg', 'images/Balloon-03.jpg', 'images/Balloon-04.jpg',]images = np.stack([cv2.imread(name) for name in image_list])# show sequenceplt.figure(figsize=(16, 12))for i, im in enumerate(images): plt.subplot(2,2,i+1) im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) plt.xticks([]) plt.yticks([]) plt.title('image-0&#123;&#125;'.format(i)) plt.imshow(im) 定义对比度、饱和度和良好曝光的权重函数 123456789101112131415def contrast_weights(im): # 转为灰度图 gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) # 进行拉普拉斯算子处理 contrast = cv2.Laplacian(gray, cv2.CV_32F) # 取绝对值 weights = np.abs(contrast) return weightsdef saturation_weights(im): # 计算标准差 return im.std(axis=2, dtype=np.float32)def well_exposedmess_weights(im, sigma=0.2): return np.prod(np.exp(-((im - 0.5)**2)/(2*sigma)), axis=2, dtype=np.float32) 对图像序列进行权重计算并显示权重图片 123456789101112131415161718192021222324252627for i, im in enumerate(images): plt.figure(figsize=(12, 8)) # origin image plt.subplot(2,2,1) im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) plt.xticks([]) plt.yticks([]) plt.title('origin') plt.imshow(im) c_weights = contrast_weights2(im) s_weights = saturation_weights2(im) e_weights = well_exposedmess_weights2(im) plt.subplot(2,2,2) plt.xticks([]) plt.yticks([]) plt.title('contrast') plt.imshow(c_weights, cmap='gray') plt.subplot(2,2,3) plt.xticks([]) plt.yticks([]) plt.title('saturation') plt.imshow(s_weights, cmap='gray') plt.subplot(2,2,4) plt.xticks([]) plt.yticks([]) plt.title('well-exposedmess') plt.imshow(e_weights, cmap='gray') 图像1的原图、对比度图、饱和度图以及曝光良好图 图像2的原图、对比度图、饱和度图以及曝光良好图 图像3的原图、对比度图、饱和度图以及曝光良好图 图像4的原图、对比度图、饱和度图以及曝光良好图 可以看到对比度图能够提取图像明显的边缘细节，对比度图能够踢球色彩艳丽的区域，曝光良好图能够提取亮度适中的区域 定义曝光融合函数 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def exposure_fusion(images, measure_weights=(1.0, 1.0, 1.0), best_expo=0.5, sigma=0.2, layers_nr=7): # 图像转为float32，归一化 images = np.stack([im.astype('float32')/255 for im in images], axis=0) # 正则化函数 normalize = lambda x: x / np.expand_dims(np.sum(x, axis=0), axis=0) # 权重计算 weights = [] for i in range(len(images)): # 计算每幅图像的对比度 c_weights = contrast_weights2(images[i]) # 计算每幅图像的饱和度 s_weights = saturation_weights2(images[i]) # 计算每幅图像的曝光良好情况 e_weights = well_exposedmess_weights2(images[i]) # 对比度、饱和度、曝光良好融合 weight = np.power(c_weights, measure_weights[0]) * \ np.power(s_weights, measure_weights[1]) * \ np.power(e_weights, measure_weights[2]) #print(weight) weights.append(weight) #layers_show(weights) weights = np.stack(weights, axis=0) weights += 1e-12 # 权重归一化 weights = normalize(weights) # 像素值恢复到0~255区间 images *= 255 # 计算图像的拉普拉斯金字塔 images_LPyrs = [] for i in range(len(images)): LPyrs = laplace_pyramid(images[i], layers_nr) images_LPyrs.append(LPyrs) layers_info_print(images_LPyrs[0], 'images_LPyrs') # 计算权重的高斯金字塔 weight_GPyrs = [] for i in range(len(weights)): GPyrs = gaussian_pyramid(weights[i], layers_nr) weight_GPyrs.append(GPyrs) layers_info_print(weight_GPyrs[0], 'weight_GPyrs') # 计算融合的拉普拉斯金字塔 fused_LPyrs = [np.sum([images_LPyrs[k][n] * np.atleast_3d(weight_GPyrs[k][layers_nr - n -1]) for k in range(len(images))], axis=0) for n in range(layers_nr)] # 迭代融合 start = fused_LPyrs[0] #print('start shape:&#123;&#125;'.format(start.shape)) for i in range(1, layers_nr): upsample = cv2.resize(start, (fused_LPyrs[i].shape[1], fused_LPyrs[i].shape[0])) start = fused_LPyrs[i] + upsample start = np.clip(start, 0, 255).astype("uint8") plt.imshow(cv2.cvtColor(start, cv2.COLOR_BGR2RGB)) 融合结果如下 Reference Mertens, Tom, Jan Kautz, and Frank Van Reeth. “Exposure fusion.” Computer Graphics and Applications, 2007. PG’07. 15th Pacific Conference on. IEEE, 2007. 知乎-山与水你和我-exposure fusion 图像曝光融合 Tom Mertens github]]></content>
      <categories>
        <category>CV</category>
        <category>HDR</category>
      </categories>
      <tags>
        <tag>DIP</tag>
        <tag>CV</tag>
        <tag>HDR</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-图像金字塔]]></title>
    <url>%2F2022%2F05%2F18%2FDIP-%E5%9B%BE%E5%83%8F%E9%87%91%E5%AD%97%E5%A1%94%2F</url>
    <content type="text"><![CDATA[图像金字塔对于执行多尺度的编辑操作非常有效，能够在保持图像细节的同时进行融合。Peter J. Burt等人在1983年的”The Laplacian Pyramid as a Compact Image Code”中提出了拉普拉斯金字塔用于图像压缩，后来该方法被用于图像融合效果很好 本文首先对拉普拉斯金字塔进行介绍，然后对拉普拉斯用于图像融合进行介绍 Laplacian Pyramid拉普拉斯金字塔通过将原始图像减去一个低通滤波的图像来消除像素间的相关性，得到一个净数据压缩，仅保留了差异。将这种做法在多尺度上进行迭代，会得到一个这种差异的金字塔，由于这种做法相当于用拉普拉斯算子对图像进行多尺度采样，因此得到的金字塔叫做拉普拉斯金字塔 该方法用于数据压缩时，仅需保留金字塔最小分辨率(最低层次)的低通图像以及一个拉普拉斯金字塔，就可以完全恢复原始图像。在重建原始图像时，将最小分辨率的低通图像逐层与拉普拉斯金字塔进行结合，最终获得压缩前的原始图像 下图是整个拉普拉斯金字塔的构建和重建原始图像的示意图 该过程中主要涉及到以下几种处理 高斯滤波或高斯模糊(Gaussian Blur) 上采样(UpSample) 下采样(DownSample) 图像相加和相减 高斯滤波主要关注核大小与方差，下采样比较好做，按偶数或奇数行采样即可。上采样可以使用各种插值方法如三次样条插值等 拉普拉斯金字塔的生成拉普拉斯金字塔的生成需要依赖高斯金字塔，上图中的黄色部分是生成高斯金字塔的过程。”Image”表示原始图像，以3层金字塔为例，G0即原始图像，经过高斯滤波和下采样得到尺度缩小为1/4的低通图像G1，重复该过程得到G2、G3，[G0, G1, G2, G3]构成了4层的高斯金字塔 拉普拉斯金字塔的每层由对应层的高斯金字塔图像与高斯金字塔下层图像的上采样相减得到，例如L3是由1/4原始尺度的G1经过上采样，在与G0相减得到。L2是由G2上采样与G1相减得到。实际需要使用的拉普拉斯金字塔只有3层，即[L1, L2, L3] python代码如下 导入库 1234import cv2import copyimport numpy as npfrom matplotlib import pyplot as plt 定义高斯金字塔生成函数 123456789def gaussian_pyramid(im, layers): GPyrs = [im] for i in range(1, layers): # 高斯滤波 blur_im = cv2.GaussianBlur(GPyrs[i-1], (5,5), 0.83) # 下采样，取偶数行 downsample = blur_im[::2, ::2] GPyrs.append(downsample) return GPyrs 定义拉普拉斯金字塔生成函数 12345678910def laplace_pyramid(im, layers): GPyrs = gaussian_pyramid(im, layers) LPyrs = [GPyrs[-1]] for i in range(1, layers): # 上采样，使用opencv默认的采样方法 size = (GPyrs[layers-i-1].shape[1], GPyrs[layers-i-1].shape[0]) upsample = cv2.resize(GPyrs[layers-i], size) # 上层高斯图像与上采样后图像相减 LPyrs.append(GPyrs[layers-i-1]-upsample) return LPyrs 定义金字塔信息打印与显示函数 123456789101112131415161718192021cv2.namedWindow('img', cv2.WINDOW_NORMAL)dpi = 80def layers_show(layers): for i, layer in enumerate(layers): l = layer.astype('uint8') # 色彩通道转换，opencv默认是BGR im = cv2.cvtColor(l, cv2.COLOR_BGR2RGB) h, w, _ = im.shape figsize = w/float(dpi), h/float(dpi) fig = plt.figure(figsize=figsize) ax = fig.add_axes([0, 0, 1, 1]) ax.axis('off') plt.title('layer[&#123;&#125;] &#123;&#125;'.format(i, im.shape)) ax.imshow(im)def layers_info_print(layers, name): print('---- &#123;&#125;\'s info ----'.format(name)) for i, layer in enumerate(layers): print('layer[&#123;&#125;] shape:&#123;&#125;'.format(i, layer.shape)) print('\n') 读入图像生成4层的高斯金字塔并显示 12345im = cv2.imread('images/cat.jpeg')im.astype('float32')GPyrs = gaussian_pyramid(im)layers_info_print(GPyrs)layers_show(GPyrs) 12345---- GPyrs's info ----layer[0] shape:(607, 1080, 3)layer[1] shape:(304, 540, 3)layer[2] shape:(152, 270, 3)layer[3] shape:(76, 135, 3) G0层图像 G1层图像 G2层图像 G3层图像 生成4层拉普拉斯金字塔并显示 123LPyrs = laplace_pyramid(im, 4)layers_info_print(LPyrs, "LPyrs")layers_show(LPyrs) 12345---- LPyrs's info ----layer[0] shape:(76, 135, 3)layer[1] shape:(152, 270, 3)layer[2] shape:(304, 540, 3)layer[3] shape:(607, 1080, 3) L0层图像 L1层图像 L2层图像 L3层图像 可以看到拉普拉斯图像中的每幅图像主要是纹理特征 原始图像重建重建过程由最小分辨率开始的低通图像开始，经过上采样并与拉普莱斯金字塔对应分辨率图像相加，并经过多次向上迭代，最终得到原始图像并显示 1234567891011reconstruction = LPyrs[0]for i in range(1, 4): size = (LPyrs[i].shape[1], LPyrs[i].shape[0]) # 上采样，使用opencv默认的采样方法 upsample = cv2.resize(reconstruction, size) # 上采样后图像与拉普拉斯图像相加 reconstruction = LPyrs[i] + upsamplereconstruction = np.clip(reconstruction, 0, 255).astype('uint8')plt.axis('off')plt.title('reconstruction')plt.imshow(cv2.cvtColor(reconstruction, cv2.COLOR_BGR2RGB)) 结果如下图，能够正常复原原始图像 图像融合利用拉普拉斯金字塔进行图像融合是一个非常有趣的应用。融合后的图像在交界处的变化是非常平滑的，图像上的高频纹理混合得更快，可以避免 “鬼影”效果。为了创建融合图像，每幅图像首先生成自己的拉普拉斯金字塔，然后用一个二值掩模图像生成的高斯金字塔作为融合的权重金字塔，与两幅待融合的拉普拉斯金字塔进行结合，得到融合后的拉普拉斯金字塔，最后用融合后的拉普拉斯金字塔与最小分辨率的融合低通图像进行重建，最终得到融合的图像 加载左右两幅待融合图像以及二值掩模图像并显示 1234567layers = 7left = cv2.imread('images/blend/left.png')right = cv2.imread('images/blend/right.png')mask = cv2.imread('images/blend/mask.png', 0)plt.axis('off')plt.title('left')plt.imshow(cv2.cvtColor(left, cv2.COLOR_BGR2RGB)) 显示右图 123plt.axis('off')plt.title('right')plt.imshow(cv2.cvtColor(right, cv2.COLOR_BGR2RGB)) 显示二值掩模图像 123plt.axis('off')plt.title('mask')plt.imshow(mask, cmap='gray') 图像预处理 12345678910# 大小调整为256x256left = cv2.resize(left, (256, 256))right = cv2.resize(right, (256, 256))mask = cv2.resize(mask, (256, 256)) # 数值定点化为int16，由于拉普拉斯计算减法存在负值left = left.astype('int16')right = right.astype('int16')mask = mask.astype('float32') / 255# 由于mask为灰度图，将其转化为3通道mask = np.stack([mask, mask, mask], axis=-1) 计算left、right、mask的高斯金字塔，left和right的拉普拉斯金字塔 1234567891011# 生成高斯金字塔left_G = gaussian_pyramid(left, layers)right_G = gaussian_pyramid(right, layers)mask_G = gaussian_pyramid(mask, layers)# 生成互补的二值掩模mask_G_r = []for mask in mask_G: mask_G_r.append(1.0 - mask)# 生成拉普拉斯金字塔left_L = laplace_pyramid(left, layers)right_L = laplace_pyramid(right, layers) 打印各金字塔尺度 12345layers_info_print(left_G, 'right_G')layers_info_print(right_G, 'right_G')layers_info_print(mask_G, 'mask_G')layers_info_print(left_L, 'left_L')layers_info_print(right_L, 'right_L') 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748---- left_G's info ----layer[0] shape:(256, 256, 3)layer[1] shape:(128, 128, 3)layer[2] shape:(64, 64, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(16, 16, 3)layer[5] shape:(8, 8, 3)layer[6] shape:(4, 4, 3)---- right_G's info ----layer[0] shape:(256, 256, 3)layer[1] shape:(128, 128, 3)layer[2] shape:(64, 64, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(16, 16, 3)layer[5] shape:(8, 8, 3)layer[6] shape:(4, 4, 3)---- mask_G's info ----layer[0] shape:(256, 256, 3)layer[1] shape:(128, 128, 3)layer[2] shape:(64, 64, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(16, 16, 3)layer[5] shape:(8, 8, 3)layer[6] shape:(4, 4, 3)---- left_L's info ----layer[0] shape:(4, 4, 3)layer[1] shape:(8, 8, 3)layer[2] shape:(16, 16, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(64, 64, 3)layer[5] shape:(128, 128, 3)layer[6] shape:(256, 256, 3)---- right_L's info ----layer[0] shape:(4, 4, 3)layer[1] shape:(8, 8, 3)layer[2] shape:(16, 16, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(64, 64, 3)layer[5] shape:(128, 128, 3)layer[6] shape:(256, 256, 3) 生成融合拉普拉斯金字塔 12345blend_L = []for i in range(layers): l = (mask_G_r[layers-i-1])*left_L[i] + (mask_G[layers-i-1])*right_L[i] blend_L.append(l.astype('int16'))layers_info_print(blend_L, 'blend_L') 12345678---- blend_L's info ----layer[0] shape:(4, 4, 3)layer[1] shape:(8, 8, 3)layer[2] shape:(16, 16, 3)layer[3] shape:(32, 32, 3)layer[4] shape:(64, 64, 3)layer[5] shape:(128, 128, 3)layer[6] shape:(256, 256, 3) 计算最小分辨率低通融合图像 12345start = (mask_G_r[-1])*left_G[-1] + (mask_G[-1])*right_G[-1]start = start.astype('int16')plt.axis('off')plt.title('start')plt.imshow(cv2.cvtColor(start.astype('uint8'), cv2.COLOR_BGR2RGB)) 进行融合 1234567891011blend = startfor i in range(1, layers): size = (blend_L[i].shape[1], blend_L[i].shape[0]) # 上采样 upsample = cv2.resize(blend, size) # 上采样图像与融合拉普拉斯对应层相加 blend = blend_L[i] + upsample blend = np.clip(blend, 0, 255).astype('uint8')plt.axis('off')plt.title('blend')plt.imshow(cv2.cvtColor(blend, cv2.COLOR_BGR2RGB)) 最终融合后的图像如下]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
      <tags>
        <tag>DIP</tag>
        <tag>图像融合</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-02-环境与资源]]></title>
    <url>%2F2022%2F05%2F04%2FVitisAI-02-%E7%8E%AF%E5%A2%83%E4%B8%8E%E8%B5%84%E6%BA%90%2F</url>
    <content type="text"><![CDATA[在第一篇文章”VitisAI-01-Overview”中，我简要介绍了什么是VitisAI、VitisAI相关的的技术栈、我的开发环境是什么样的以及需要安装下载哪些资源。本文对我的开发环境以及所需资源的下载安装进行一个更为详细的说明 环境介绍我使用的台式机操作系统是Windows11，我采用的是在虚拟机中搭建VitisAI的方式来进行开发和研究。由于VitisAI开发需要PetaLinux编译，而目前PetaLinux仅支持Linux操作系统，而我并不使用Linux作为我的主操作系统，因此选择在虚拟机中搭建Linux系统的方式。我的系统配置详细信息如下 主操作系统：Windows 11 专业版(22000.613) 处理器：12th Gen Intel(R) Core(TM) i7-12700K 3.61 GHz 虚拟机：VMware Workstation 16.2.2 操作系统：Ubuntu20.0.4 处理器：内核数8 内存：12GB 磁盘：400GB 这里需要注意的是虚拟机中处理器相关的设置，截图如下 处理器数量选择1，每个处理器的内核数量选择8，因此总内核数为8。虚拟化引擎选中了”虚拟化 Inter VT-x/EPT 或 AMD-V/RVI(V)”。按照我的处理器设置整个VitisAI的开发过程实测是没有问题的。主要容易出问题的地方是PetaLinux编译，因为PetaLinux的多线程编译与内核数量相关，内核数量越多，可同时执行的编译线程越多，8个内核同时只能进行8个编译线程。曾经尝试过将处理器数量调整到12或16以加快PetaLinux的编译，但是编译到一定进度后总会发生停住不动的问题(查看CPU占用率基本为0，表明CPU未分给编译线程资源)，这里并没有搞清楚到底是什么原因，可能是12代CPU使用了Alder Lake的大小核架构与VMware Workstation不兼容问题 资源下载及安装虚拟机软件VMware Workstation与Ubuntu20.0.4虚拟机的下载安装并不包含在内。这里主要介绍与VitisAI相关的各个开发工具的下载与安装。其中Vivado和Vitis 2021.1和2021.2都可以，PetaLinux建议使用2021.2 Xilinx Unified Installer子2019年之后，Xilinx将软件开发平台进行了统一，其后的软件开发平台称作Vitis统一开发平台，Xilinx Unified Installer就是统一开发平台的安装包，这里推荐下载2021.2版本，打开地址Vitis下载，可以看到如下内容 点击2021.2，下翻页面到如下位置，点击”赛灵思统一安装程序 (Xilinx Unified Installer 2021.2) SFD (TAR/GZIP - 71.9 GB)”进行下载 下载下来是一个.tar.gz的压缩包，将其移动到虚拟机中，解压后运行其中的xsetup即可开始安装，安装过程是UI界面的，选择Vitis，会安装Vivado和Vitis以及一系列相关依赖库，然后选择安装路径即可自动安装 我的安装路径为”/home/opt/pkg/Xilinx”，安装完成后，可通过如下命令打开Vivado来检查是否安装成功 12source /home/opt/pkg/Xilinx/Vivado/2021.2/setting64.shvivado PetaLinuxPetaLinux的安装文件可单独下载，也可以使用Xilinx Unified Installer进行安装。使用Xilinx Unified Installer安装时仅需要在安装步骤的第一步选择PetaLinux即可。推荐使用Xilinx Unified Installer进行安装 我的PetaLinux安装路径为”/home/opt/pkg/Xilinx/PetaLinux”，PetaLinux安装完成后，可通过如下命令来查看PetaLinux是否安装成功 12source /home/opt/pkg/Xilinx/PetaLinux/2021.2/setting64.shpetalinux -h PetaLinux依赖库由于PetaLinux编译需要很多额外的依赖库，Xilinx为PetaLinux提供了Release Note来说明不同系统中需要依赖哪些库，打开PetaLinux下载地址 在右侧旁边有对应版本的发布说明，点击它，并在打开的页面中下拉到最下方，下载”2021.2_PetaLinux_Package_List.xlsx” 用Excel打开该文件，内容如下 其中有Ubuntu系统下的依赖库安装命令 1sudo apt-get install iproute2 gawk python3 python build-essential gcc git make net-tools libncurses5-dev tftpd zlib1g-dev libssl-dev flex bison libselinux1 gnupg wget git-core diffstat chrpath socat xterm autoconf libtool tar unzip texinfo zlib1g-dev gcc-multilib automake zlib1g:i386 screen pax gzip cpio python3-pip python3-pexpect xz-utils debianutils iputils-ping python3-git python3-jinja2 libegl1-mesa libsdl1.2-dev pylint3 PetaLinux离线编译由于PetaLinux在编译过程中需要从网络上下载很多第三方包，而很多第三方包的资源在外网中，编译速度受限于网络下载速度。Xilinx提供了离线的第三方包缓存文件，将其下载到本地后，在配置PetaLinux时选择编译方式为离线编译，即可不依赖网络进行离线编译，加快编译速度。离线缓存文件下载位置仍然在PetaLinux下载地址页面，下拉到最下方，选择”aarch64 sstate-cache (TAR/GZIP - 20.27 GB)”和”下载下载 (TAR/GZIP - 57.4 GB)” Vitis AI仓库Vitis AI相关库和工具在Xilinx官方提供的github仓库中，需要将其克隆到本地，该仓库较大，网络不佳克隆可能需要较长时间，命令为 1git clone https://github.com/Xilinx/Vitis-AI/tree/master Vitis AI dockerdocker添加到用户组 123sudo usermod -aG docker &lt;username&gt;sudo systemctl restart dockersudo chmod 666 /var/run/docker.sock VitisAI docker镜像下载 12cd Vitis-AIdocker pull xilinx/vitis-ai:latest 运行docker镜像 1./docker_run.sh xilinx/vitis-ai 显示如下界面表示运行成功 安装主机交叉编译12cd Vitis-AI/setup/mpsoc/VART./host_cross_compiler_setup.sh 安装后运行以下命令测试是否安装正确 123source ~/petalinux_sdk_2021.2/environment-setup-cortexa72-cortexa53-xilinx-linuxcd Vitis-AI/demo/VART/resnet50bash –x build.sh]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>AI加速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python从PDF提取图片]]></title>
    <url>%2F2022%2F04%2F29%2FPython%E4%BB%8EPDF%E6%8F%90%E5%8F%96%E5%9B%BE%E7%89%87%2F</url>
    <content type="text"><![CDATA[阅读PDF格式的论文或者一些书籍时，经常希望将PDF文件中的图片提取出来，市面上有些PDF格式转换的工具可以做到，但是很多都要收费。通过Python的pymupdf库可以完成该功能 安装pymupdf 1pip install pymupdf 这里要注意的是安装的库是pymupdf，但是实际使用的是pymupdf安装时带的fitz模块 首先是需要加载的库 12345import osimport reimport timeimport fitzimport tqdm 然后是定义pdf2image函数 1234567891011121314151617181920212223242526272829303132def pdf2image(pdf_path, img_path): pdf_name = pdf_path.split('.pdf')[0] start_time = time.time() count = 0 checkXO = r"/Type(?= */XObject)" # 判断是否为XObject的正则 checkIM = r"/Subtype(?= */Image)" # 判断是否为图像的正则 doc = fitz.open(pdf_path) # 获取pdf的对象树 xref_length = doc.xref_length() # 获取对象树的对象实例数 print("processing &#123;&#125; pages:&#123;&#125; objs:&#123;&#125;".format( pdf_path, len(doc), xref_length - 1)) # 遍历每个对象 for i in tqdm.tqdm(range(1, xref_length)): text = doc.xref_object(i) isXObject = re.search(checkXO, text) isImage = re.search(checkIM, text) if not isXObject or not isImage: continue count += 1 # 将对象转换为像素图 pix = fitz.Pixmap(doc, i) new_name = pdf_name.replace('\\', '_') + "img&#123;&#125;.png".format(count) new_name = new_name.replace(':', '') if pix.n &lt; 4: pix.save(os.path.join(img_path, new_name)) else: pix0 = fitz.Pixmap(fitz.csRGB, pix) pix0.save(os.path.join(img_path, new_name)) pix0 = None end_time = time.time() print("running time:&#123;&#125;s".format(end_time - start_time)) print("get &#123;&#125; images".format(count)) 定义一个从当前目录获取所有pdf文件列表的函数 1234567def get_pdf_files(): files = os.listdir() pdf_files = [] for f in files: if f.find('.pdf') &gt;= 0: pdf_files.append(f) return pdf_files 定义main函数 12345678910111213141516if __name__ == "__main__": pdf_files = get_pdf_files() print('PDF文件列表:') for f in pdf_files: print(f) # 循环对每个pdf文件进行图片提取 for f in pdf_files: pdf_name = f.split('.pdf')[0] img_dir = "&#123;&#125;-images".format(pdf_name) # 为每个pdf创建一个 pdfname-images 的文件夹 if not os.path.exists(img_dir): os.mkdir(img_dir) pdf2image(f, img_dir) input('Enter') 运行程序，以”MUSIQ: Multi-scale Image Quality Transformer”论文为例，获取到了如下图片]]></content>
      <categories>
        <category>CS</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VitisAI-01-Overview]]></title>
    <url>%2F2022%2F04%2F25%2FVitisAI-01-Overview%2F</url>
    <content type="text"><![CDATA[研究Vitis AI好几个月了，终于实现了在VitisAI平台上部署神经网络模型并成功运行的目标，搞清楚了在整个全流程中如何做自定义，这里的自定义主要包括以下几个方面 硬件平台自定义：这里的硬件平台指的不是PCB板级自定义，而是在Xilinx FPGA芯片上的硬件工程自定义。由于不同人不同项目使用的FPGA芯片架构、型号各有不同，为不同的FPGA芯片准备一个DPU可以运行的硬件平台是非常重要的 DPU自定义：模型最终依赖DPU运行，而DPU的编译依赖FPGA芯片资源，不同FPGA芯片资源能力不同，因此需要针对所用的FPGA芯片资源来定制DPU参数 模型自定义：官方仅给出了适用于固定demo板型号的固定数量的预训练模型，而如果所用FPGA平台不是对应demo板的型号，或者想运行自己的模型，需要对模型进行量化压缩编译 本系列将对使用Vitis AI进行神经网络应用加速进行全流程的讲解，阅读本系列，您可以了解到以下内容 进行Vitis AI开发的环境要求是什么，有哪些资源方面的要求？ Vitis AI开发需要下载安装哪些依赖工具，什么才是最为正确和高效的配置方式？ 什么是Vitis AI，Vitis AI的软件栈如何使用？ 什么是DPU，如何在FPGA上部署DPU，DPU的各个参数是什么含义？ 自己做的模型，如何在Vitis AI上做量化和编译，并正确的运行？ 如何使用Vitis AI提供的性能分析工具，对所运行的模型进行分析？ Vitis AI 介绍Vitis AI是Xilinx推出的专用于其硬件平台的AI推理加速开发环境，能够同时适用于边缘计算设备和数据中心加速卡。Vitis AI包含了从底层的IP核、编译器、运行时到C++/Python封装库、性能分析工具、预训练模型、模型量化压缩工具等一系列全栈内容，如下图所示 上层是通用的深度学习框架：Caffe、PyTorch和Tensorflow，这一部分并不是VitisAI的内容，模型首先在PC端使用通用的深度学习框架进行训练和评估，Vitis AI使用生成的模型文件 Vitis AI Models ZooModels Zoo为开发者提供了一组预优化的深度学习模型，方便开发者进行快速开发。Models Zoo提供了大量的主流深度学习模型，例如用于分类的VGG、Resnet、Inception、Moblienet，用于检测的RCNN、SSD、YOLOv2、YOLOv3，用于图像分割的Enet、Seqnet、FPN、DeeplabV3+，用于姿态估计的Openpose、Coordinates regression等 Optimizer提供了世界领先的模型压缩技术，可以将模型复杂度降低5至50倍，而精度损失非常小。但是该功能需要购买商业许可 Quantizer网络模型量化工具，将32位浮点模型转化为INT8。量化后的模型能够减小很多内存和带宽需求，比浮点模型具备更快的访存效率和更高的能效 Compiler编译器将模型映射为高效的指令集和数据流模型，可以执行复杂的优化，如层融合、指令调度和尽可能多地重用片上内存 Profiler能够对AI应用进行信息收集和可视化分析，可以进行网络逐层分析以快速发现性能瓶颈 Library一组高层次统一封装API，支持C++和Python，用于AI模型的高效推理 Runtime运行时使应用程序能够为云和边缘使用统一的高级运行时API，从而实现云到边缘的无缝和高效部署 DPU深度学习处理单元(Deep Learning Processor Unit, DPU)是针对深度神经网络优化的可编程引擎，是实现在FPGA中进行AI加速最核心的部件。经过Vitis AI Compiler编译后的模型能够以DPU专用指令集在DPU上运行。Vitis AI对DPU进行了预实现，开发者只需要将其当作一个IP核来使用即可。DPU是可配置的，开发者可自由的在性能与资源占用之间进行权衡，在资源充足的情况下，开发者可以在一个FPGA芯片中部署多个DPU，以实现性能的最大化。不同架构的芯片适用于不同的DPU，Xilinx推出了多个型号的DPU DPUCZDX8G：用于Zynq UltraScale+ MPSoC DPUCAHX8H：用于Alveo U50LV/U55C Card DPUCADF8H：用于Alveo U200/U250 Card DPUCVDX8G：用于Versal AI Core Series 开发环境介绍强烈建议使用PetaLinux2021.2版本！！！首先，非常强烈建议的一点是使用PetaLinux2021.2版本，这是目前PetaLinux最新版本，也是我非常推荐的版本。我曾经使用过2020.1、2020.2、2021.1版本，这几个版本由于历史开发原因，和Vitis AI以及Ubuntu系统适配上做的并不是很好，导致我研究几个月时间里，有超过半数的时间是在反复进行PetaLinux的编译和解决编译中出现的各种各样的错误，而为了解决这些错误去查阅各种PetaLinux和Vitis AI的Issues对于研究Vitis AI来说，并没有什么帮助和收获。因此，我建议使用PetaLinux2021.2版本来进行Linux系统的编译，使用该版本几乎不会出现什么问题，整个编译过程是非常顺畅和迅速的 我的系统配置介绍我使用的主机CPU为Intel 12700K，运行Windows11操作系统，Vitis AI开发环境是安装和运行在Ubuntu虚拟机上，以下是虚拟机各资源分配介绍 CPU：8核，不知道是由于虚拟机对12代CPU适配问题还是什么原因，如果我把核心数量调整为更多(12或16)，在编译PetaLinux过程中必定会出现编译过程停住不动的问题，8核可以正常编译 内存：16G，这个看主机配置情况，分配8G也是可以正常使用的。内存大小主要会影响模型量化时的方式，后文模型量化部分会讲到，不同的量化方式对系统运行内存有不同的要求 磁盘：400G，磁盘大小是非常重要的，主要占用磁盘的部分如下 Vitis统一开发环境，Vivado、Vitis和PetaLinux的安装，需要占用100G+ 如果希望PetaLinux编译更快，需要下载离线缓存包进行离线编译，离线缓存包占用100G+ Vitis AI git仓库和进行模型量化编译的Docker镜像，占用100G+ Vivado工程、Vitis工程、PetaLinux工程及Linux镜像编译过程的中间文件，占用几十个G Ubuntu版本：Ubuntu20.0.4 我使用的FPGA平台是Alinx出的Zynq UltraScale MPSoC XCZU2CGB开发板 需要下载和安装的内容预先安装好的VMware Workstation和安装Ubuntu20.0.4虚拟机就不介绍了，主要需要下载以下内容 Xilinx Unified Installer 2021.2：Vitis统一安装程序，包括了Vitis和Vivado PetaLinux 2021.2 安装程序 Ubuntu虚拟机中安装PetaLinux依赖库 PetaLinux离线缓存包(sstate-cache)，仅在需要进行离线编译时下载 downloads_2021.2.tar.gz sstate_aarch64_2021.2.tar.gz 克隆Vitis AI github仓库 在Vitis AI中下载安装Vitis AI Docker 我的虚拟机中路径介绍 123456789101112131415161718|--home |--opt |--sstate-cache 离线缓存包路径 |--download |--sstate_aarch64 |--pkg |--Xilinx |--Vivado Vivado开发工具 |--Vitis Vitis开发工程 |--PetaLinux PetaLinux开发工具 |--dev |--xilinx |--Vitis-AI Vitis-AI github仓库 |--meta-vitis-ai |--dpu_custom |--dpu_vivado Vivado工程 |--dpu_vitis Vitis工程 |--dpu_plnx PetaLinux工程 详细的安装步骤介绍，见本系列其他文章 开发流程介绍Vitis AI开发流程分为平台流程和应用流程两部分，平台流程一次性构建好后，就不再需要变动，应用流程是需要按功能需求变动的部分 平台流程平台流程的最终目标是生成可运行于目标平台的Linux镜像，以及DPU运行二进制文件 Vivado流程：在Vivado中进行Block Design配置，配置好PS侧的时钟、复位、各外设MIO、PCIE、DDR等，并为DPU提供中断、复位和时钟等资源，最终生成硬件描述文件.xsa PetaLinux流程：以Vivado流程生成的硬件描述文件为基础，配置Linux内核及rootfs各项功能，配置Yocto的离线编译，最终编译生成内核镜像及根文件系统 Vitis流程 平台工程：以硬件描述文件和PetaLinux编译生成的根文件系统生成Vitis平台工程 DPU应用工程：以Vits平台工程、PetaLinux编译生成的内核镜像编译生成DPU运行的二进制文件以及合并后的SD卡镜像 应用流程应用流程主要是模型变动和修改所需做的工作 深度学习框架流程：Caffe、PyTorch或Tensorflow上训练生成模型文件 模型量化流程：使用Vitis AI的量化工具对模型文件进行量化，生成量化模型 模型编译流程：使用Vitis AI的编译工具对量化模型进行编译，生成可部署在DPU上的模型]]></content>
      <categories>
        <category>AI</category>
        <category>VitisAI</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
        <tag>Vitis AI</tag>
        <tag>DPU</tag>
        <tag>DeepLearning</tag>
        <tag>AI加速</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[论文精读-Resnet]]></title>
    <url>%2F2022%2F04%2F24%2F%E8%AE%BA%E6%96%87%E7%B2%BE%E8%AF%BB-Resnet%2F</url>
    <content type="text"><![CDATA[何凯明等人在2015年的”Deep Residual Learning for Image Recognition”这篇论文中提出了Resnet网络架构，是CNN图像领域一个重要的里程碑，后续多年直至如今在图像分类领域仍然有超过半数的网络使用着Resnet或其变种。Resnet提出了一种残差块结构，解决了在深度神经网络中由于梯度消失或梯度爆炸导致的网络难以训练的问题，以及更深的层次反而导致网络性能下降问题 深度神经网络的训练问题梯度爆炸和梯度消失深度神经网络可以设置很深的层次结构，深层结构可以得到更高层级的语义特征。但是随着网络层数越来越深，训练过程会出现梯度消失和梯度爆炸的问题，训练难以进行 梯度消失和梯度爆炸是同一个问题，主要原因是由于SGD反向传播是以链式法则进行梯度的反向连乘传导，而随着网络层数加深，梯度将以指数形式传播，很容易将梯度大幅度放大或缩小，从而导致梯度消失或爆炸 在这篇文章发布之前，业界通常的做法主要有以下两方面 使用较好的权重初始化方式，让权重分布更合理 使用BatchNormalization，在层与层之间做正则化，避免出现特别大或特别小的层 使用了这些技术之后，确实能够在一定程度上解决训练不收敛的问题 网络越深性能反而下降解决了训练不收敛问题后，仍然存在的一个问题是，随着网络层数的加深，性能不增反降 上图是文章在CIFAR-10数据集上分别以一个20层和56层深度的模型进行训练，并绘制训练误差和测试误差曲线。从图中可以看出，56层的模型训练误差和测试误差反而更高。这种现象并不是发生了过拟合，而是网络训练不动了 假设有一个层数较浅的网络shallower和一个在shallower基础上增了若干层后较深的网络deeper。如果在shallower网络性能还不错的情况下，理论上deeper网络最差也是和shallower一样，不应该比shallower更差，因为可以将deeper和shallower相同层数的层看作是shallower的一个拷贝，即图中deeper网络中黄色层部分的权重参数和shallower完全一致，而将剩余的蓝色部分看作是一个identity mapping，即这些层什么都不干，输入是什么，输出还是什么，这样的话，deeper网络的性能和shallower性能应该完全相同 但是SGD无法学习和找到这样的权重 Resnet网络架构Residual结构Resnet显式的构造了一个identity mapping，使得更深的网络不会比浅层网络的性能更差。假设有一个网络，现在给它增加一个层，假设之前层的输出是$x$，按照正常的方式处理，新增加的层可以看作是学习一个$H(x)$的映射，如下图所示 而现在，不让新增加的层直接学习$H(x)$，而是学习一个$F(x)=H(x)-x$的映射，其含义是新增加的层不要直接从前面的网络已经学习到的输出$x$去再学习，而是学习前面网络已经学到的东西和label之间的残差。这样使得新增加的整个结构Residual学习的映射变换为$H(x) = F(x)+x$。如下图所示，新增加的层New layer，New layer本身学习的是$F(x)$，而Residual结构是New layer的输出加前面网络的输出$x$ Residual结构和正常增加一个层唯一需要变动的就是增加了一个上层网络输出$x$和新增层输出$F(x)$的shortcut connection，如图中红色箭头部分。这个结构不会增加任何要学习的参数，也不会让计算复杂的变高 文章指出，这种Residual结构的网络非常容易优化，而且随着网络层数越深，网络性能越好 残差连接shape问题当残差连接的输入和输出shape不相等时，文章提供了两种将输入和输出变换为相同shape的方法 输入和输出都用0补充到相同shape 使用一个1x1 stride=2的卷积，将输入和输出shape匹配 文章在实验中对比了这两种方案，使用第二种方案效果更好 不同版本的Resnet文章给出了以下5种不同大小的Resnet模型 Resnet18 Resnet34 Resnet50 Resnet101 Resnet152 不同大小的Resnet版本后跟的数字表示的是网络的层数，例如Resnet18表示整个网络有10层。5个版本网络架构的第一层7x7的卷积层、第二层的maxpool和最后一层全连接层都是相同的，不一样的是中间的残差块Block部分，各个残差块之间使用了残差连接。这里有两种类型的残差块Basic和Bottleneck，下图中用不同颜色区分开了，Resnet18和Resnet34使用的是一种残差块，而Resnet50、Resnet101和Resnet152使用的是另一种残差块 当网络层数为18和34时，使用的残差块是Basic，大于等于50层后，使用的残差块是Bottleneck 当网络层数更多时，feature的维度比较大，能够学习到更多特征，为了降低计算复杂度，在Bottleneck中，使用了(1x1, 64)的卷积层将256维的feature投影回64维，经过(3x3, 64)卷积后，再经过(1x1, 256)的卷积投影回256维度 这里需要注意，在一种Resnet版本种，每个Block表示一个残差块组，一个组中的残差块使用相同的特征维度在残差块之间传递。图中Block旁边标识了该Block中有多少个重复的残差块。不同残差块组的特征维度是不同的，Resnet18和Resnet34的特征维度从上到下分别是64、128、256、512，Resnet50、Resnet101和Resnet152的特征维度分别是256、512、1024、2048 Resnet实验结果文章对比了两种相同层数(18和34层)的网络版本，plain-18和plain-34是未使用残差结构的网络，ResNet-18和ResNet-34是使用了残差结构的网络。下图显示了测试对比结果，图中较粗的曲线是验证误差，较细的曲线是训练误差 对比plain和ResNet的结果可以看出，使用了残差结构的网络误差低于未使用残差结构的网络，且更深的层数能够取得更好的性能，训练时收敛速度也更快 文章在CIFAR-10上还做了一个110层模型的实验，对比了使用残差块和未使用残差块模型中训练收敛时不同层是否还在起作用，实验表明Resnet网络在前若干层已经训练收敛后，后面的层不会起太多作用，这样使得网络不会因为层的加深反而性能变差，而未使用残差块的网络较深的层仍然在其比较大的作用 原理正常的网络假设是$g(x)$，在其后增加一些层，则网络变为$f(g(x))$，根据链式法则，对该网络对$x$求导为 \begin{equation}\frac{df(g(x))}{dx} = \frac{df(g(x))}{dg(x)} \cdot \frac{dg(x)}{dx}\end{equation} 因为深层网络的梯度链是很长的连乘，当网络逐渐收敛时，梯度本身比较小，长的连乘很容易导致最终输出的梯度为0。在Resnet的情况下，增加一些层后，网络变为$f(g(x)+x)$，对$x$求导变为 \begin{equation}\frac{df(g(x)+x)}{dx} = \frac{df(g(x))}{dg(x)} \cdot \frac{dg(x)}{dx} + \frac{dg(x)}{dx}\end{equation} 多出来的$\frac{dg(x)}{dx}$这一项会使得整个梯度变大一些，更容易训练]]></content>
      <categories>
        <category>AI</category>
        <category>论文精读</category>
      </categories>
      <tags>
        <tag>CNN</tag>
        <tag>Resnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[台大李宏毅机器学习2021-homework3]]></title>
    <url>%2F2022%2F04%2F23%2F%E5%8F%B0%E5%A4%A7%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02021-homework3%2F</url>
    <content type="text"><![CDATA[homework3的任务是创建一个用于图像分类的CNN网络，其中可能会用到一些优化技巧。任务包含以下3个级别的要求 Easy：Baseline模型，由作业提供，直接训练可以达到44.862%的精度 Middle：需要使用数据增强技术或者优化网络结构(不可以使用预训练好的网络架构，不能使用额外的数据集)，以进行性能的提升，目标精度52.807% Hard：使用提供的未标记数据获得更好的结果，目标精度82.138% Dataset作业使用的数据集是特别处理过的food-11数据集，其中收集了11个食物类别 训练集：280个已标记数据和6786个未标记数据 验证集：60已标记数据 测试集：3347个数据 其中的类别编码为 00：披萨、鸡肉卷、汉堡、土司、面包 01：黄油、芝士、奶酪 02：蛋糕、马卡龙、 03：鸡蛋、煎蛋 04：油炸类食物 05：烤肉 06：意面 07：米饭、炒饭 08：生蚝、扇贝、三文鱼、北极贝 09：各种汤 10：蔬菜水果 Baseline首先加载各个依赖模块 12345678910import torchimport torchvisionimport torchvision.transforms as transformsimport torch.nn as nnimport numpy as npimport matplotlib.pyplot as pltfrom torchvision.datasets import DatasetFolderfrom torch.utils.data import ConcatDataset, DataLoader, Subsetfrom PIL import Imagefrom tqdm.auto import tqdm 数据准备数据变换定义数据转换方式为仅进行resize操作 123456789train_tfm = transforms.Compose([ transforms.Resize((128, 128)), transforms.ToTensor(),])test_tfm = transforms.Compose([ transforms.Resize((128, 128)), transforms.ToTensor(),]) DataLoader设置batchsize为16，训练集和验证集进行shuffle操作，测试集不进行shuffle操作 12345678910batch_size = 16train_set = DatasetFolder("food-11/training/labeled", loader=lambda x: Image.open(x), extensions="jpg", transform=train_tfm)valid_set = DatasetFolder("food-11/validation", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)unlabeled_set = DatasetFolder("food-11/training/unlabeled", loader=lambda x: Image.open(x), extensions="jpg", transform=train_tfm)test_set = DatasetFolder("food-11/testing", loader=lambda x: Image.open(x), extensions="jpg", transform=test_tfm)train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)valid_loader = DataLoader(valid_set, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False) 模型定义作业给出的基线模型为3个卷积层加3个FC，每个卷积层后跟一个BN、一个ReLU和一个MaxPool 1234567891011121314151617181920212223242526272829class MyModel(nn.Module): def __init__(self): super(Classifier, self).__init__() # input image size: [3, 128, 128] self.cnn_layers = nn.Sequential( # Conv2D_1 nn.Conv2d(3, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # Conv2d_2 nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2, 0), # Conv2d_3 nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(4, 4, 0), ) def forward(self, x): # input (x): [batch_size, 3, 128, 128] # output: [batch_size, 11] x = self.cnn_layers(x) x = x.flatten(1) x = self.fc_layers(x) return x 模型结构如下图所示 Training损失函数使用交叉熵，优化器使用Adam，优化器学习率为0.0003，训练批次为80 123456789101112131415161718192021222324252627282930313233343536373839404142device = 'cpu'model = MyModel().to(device)model.device = devicecriterion = nn.CrossEntropyLoss()optimizer = torch.optim.Adam(model.parameters(), lr=0.0003, weight_decay=1e-5)n_epochs = 80for epoch in range(n_epochs): model.train() train_loss = [] train_accs = [] for batch in tqdm(train_loader): imgs, labels = batch logits = model(imgs.to(device)) loss = criterion(logits, labels.to(device)) optimizer.zero_grad() loss.backward() grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10) optimizer.step() acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean() train_loss.append(loss.item()) train_accs.append(acc) train_loss = sum(train_loss) / len(train_loss) train_acc = sum(train_accs) / len(train_accs) print(f"[ Train | &#123;epoch + 1:03d&#125;/&#123;n_epochs:03d&#125; ] loss = &#123;train_loss:.5f&#125;, acc = &#123;train_acc:.5f&#125;" model.eval() valid_loss = [] valid_accs = [] for batch in tqdm(valid_loader): imgs, labels = batch with torch.no_grad(): logits = model(imgs.to(device)) loss = criterion(logits, labels.to(device)) acc = (logits.argmax(dim=-1) == labels.to(device)).float().mean() valid_loss.append(loss.item()) valid_accs.append(acc) valid_loss = sum(valid_loss) / len(valid_loss) valid_acc = sum(valid_accs) / len(valid_accs) print(f"[ Valid | &#123;epoch + 1:03d&#125;/&#123;n_epochs:03d&#125; ] loss = &#123;valid_loss:.5f&#125;, acc = &#123;valid_acc:.5f&#125;") 经过80批次训练后，验证集精度为0.47173 12[ Train | 080/080 ] loss = 0.04817, acc = 0.98478[ Valid | 080/080 ] loss = 5.32257, acc = 0.47173 Loss曲线如下图所示，可以看出网络其实并没有收敛，验证集误差还在增大 Middleoptimize1-数据增强对训练集和验证集使用以下数据增强技术 随机水平/垂直翻转 随机旋转 亮度/对比度/饱和度变换 123456789101112train_tfm = transforms.Compose([ # 随机亮度、对比度、饱和度 transforms.ColorJitter(brightness=0.25, contrast=0.25, hue=0.25), # 随机水平翻转 transforms.RandomHorizontalFlip(p=0.5), # 随机垂直翻转 transforms.RandomVerticalFlip(p=0.5), # 随机旋转 transforms.RandomRotation((-90, 90)), transforms.Resize((128, 128)), transforms.ToTensor(),]) optimize2-使用Resnet18直接使用pytorch中定义好的Resnet18模型进行训练，但是不加载训练权重参数 1model = torchvision.models.resnet18(pretrained=False) 进行80个批次训练后，结果如下 12[ Train | 080/080 ] loss = 0.76552, acc = 0.74223[ Valid | 080/080 ] loss = 1.38644, acc = 0.59524 验证集精度达到了59.524%，训练曲线如下 Hard使用半监督学习技术Semi-Supervised Learning，每次训练时从unlabeled数据中筛选以当前模型输出结果最大置信度超过某个阈值的样本，加入到labeled数据中，构成新的数据集。 模型使用resnet50，训练结果 12[ Train | 080/080 ] loss = 0.70109, acc = 0.76370[ Valid | 080/080 ] loss = 1.42967, acc = 0.57589]]></content>
      <categories>
        <category>AI</category>
        <category>台大李宏毅机器学习2021</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>CNN</tag>
        <tag>Resnet</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-高斯滤波器]]></title>
    <url>%2F2022%2F04%2F22%2F%E9%AB%98%E6%96%AF%E6%BB%A4%E6%B3%A2%E5%99%A8%2F</url>
    <content type="text"><![CDATA[简单的盒式滤波器有很多局限性，在涉及精细细节或者强几何分量的应用中，并不适用。高斯滤波器相比盒式滤波器更有优势 高斯核定义如下 \begin{equation}w(s,t)=Ke^{-\frac{s^{2} + t^{2}}{2 \sigma^{2}}}\end{equation} 高斯核是唯一可分离的圆对称核 可分离性如果二维函数$G(x,y)$可表示为两个一维函数的乘积$G{1}(x)G{2}(x)$，则称它是可分离的。对于空间滤波器而言，可分离的核能够表示为两个向量的外积 \begin{equation}w = uv^{T}\end{equation} 由于卷积满足交换律和结合律，可分离性可以带来卷积运算复杂度上的降低。假设图像$f$大小为$m \times n$，卷积核$w$大小为$a \times b$，如果按照正常的卷积运算，需要$mnab$次卷积运算。如果卷积核为可分离的，那么可将$w$分离为$w{1}$和$w{2}$，$f \ast w$可拆分为$f \ast w{1} \ast w{2}$，和$w{1}$的运算需要$mna$次卷积，其结果和$w{2}$运算需要$mnb$次卷积，因此可分离核卷积与不可分离核卷积运算量比为 \begin{equation}C = \frac{mn(a+b)}{mnab} = \frac{a+b}{ab}\end{equation} 判断一个滤波器是否为可分离的，只需确定其秩是否为1，如果为1，表明是可分离的 圆对称性圆对称性也叫做各向同性，即其响应与方向无关，这意味着卷积操作不会带来方向性的负面效果。 以下仿真产生一个$5 \times 5, K=1, \sigma=1$的高斯核 123456789101112import numpy as npdef gaussian_kernel(x, y, k, sigma): return k * np.exp(-(x**2+y**2)/(2*sigma**2))kernel = np.zeros((11, 11))for i in range(kernel.shape[0]): for j in range(kernel.shape[1]): x = np.abs(i-5) y = np.abs(j-5) kernel[i, j] = round(gaussian_kernel(x, y, 1, 1), 3)print(kernel) 1234567891011[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0. 0. 0.002 0.007 0.011 0.007 0.002 0. 0. 0. ] [0. 0. 0.002 0.018 0.082 0.135 0.082 0.018 0.002 0. 0. ] [0. 0. 0.007 0.082 0.368 0.607 0.368 0.082 0.007 0. 0. ] [0. 0. 0.011 0.135 0.607 1. 0.607 0.135 0.011 0. 0. ] [0. 0. 0.007 0.082 0.368 0.607 0.368 0.082 0.007 0. 0. ] [0. 0. 0.002 0.018 0.082 0.135 0.082 0.018 0.002 0. 0. ] [0. 0. 0. 0.002 0.007 0.011 0.007 0.002 0. 0. 0. ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ] [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. ]] 可以看到，核中心值为1，与中心距离相同的位置值相同 3$\sigma$高斯函数的性质存在一个3倍$\sigma$位置，距均值距离超过$3\sigma$的点数值上非常小，如果选择高斯核大小为$6\sigma \times 6 \sigma$，那么得到的结果与使用任意比其大的高斯核处理结果相同。因此，如果$\sigma=7$，则应使用$43 \times 43$的高斯核 高斯函数的乘积和卷积仍是高斯函数贝叶斯估计就是源于此，两个高斯分布相乘仍是高斯分布，新的高斯分布方差更小，均值更大 因此，多个高斯核的卷积处理可以合并为1个高斯核的卷积 两个高斯核卷积后的均值等于两个均值的和，由于用高斯做卷积时均值为0，因此均值项可以不考虑 \begin{equation}m{f \ast g}=m{f} + m_{g}\end{equation} 卷积前后方差的关系为 \begin{equation}\sigma{f \ast g} = \sqrt{\sigma{f}^{2}+ \sigma_{g}^{2}}\end{equation} 如果多个高斯核的大小都是$m \times m$，则合并的核大小$W \times W$为 \begin{equation}W = Q \times (m-1 + m)\end{equation} 其中Q为高斯核数量 Python Code123456789101112131415161718192021222324252627282930313233343536import matplotlib.pyplot as pltimport numpy as npimport cv2 as cvdef conv2d_kernel(matrix, kernel): n = len(kernel) ans = 0 for i in range(n): for j in range(n): ans += matrix[i,j]*float(kernel[i,j]) return ansdef image_conv2d(im, kernel): n = len(kernel) # padding size: 2*(kernelsize - 1) im_padding = np.zeros((im.shape[0]+2*(n-1), im.shape[1]+2*(n-1))) # im data copy to padding matrix im_padding[(n-1):(n+im.shape[0]-1), (n-1):(n+im.shape[1]-1)] = im im_conv = np.zeros((im_padding.shape[0]-n+1, im_padding.shape[1]-n+1)) for i in range(im_padding.shape[0]-n+1): for j in range(im_padding.shape[1]-n+1): neighbor = im_padding[i:i+n, j:j+n] im_conv[i, j] = conv2d_kernel(neighbor, kernel) pos = int((n-1)/2) im_new = im_conv[pos:(pos+im.shape[0]), pos:(pos+im.shape[1])] return im_newdef gaussian_kernel_create(size, k, sigma): kernel = np.zeros((size, size)) center = (size-1)/2 for i in range(kernel.shape[0]): for j in range(kernel.shape[1]): x = np.abs(i-center) y = np.abs(j-center) kernel[i, j] = k * np.exp(-(x**2+y**2)/(2*sigma**2)) return kernel/np.sum(kernel) 分别以$7 \times 7, \sigma=1$和$21 \times 21, \sigma=3.5$创建两个高斯核 12kernel_1 = gaussian_kernel_create(7, 1, 1)kernel_2 = gaussian_kernel_create(21, 1, 3.5) 显示原始图像 12aussian_kernel_create(21, 1, 3.5)im = cv.imread('images/02.tif', 0) $7 \times 7$卷积核处理后的图像 12im_out1 = image_conv2d(im, kernel_1)plt.imshow(im_out1, cmap='gray') $21 \times 21$卷积核处理后的图像 12im_out2 = image_conv2d(im, kernel_2)plt.imshow(im_out2, cmap='gray') 以$21\times 21, \sigma=1$创建高斯核，可以看到处理结果与$7 \times 7, \sigma=1$处理结果基本一致 123kernel_3 = gaussian_kernel_create(21, 1, 1)im_out3 = image_conv2d(im, kernel_3)plt.imshow(im_out3, cmap='gray')]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
      <tags>
        <tag>DIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-中值滤波器]]></title>
    <url>%2F2022%2F04%2F22%2F%E4%B8%AD%E5%80%BC%E6%BB%A4%E6%B3%A2%E5%99%A8%2F</url>
    <content type="text"><![CDATA[中值滤波器属于非线性滤波器，其响应基于滤波器所在区域像素的排序中值来决定，中值滤波器对椒盐噪声有比较优秀的降噪能力，同时相比于线性平滑滤波器对图像的模糊程度小很多 中值滤波器计算过程为，对图像某点执行中值滤波，首先要对邻域内像素进行排序，确定它们的中值，并将中值赋值给滤波后的中心像素，例如在一个 $3 \times 3$ 滤波器中，中值是第5大的值，在$5 \times 5$滤波器中，中值是第13大的值 下面以被椒盐噪声污染的电路板图像为例，展示中值滤波的效果 123456789101112131415161718192021import numpy as npimport cv2 as cvfrom matplotlib import pyplot as pltdef median_kernel(matrix): return int(np.median(matrix))def median_filtering(im, kernel_size): n = kernel_size # padding im_padding = np.zeros((im.shape[0]+2*(n-1), im.shape[1]+2*(n-1))) # im data copy to padding matrix im_padding[(n-1):(n+im.shape[0]-1), (n-1):(n+im.shape[1]-1)] = im im_conv = np.zeros((im_padding.shape[0]-n+1, im_padding.shape[1]-n+1)) for i in range(im_padding.shape[0]-n+1): for j in range(im_padding.shape[1]-n+1): neighbor = im_padding[i:i+n, j:j+n] im_conv[i, j] = median_kernel(neighbor) pos = int((n-1)/2) im_new = im_conv[pos:(pos+im.shape[0]), pos:(pos+im.shape[1])] return im_new 读取原始图像并显示 123im = cv.imread('images/01.tif', 0)plt.title('origin')plt.imshow(im, cmap='gray') 进行中值滤波处理 1234im_k3 = median_filtering(im, 3)plt.figure()plt.title('kernel size: 3')plt.imshow(im_k3, cmap='gray')]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
      <tags>
        <tag>DIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-Convolution]]></title>
    <url>%2F2022%2F04%2F22%2FConvolution%2F</url>
    <content type="text"><![CDATA[卷积操作在图像处理中非常重要，大部分空域滤波都需要进行卷积操作。卷积运算是在卷积核(kernel)与图像邻域之间进行的一种变换操作，并以图像为参照系，逐像素移动卷积核的中心，从左向右，从上到下，最终获得经过卷积处理后的图像 空间相关与卷积严谨的说，以上对于卷积操作的描述并不准确，这实际上是空间相关(Spatial Correlation)的过程，大小为$m \times n$的核$w(s,t)$与图像$f(x,y)$的相关$(w\bigstar f)(x,y)$定义为 \begin{equation}(w \bigstar f)(x,y) = \sum{s=-a}^{a}\sum{t=-b}^{b}w(s,t)f(x+s, y+t)\end{equation} 其中，$a=(m-1)/2$，$b=(n-1)/2$，且$m、n$都为奇数。相关运算存在核旋转特性，以下进行示例说明 定义一个$5 \times 5$的测试图像im，仅在中心像素点im[2][2]值为1，其余像素值全0，将其与一个$3 \times 3$的kernel进行运算，kernel的值为0-9顺序排列，可以观察到相关运算的结果是产生了一个旋转180°的kernel 首先定义相关运算 1234567891011121314151617181920212223242526import numpy as np# define kernel calculatedef sum_kernel(matrix, kernel): n = len(kernel) sum = 0 for i in range(n): for j in range(n): sum += matrix[i,j]*kernel[i,j] return sum# define correlation calculatedef image_correlation(im, kernel): n = len(kernel) # padding size: 2*(kernelsize - 1) im_padding = np.zeros((im.shape[0]+2*(n-1), im.shape[1]+2*(n-1))) # im data copy to padding im im_padding[(n-1):(n+im.shape[0]-1), (n-1):(n+im.shape[1]-1)] = im im_conv = np.zeros((im_padding.shape[0]-n+1, im_padding.shape[1]-n+1)) for i in range(im_padding.shape[0]-n+1): for j in range(im_padding.shape[1]-n+1): neighbor = im_padding[i:i+n, j:j+n] im_conv[i, j] = conv2d_kernel(neighbor, kernel) pos = int((n-1)/2) im_new = im_conv[pos:(pos+im.shape[0]), pos:(pos+im.shape[1])] return im_new 创建测试图像数据 123im = np.zeros((5, 5))im[2,2] = print(im) 12345[[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 1. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]] 创建kernel 12kernel = np.array([[1,2,3], [4,5,6], [7,8,9]])print(kernel) 123[[1 2 3] [4 5 6] [7 8 9]] 进行相关运算 12im_new = image_correlation(im, kernel)print(im_new) 12345[[0. 0. 0. 0. 0.] [0. 9. 8. 7. 0.] [0. 6. 5. 4. 0.] [0. 3. 2. 1. 0.] [0. 0. 0. 0. 0.]] 因此，在进行卷积运算时，需要将卷积核旋转180°，卷积运算的定义为 \begin{equation}(w \ast f)(x,y) = \sum{s=-a}^{a}\sum{t=-b}^{b}w(s,t)f(x-s, y-t)\end{equation} 如果卷积核是关于中心对称时，相关和卷积运算结果相同，也就不需要进行旋转了 Padding以$3 \times 3$的卷积核为例，由于在像素$f(x,y)$处的卷积运算需要$f(x-1,y-1),f(x+1,y+1)$等邻域的值，卷积核只能在图像边界内与边界重合移动，如果不进行填充操作，边界处的像素无法计算卷积(红色边框的像素)，卷积最终得到的图像大小将小于原始图像 因此如果希望卷积后的图像大小与原始图像一致，就需要进行边界填充操作。大小为$n \times n$的卷积核与大小为$m \times m$的图像进行卷积，每个边需要向外填充$(n-1)$的像素，整个填充后图像大小为$m+2(n-1)$。如下图，左侧为$8 \times 8$的原始图像，填充图像为$10 \times 10$大小，绿色为填充的部分，每边向外填充了2个宽度的像素，经过卷积运算后大小为$8 \times 8$]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
      <tags>
        <tag>DIP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[DIP-直方图均衡化]]></title>
    <url>%2F2022%2F04%2F16%2F%E7%9B%B4%E6%96%B9%E5%9B%BE%E5%9D%87%E8%A1%A1%E5%8C%96%2F</url>
    <content type="text"><![CDATA[直方图(Histogram)定义图像灰度级为$r_{k}$，其中$k=0,1,2,…,L-1$，则未归一化的直方图定义为 \begin{equation}h(r{k})=n{k}\end{equation} 其中$n{k}$是图像在灰度级$r{k}$的像素数量，归一化的直方图定义为 \begin{equation}p(r{k})=\frac{h(r{k}}{MN}=\frac{n_{k}}{MN}\end{equation} 其中$M$和$N$是图像的长和宽 直方图均衡化(Histogram Equalization)直方图均衡化是一种强度转换映射，这种映射需要满足以下两点条件 $T(r)$在$0 \le r \le L-1$上是单调递增的 在$0 \le r \le L-1$上满足$0 \le T(r) \le L-1$ 单调递增是为了避免映射过程出现像素值的反转，更苛刻条件是严格单调递增，因为单调递增存在多对一的映射关系，如果要求从直方图均衡化转换回来， 单调递增的多对一映射关系是有问题的，必须使用严格单调递增 直方图均衡化的目的是通过某种强度变换函数，使得变换后的图像灰度级均匀分布，展现更多细节和动态范围。图像的强度可以看作是在$[0, L-1]$范围内的随机变量。定义$p{r}(r)$和$p{s}(s)$分别表示变换前后图像的强度概率密度函数(PDF)。在概率论中，如果$p{r}(r)$和$T(r)$是已知的，且$T(r)$在整个范围内连续可微，那么$p{r}(r)$到$p_{s}(s)$的映射可定义为 \begin{equation}p{s}(s) = p{r}(r)\left|\frac{dr}{ds}\right|\end{equation} 给出如下变换函数 \begin{equation}s = T(r) = (L-1)\int{0}^{r}p{r}(\omega)d\omega\end{equation} 积分部分是随机变量$r$的累积分布函数(CDF)。由于PDF总为正，积分是递增的，满足条件1。当积分上限$r=L$时，积分值为1，因此最大值为$L-1$，满足条件2。现在来推导一下$\frac{ds}{dr}$，以下推导用到了莱布尼兹法则：定积分对其上限值的导数等于被积函数在上限处的值 \begin{equation}\frac{ds}{dr} = \frac{dT(r)}{dr} \= (L-1)\frac{d}{dr}\int{0}^{r}p{r}(\omega)d\omega \= (L-1)p_{r}(r)\end{equation} 将结果带回到变换中 \begin{equation}p{s}(s) = p{r}(r)\left|\frac{dr}{ds}\right| \= p{r}(r)\left|\frac{1}{L-1}p{r}(r)\right|\= \frac{1}{L-1}\end{equation} 可以看到，经过这个变换函数，变换后的图像概率密度函数在每个灰度级概率分布均匀 对于离散变量，可以使用求和来代替连续变量中的积分 \begin{equation}s{k} = T(r{k}) = (L-1)\sum{j=0}^{k}p{r}(r_{j}) k=0,1,2,…,L-1\end{equation} 直方图匹配直方图均衡化由于结果的可预测性以及实现较为简单，在自动增强中是一个好的选择。但是有些应用场景中并不适合，尤其是期望经过处理后的图像直方图是某种特定分布时。生成具有特定直方图分布的方法叫做直方图匹配 如何将一个直方图分布变换到任意分布呢？假定期望的目标分布图像为$z$，根据直方图均衡化的定义，同样也存在$z$到$s$的均衡化映射，定义$s=G(z)$为 \begin{equation}S = G(z) = (L-1)\int{0}^{z}p{z}(v)dv\end{equation} 那么可以得到从$z$到$r$的关系 \begin{equation}z=G^{-1}(s)=G^{-1}[T(r)]\end{equation} 由以上公式推断，可以看出，得到一个指定强度分布的图像可以通过如下步骤 从输入图像$s$得到强度等级的PDF$p_{r}(r)$ 使用目标分布的PDF$p_{z}(z)$得到G(z) 计算反变换z=G^{-1}(z) 通过输入图像得到均衡化的输出图像，并对均衡化输出图像的每个像素值进行z=G^{-1}(s)的逆变换映射 在连续变量中，逆变换并不好求得，但是在离散变量的处理中，实际上是不需要计算$G^{-1}$的，因为在进行$s=G(z)$映射时，所有强度等级的变换都记录在一个映射表中，逆映射时只需要从表中找出最接近的索引位置即可 例如以下例子中，映射数组tab将强度等级0~7映射到1,2,2,3,4,5,5,6，tab=[1,2,2,3,4,5,5,6]。要从5这个强度等级逆变换到映射前的强度等级，因为tab[5]和tab[6]都为5，选取最小的index=5就是逆变换后的值 Python Code1234import cv2 as cvimport numpy as npfrom PIL import Imageimport matplotlib.pyplot as plt 读取并显示图像信息 1234im = Image.open('images/02.tif').convert('L')pixels_nr = im.width * im.heightpixels_max = 256im.show() 直方图计算函数 12345678def histogram(im): hist = np.zeros((pixels_max)) data = np.array(im) for i in range(data.shape[0]): for j in range(data.shape[1]): v = data[i][j] hist[v] += 1 return hist 绘制直方图 12hist = histogram(im)plt.bar(range(len(hist)), hist) 直方图均衡化函数 12345678910111213141516def histogram_equalization(im): r_data = np.array(im) # calculate hist hist = histogram(im) # calculate pdf pdf = hist / pixels_nr # calculate cdf cdf = pdf.cumsum() # generate map pmap = cdf * (pixels_max - 1) pmap = pmap.astype(np.uint8) # create new image s_data = np.zeros((r_data.shape[0], r_data.shape[1]), dtype=np.uint8) # mapping input image to new image s_data = pmap[r_data] return Image.fromarray(s_data) 进行直方图均衡化处理 12out = histogram_equalization(im)out.show() 绘制均衡化后的图像直方图分布 12out_hist = histogram(out)plt.bar(range(len(out_hist)), out_hist)]]></content>
      <categories>
        <category>CV</category>
        <category>DIP</category>
      </categories>
      <tags>
        <tag>DIP</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[FreeRTOS+Trace]]></title>
    <url>%2F2022%2F04%2F11%2FFreeRTOS-Trace%2F</url>
    <content type="text"><![CDATA[1. 概述多任务的实时操作系统例如 FreeRTOS，为了满足实时性的要求，在任务的时间片划分、切换等方面做了非常严格和复杂的控制。当应用场景中多任务的功能、时序、交互较为复杂时，要分析系统整体的运行情况是一件非常棘手的事情。由于多任务操作系统存在切换代码堆栈空间的操作，因此通过各种调试器的断点、单步等调试手段无法进行函数执行跟踪，也无法进行实时性调试。通过调用系统 API 获取任务各项状态并打印输出的方式也无法保证实时性。通常的调试手段是将系统运行各种信息暂存在 RAM 中，并在需要的时刻导出为文件 (dump)，并通过一个可视化软件离线进行分析 FreeRTOS 提供了 FreeRTOS+Trace 的跟踪调试方案，该方案是 FreeRTOS 官方合作商 Percepio 开发的一种运行时诊断和优化工具，可以捕获系统运行时有价值的信息，然后在可视化的图形界面中展示这些信息，界面支持多种类型的视图，且视图间支持同步联动。在分析、排除故障或优化 FreeRTOS 应用程序性能时，这种调试方案是非常有用的 本文对FreeRTOS+Trace的调试方案进行一个基本的介绍，通过阅读本文，您可以了解到以下内容 使用FreeRTOS+Trace需要准备什么环境，如何下载及安装相关工具 FreeRTOS+Trace是如何工作的 如何导出跟踪数据并用可视化工具打开进行分析 如何衡量RAM占用与记录时长的关系，如何进行有效的设置以减少RAM占用量 如何在Xilinx Microblaze 中使用FreeRTOS+Trace方案 2. TracealyzerPercepio 提供了一个桌面端应用程序 Tracealyzer，用于在 PC 机上对导出后的跟踪数据进行可视化分析。同时安装 Tracealyzer 后，在其安装路径下也会携带一个需要在目标平台编译构建的跟踪库 TraceRecorder 2.1 下载安装Tracealyzer 的下载地址为https://percepio.com/downloadform/，由于 Percepio 提供的 Tracealyzer 支持多种 RTOS，因此在下载时需要注意选择目标系统为 FreeRTOS，输入相关信息就可以下载 下载后双击 Tracealyzer­x.y.z­windows64.exe 文件即可安装。本文下载使用的版本为 4.5.1 2.2 破解由于从官方下载的 Tracealyzer 需要 License，下载时官方只提供 10 天的临时许可，到期后需要购买套餐才可以使用。为了能够长期使用，需要对软件进行破解。破解 Tracealyzer 需要用到一个压缩包工具“tracealyzer 破解工具.rar”，解压后包括两个工具： de4dot 和dnSpy 2.2.1 安装de4dot对Tracealyer.exe进行预处理安装 de4dot.exe， PowerShell 或 cmd 进入 de4dot 安装路径下，并按以下命令执行 12cd D:/software-setup/de4dot./de4dot.exe -r "D:/software−setup/Tracealyzer/Tracealyzer 4" 运行截图如下 2.2.2 安装dnSpy对Tracealyzer.exe进行反汇编解压dnSpy-net472.zip得到如下内容，双击打开dnSpy.exe 进入 Tracealyzer 安装路径下，并将 Tracealyzer.exe 拖入到 dnSpy 运行界面左侧 注意后面几处截图来源并非操作截图，而是来源于网络，因为破解后再次打开 dnSpy后显示与未破解前已经不同 展开 Tracealyzer→{}→auh，在显示的代码中搜索 bytes 在auj上右键，选择编辑IL指令，弹出一个新的页面，找到brtrue 将brtrue以上4行都修改为nop，brtrue改为br，点击确定，退出关闭dnSpy 退出时会弹出以下界面，选择确定 如果破解了再打开Tracealyzer仍然提示License，则修改C:/ProgramData/TracealyzerData/License.xml文件中的ExpiresOn 以后再次打开Tracealyzer就不会要求License了 2.3 TraceRecorderTracealyzer 的使用需要一个记录器库 TraceRecorder， TraceRecorder 需要集成到目标平台中编译构建。 TraceRecorder 由目标平台的 RTOS 内核进行调用，以存储记录重要的事件，如上下文切换和 RTOS API 调用等。用户也可以创建自定义的事件，并通过手动调用记录器库提供的 API 来存储“用户事件”，允许在目标系统中记录任何软件事件或数据，而不需要要额外的支持。将事件数据传输到主机上的Tracealyzer 应用程序进行可视化，传输方式可以是流模式 (streaming)，也可以是快照模式 (snapshot) 流模式(streaming) 跟踪数据被连续地传输到主机 PC，从而允许长时间跟踪，跟踪时长取决于主机能够使用的 RAM 和磁盘空间。这支持通过调试探针如 IAR I­Jet, Keil ULINKpro和 SEGGER J­Link 流，但也可以利用系统中的其他接口，如 USB, TCP/IP, SPI，高速 UARTs，或设备文件系统 快照模式(snapshot) 跟踪数据保存在目标平台的 RAM 中，允许通过保存 RAM 缓冲区的内容在任何时候进行“快照”。快照模式对内存效率进行了高度优化，结果的数据速率通常仅为10­20 KB/s，具体取决于系统。几个 KB 的跟踪缓冲区通常就足以对最近的事件进行适当的跟踪 本文主要对快照模式的使用进行说明 3. 在FreeRTOS上使用TraceRecorder3.1 FreeRTOS Trace原理Tracealyzer 本身只提供了图形化界面的显示以及从 TraceRecorder 接收事件的形式， TraceRecorder 提供的是目标平台以固定形式记录事件的能力，但是具体要记录什么事件，以及何时进行记录，是由目标平台来定义的。在 FreeRTOS 中，在很多关键代码中插入了形如 traceXXX 的宏函数，这些宏函数被作为一个插槽可以被第三方库来使用，例如下图 在 FreeRTOS 的 systick 中，通过 traceINCREASE_TICK_COUNT 来跟踪系统节拍事件。这些宏函数只是定义了一个事件触发机制以及事件携带什么参数，在 FreeRTOS 的 FreeRTOS.h 头文件中，都定义为了空函数，即如果外部未定义，则所有的跟踪宏函数不执行任何操作 以下列举了一些常用的跟踪宏函数 宏定义 含义 traceTASK_SWITCHED_IN 一个任务被选中进入运行态之前 traceINCREASE_TICK_COUNT systick 计数 traceLOW_POWER_IDLE_BEGIN 在进入 tickless idle 之前 traceLOW_POWER_IDLE_END 在 tickless idle 之后 traceTASK_SWITCHED_OUT 一个任务从运行态转换到 Not Running traceTASK_PRIORITY_INHERIT 当任务试图获取已由低优先级任务持有的互斥锁时调用 traceTASK_PRIORITY_DISINHERIT 当任务释放互斥锁时调用，保持互斥锁会导致任务继承更高优先级任务的优先级 traceBLOCKING_ON_QUEUE_RECEIVE 任务因为无法从队列/互斥体/信号量中读取而阻塞 traceBLOCKING_ON_QUEUE_PEEK 任务因为无法从队列/互斥体/信号量中读取而阻塞 traceBLOCKING_ON_QUEUE_SEND 任务因为无法从队列/互斥体/信号量中写入而阻塞 traceMOVED_TASK_TO_READY_STATE 任务转为 Ready 状态时 用户层面或者第三方库可以利用这些宏函数插槽插入具体的处理函数覆盖 FreeRTOS.h 中原本的空定义，以对具体事件进行捕获处理，启用 FreeRTOS 的 Trace 功能，需要在 FreeRTOSConfig.h 中配置 configUSE_TRACE_FACILITY 为 1 3.2 TraceRecorder库在 Tracealyzer 安装路径 Tracealyzer 4/FreeRTOS/TraceRecorder 文件夹下，是针对 FreeRTOS 的 TraceRecorder 库源文件，在 FreeRTOS 中使用TraceRecroder，需要将其中部分文件添加到目标平台的工程中，对其进行一些配置，然后与 FreeRTOS 共同编译。另外， FreeRTOS 官方发布的源码中，在 FreeRTOS+Plus中的 FreeRTOS­Plus­Trace 也是 TraceRecroder 源码，使用这个源码也可以 3.2.1 TraceRecorder文件结构TraceRecorder 文件结构如图所示 以下对关键文件进行介绍 config/trcConfig.h 该文件是对 TraceRecorder 库主要和整体层面的一个配置文件，包含了记录模式选择 (流/快照)、 FreeRTOS 哪些事件被记录等。该文件必须包含在目标工程中 config/trcSnapshotConfig.h 该文件是快照模式下的一些配置选项，包括可记录事件数量等。当使用快照模式时，该文件需要包含在目标平台工程中 config/trcStreamingConfig.h 该文件是流模式下的一些配置选项，包括定义流模式、读写模式等。当使用流模式时，该文件需要包含在目标平台工程中 include/trcHardwarePort.h 该文件是与目标平台相关定义，必须被包含在目标平台工程中，且在适配目标平台时需要对其进行修改 include/trcKernelPort.h 该文件中定义了 TraceRecorder 的核心功能函数，对 FreeRTOS 跟踪函数的重定义也在其中，必须被包含在目标平台工程中 include/trcPortDefines.h 该文件中是一些公共定义，必须被包含在目标平台工程中 include/trcRecorder.h 该文件中定义了 TraceRecorder 的对外 API，必须被包含在目标平台工程中 trcKernelPort.c 与 trcKernelPort.h 对应的跟踪函数实现，必须被包含在目标平台工程中 trcSnapshotRecorder.c 快照相关功能实现，快照模式需要包含在目标平台工程中 trcStreamingRecorder.c 流模式相关功能实现，流模式需要包含在目标平台工程中 3.2.2 TraceRecorder原理用户使用 TraceRecorder 对 FreeRTOS 进行跟踪，需要在初始化时显式的调用vTraceEnable() 对 TraceRecorder 库进行初始化，在 TraceRecorder 的初始化过程中会在内存中创建 RecorderDataType 数据结构，所有的 FreeRTOS 事件以及用户自定义事件都是记录到这个 RecorderDataType 结构中。当用户需要导出记录数据时，本质上是将 RecorderDataType 所在的整个 RAM 空间导出 RecorderDataType 的数据结构如图 3.4 所示，开头和结尾的固定掩码 marker用于 Tracealyzer 从一段二进制数据中定位 RecorderDataType 的位置。RecorderDataType 结构中最为主要的 3 个成员分别是 ObjectPropertyTable、 SymbolTable 和 eventData。 ObjectPropertyTable 以 Class/Object 的抽象形式记录管理 FreeRTOS 中的 9 种类型资源信息，例如任务、中断、队列、信号量等，并将这些资源的名称字符串、状态、计数等存放于一段连续内存中。 SymbolTable 主要用于存放用户自定义事件的名称字符串等信息。 eventData 用于存放每一个独立的事件信息 由于 RecorderDataType 中各信息或事件都是以连续内存 BytePool 的形式连续存放，并由另外一些变量存放数据的索引，因此整个 RecorderDataType 在内存中是连续且统一的，导出到 Tracealyzer 中可以只根据相对位置解析各部分信息 ObjectPropertyTableObjectPropertyTable 主要用于记录 FreeRTOS 中内核资源相关信息，在头文件 trcKernelPort.h 中定义了 9 种内核资源的类别 ObjectPropertyTable 抽象了 Class/Object 的概念，即对所关心的 Task、Queue、 Semaphore 等划分了类别，而具体记录的数据是每种类别的实例化对象 Object。每种 Class 记录多少个 Object 是由用户决定的 (在配置文件中设置，见后文)，所有 Objects 的信息最终都存放在 ObjectPropertyTable 里的 Objbytes 结构中，该结构以 BytePool 的形式记录存放所有内核资源对象，连续排列。每种 Object记录的内容如下 Queue 队列名称和当前该队列中 message 的数量 Semaphore 信号量名称和当前该信号量的状态， 0:cleared， 1:signaled Mutex 互斥量名称和当前拥有该互斥量的任务句柄， 0:free Task 任务名称和当前任务优先级以及任务状态 ISR 中断名称和当前中断优先级、中断状态 Timer 定时器名称和当前定时器状态 EventGroup 事件组名称 StreamBuffer StreamBuffer 名称 MessageBuffer MessageBuffer 名称 这样设计的好处是所有内核对象的名称信息仅在 ObjectPropertyTable 存在一份，在事件记录时不需要额外存储对象名称，而只需要使用一个标号来代表哪个对象即可 SymbolTableSymbolTable 使用了和 ObjectPropertyTable 类似的设计方法，字符串名称和符号等仅存一份，区别是 SymbolTable 记录的是用户自定义事件。 TraceRecorder提供了 API 可以让用户创建自定义的事件，用户创建的不同事件用标签区别，标签(Label) 也是一个字符串，与内核对象 Object 对应。 SymbolTable 中使用了哈希表结构来将不同标签映射到 BytePool 的不同位置，哈希函数是计算字符串的校验和 3.3 TraceRecorder配置快照模式下，对 TraceRecorder 的配置主要关心 trcConfig.h 和 trcSnapshotConfig.h 这两个文件 trcConfig.h 中的配置项如下 TRC_CFG_HARDWARE_PORT硬件平台选择宏，需要修改为目标硬件平台的类型， TraceRecorder 支持的硬件平台宏定义在 trcPortDefines.h 文件中。默认是未定义，需要手动修改 TRC_CFG_RECORDER_MODE记录模式选择宏，即选择快照模式或者流模式，默认为快照模式 TRC_RECORDER_MODE_SNAPSHOT为快照模式， TRC_RECORDER_MODE_STREAMING 为流模式 TRC_CFG_FREERTOS_VERSIONFreeRTOS 版本选择，默认未定义，需要手动修改 TRC_CFG_SCHEDULING_ONLY是否仅记录调度事件，默认是 0，即所有 FreeRTOS 的 trace 都记录。为 1 时不记录用户自定义事件 TRC_CFG_INCLUDE_MEMMANG_EVENTS 是否记录内存管理事件，默认是 1，即用户或内核调用内存申请和释放事件也会记录 TRC_CFG_INCLUDE_USER_EVENTS 是否记录用户自定义事件，默认是 1 TRC_CFG_INCLUDE_ISR_TRACING 是否记录中断事件，该配置项禁止的是用户在中断服务函数中通过调用vTraceStoreISRBegin 和 vTraceStoreISREnd 进行的记录，而非 FreeRTOS 本身对中断的记录 TRC_CFG_INCLUDE_READY_EVENTS 是否记录任务进入 Ready 状态时的事件 TRC_CFG_INCLUDE_OSTICK_EVENTS 是否允许记录 OS Tick 事件 TRC_CFG_INCLUDE_EVENT_GROUP_EVENTS 是否允许记录 EventGroup 事件 TRC_CFG_INCLUDE_TIMER_EVENTS 是否允许记录Timer事件 TRC_CFG_INCLUDE_PEND_FUNC_CALL_EVENTS 是否记录任何挂起任务的调用事件 TRC_CFG_INCLUDE_STREAM_BUFFER_EVENTS 是否记录 StreamBuffer 事件 TRC_CFG_ENABLE_STACK_MONITOR 是否允许 TraceRecorder 创建一个栈监视任务 TzCtrl TRC_CFG_STACK_MONITOR_MAX_TASKS 定义栈监视任务可监视任务最大数量 trcSnapshotConfig.h 中的配置项如下 TRC_CFG_SNAPSHOT_MODE 快照模式选择宏，TRC_SNAPSHOT_MODE_RING_BUFFER 表示跟踪数据是以 RingBuffer 的形式存放在内存中，新的数据会覆盖老的数据，记录不会停止直到人为触发系统暂停。 TRC_SNAPSHOT_MODE_STOP_WHEN_FULL 表示当记录数据满后会自动停止记录。默认是 RingBuffer 形式 TRC_CFG_EVENT_BUFFER_SIZE 表示可记录事件的数量，决定了 RecorderDataType 中 eventData 的大小 TRC_CFG_NTASK/NISR/NQUEUE/NSEMAPHORE/NMUTEX/NTIMER… 表示可记录跟踪任务、中断、队列等资源的数量，决定了 RecorderDataType 中ObjectPropertyTable 的对象数量 TRC_CFG_INCLUDE_FLOAT_SUPPORT 表示是否支持浮点格式，主要用户用户自定义事件 TRC_CFG_SYMBOL_TABLE_SIZE 表示符号表大小，主要用于用户事件记录时 TRC_CFG_USE_SEPARATE_USER_EVENT_BUFFER 用户自定义事件是否存放于单独的区域，默认是用户自定义事件与内核事件都在eventData 中 TRC_CFG_SEPARATE_USER_EVENT_BUFFER_SIZE 当用户自定义事件单独存放是的 Buffer 大小 TRC_CFG_UB_CHANNELS 用户自定义事件的通道数量 3.4 TraceRecorder 的使用编译方面，将上文提到的 TraceRecorder 相关源文件添加到目标平台项目中，在 FreeRTOSConfig.h 最后引用 trcRecorder.h 头文件。代码方面，需要在系统初始化时调用 vTraceEnable(TRC_START) 初始化 TraceRecorder，然后调用uiTraceStart 记录启动记录事件，然后 FreeRTOS 运行的各种内核事件会自动向RecorderDataType 中记录，如果需要使用用户自定义事件，则通过 xTraceRegisterString 函数创建事件标签，再在需要记录的位置调用 vTracePrint 或 vTracePrintF 进行记录。当需要将记录数据导出时，只需将 RecorderDataType 所在的内存空间导出即可， TraceRecorder 库中有个全局指针 RecorderDataPtr 方便在调试时对 RecorderDataType 地址进行定位 3.5 Tracealyzer 的使用导出的记录文件，通过 Tracealyzer 来打开。打开已安装的 Tracealyzer，点击导航栏 File­&gt;Open­&gt;Open File，选择记录文件 打开记录文件后， Tracealyzer 界面显示如下图 Tracealyzer 提供了 20 多种视图来以不同的可视化方式对同一个记录文件进行展示分析，最主要也是默认显示的是 TraceView，其余常用的视图在左侧红框标记出来的区域 3.5.1 TraceViewTraceView 是以垂直视图显示时间的先后顺序，最先发生的在最上方，最后发生的在最下方。视图以列的形式划分了几个事件域，比较重要的 CPU 域和 EventField，CPU 域包括 CPU 状态、当前运行的任务事件以及 TmrSvc 事件， EventField 包括各种任务事件和其他内核事件。默认情况下由于时间线比较密集，不方便查看，可以在左上角点击 +/­号来调整时间粒度，将其展开后如下图所示 3.5.2 EventLog另一个比较方便的视图是 EventLog，在左侧 View 栏选择 EventLog 打开，如下图所示 EventLog 视图以日志的形式列举一条一条发生的事件，不同事件以不同颜色区分，能够比较直观的查看每一条发生的事件，以及时间上的先后关系 Service Block TimeService Block Time 视图可以方便的统计查看各个系统调用阻塞的情况。左侧视图面板点击下方省略号，选择 Service Blocking­&gt;Show all objects in one graph 打开后的界面如图所示，界面以二维坐标系的形式展示，横轴是运行时间，纵轴是阻塞时间，每一个节点代表一个系统调用事件。鼠标选中一个节点事件后，在右侧的 Selection Details 中可以查看详细阻塞情况 在节点上鼠标右键，可选择 Blocking Time Graph(以单独对象查看所有阻塞情况)、Object History(以日志的形式展现该对象的所有操作)、Object Statistic(对象事件统计) 3.5.4 Service IntensityService Intensity 视图能够以类似直方图的形式展示固定周期内各事件的发生情况 选择 Service Intensity­&gt;Create view Serive Call Intensity ­ All Objects 图中以固定宽度的时间跨度来计算统计在这个跨度内各事件的发生次数，点击左上角+/­符号可以改变统计的时间跨度。不同事件类型用不同的颜色区分，鼠标选中一个事件后，在右侧的 Selection Details 中可以查看详细情况 3.5.5 OverviewOverview 视图提供了对整个跟踪数据的整体描述，上方导航栏选择 View­&gt;Trace Overview 打开 Oviewview 中展示了整个跟踪数据的记录时长、 RAM 大小、 EventBuffer 的使用情况、每种事件的记录占比、跟踪内核对象数量等信息，能够快速判断跟踪数据的资源使用情况 3.6 TraceRecorder 的记录时长与 RAMTraceRecorder 将所有的记录数据都保存在 RecorderDataType 中， TraceRecorder 对 RAM 的使用就是 RecorderDataType 结构的大小。在现实使用中，通常希望明确的知道如何设置和使用 TraceRecorder，以获得充足的记录信息，而同时尽可能少的占用 RAM 空间。然而记录时间的长短与 RAM 占用多少并没有一个直接的计算公式，而是需要以不同的配置参数在目标平台上进行多组测试，并通过 Tracealyzer 的Overview 视图查看 RAM 占用情况，以获得最终的希望设置。例如当发现 Overview中 EventBuffer 的占用率只有百分之 10，那么说明浪费了很多 RAM 空间，如果认为此时记录的信息量是充足的，就可以修改配置，减小 EventBuffer 的大小 本文在 FreeRTOS 的 Linux 仿真环境下进行了一些测试，如下表所示 测试选取了 OSTick 为 1ms 和 10ms，记录时长 6、 10、 16、 20、 30 秒，不丢失事件的情况下，所占用的 RAM 空间大小。由于测试创建的任务功能较为简单，任务主动调用的系统调用频率较低，因此在 OSTick 为 1ms 的场景下， EventBuffer 中大部分的事件都是 OSTick 事件，记录 30 秒的数据量， RAM 占用在 130+KB；而 OSTick为 10ms 的场景下，记录 30 秒的数据量， RAM 占用就能够降低到 30KB 以下列举一些在 RAM 占用方面可以考虑的点 3.6.1 OSTick当 FreeRTOS 的时钟节拍频率较高时，由于 OSTick 本身也会记录为一个事件，而一般 Task 的实际运行频率比 OSTick 要低很多，导致记录数据中 OSTick 事件占比会非常高。通常对于调试分析而言， Task 的运行、消息队列等事件信息才是有价值的，因此可以对 OSTick 事件记录进行一些配置 修改系统运行频率 通过修改 FreeRTOSConfig.h 中的 configTICK_RATE_HZ，降低系统频率，可以有效减少 OSTick 事件的记录频率，但需注意评估是否会影响业务场景的性能和功能 不记录 OSTick 事件 通过修改 trcConfig.h 中的 TRC_CFG_INCLUDE_OSTICK_EVENTS，可以屏蔽对 OSTick 事件的记录 3.6.2 EventBufferSize最为直接有效的方式是修改 EventBufferSize， trcShapshotConfig.h 中的 TRC_CFG_EVENT_BUFFER_SIZE 直接决定 RecorderDataType 中的 eventData 大小，而 eventData 大小占 RecorderDataType 大小的比例非常高，修改TRC_CFG_EVENT_BUFFER_SIZE 的值，对整个记录数据的 RAM 占用影响很大 3.6.3 内核资源数量在已知各种内核资源使用量的情况下，可以修改 trcShapshotConfig.h 中 TRC_CFG_NTASK、TRC_CFG_NISR、TRC_CFG_NQUEUE、TRC_CFG_NSEMAPHORE、TRC_CFG_NMUTEX、TRC_CFG_NTIMER、TRC_CFG_NEVENTGROUP、 TRC_CFG_NSTREAMBUFFER、 TRC_CFG_NMESSAGEBUFFER的数值，减少多余的开销 4. Xilinx Microblaze平台使用FreeRTOS+Trace本章节以 Xilinx Microblaze 平台为例，介绍如何在该目标平台上使用 FreeRTOS+Trace 的快照记录功能。所使用的 Xilinx SDK 版本为 2018.2，并在 SDk 中已经创建好了一个目标平台为 Microblaze 的 FreeRTOS Hello World 模板工程，如下图所示 在 Xilinx SDK 工程中使用 FreeRTOS+Trace 有几个设置步骤，首先需要将 TraceRecorder 库中的部分源文件和头文件导入到已创建好的工程中，然后对TraceRecorder 库的一些宏定义配置进行修改，以适配 Microblaze 平台，然后在工程中设置编译依赖和头文件路径，正确的编译和链接 FreeRTOS、 TraceRecorder以及应用程序，并在应用程序中引用 TraceRecorder 的 API 以开启 TraceRecorder的记录功能，最后使用 SDK 的内存导出功能将记录数据导出为文件，使用 Tracealyzer打开展示。以下将详细介绍各个步骤 4.1 TraceRecorder 移植4.1.1 导入TraceRecorder 到 SDK 工程第一个步骤就是将 TraceRecorder 库导入到工程中，需要提前在本机上安装好Tracealyzer 程序。在工程的 src 目录上，鼠标右键选择 import 在弹出框中选择 General­&gt;File System，点击 Next 点击 Browser 选择 Tracealyzer 安装路径下的“Tracealyzer 4/FreeRTOS/TraceRecorder” 点击 TraceRecorder 左侧的小三角展开路径，鼠标选中 TraceRecorder，并在右侧勾选 trcKernelPort.c、trcShapshotRecorder.c、trcStreamimgRecorder.c 左侧选中 config 文件夹，并在右侧勾选 trcConfig.h、 trcShapshotConfig.h、 trcStreamingConfig.h 文件 左侧选中 include 文件夹，并在右侧勾选全部头文件 点击 Finish，导入后的工程目录如图所示 4.1.2 FreeRTOSConfig.h修改确保 BSP 的 FreeRTOSConfig.h 中 configUSER_TRACE_FACILITY 设置为1，以使能 FreeRTOS 的 Trace 功能 在 FreeRTOSConfig.h 尾部添加以下代码，使 FreeRTOS 引用 trcRecorder.h，重定义 FreeRTOS 本身的 Trace 宏函数 4.1.3 trcConfig.h 修改去掉 trcConfig.h 中需要添加处理器头文件的编译检查 #error 修改硬件平台定义为 Microblaze 修改 FreeRTOS 版本为 10.0.0 4.1.4 trcHardwarePort.h 修改在 trcHardwarePort.h 文件中 Microblaze 定义部分，修改 XTmrCtr_mGetLoadReg为 XTmrCtr_GetLoadReg，并在下方添加 TRACE_ENTER_CRITICAL_SECTION 和TRACE_EXIT_CRITICAL_SECTION 定义 CRITICAL_SECTION 的定义对 TraceRecorder 非常重要，为了确保代码执行不被打断，所有记录事件的过程都会进入/退出 CRITICAL_SECTION，而 TraceRecorder对 Microblaze 平台未定义 CRITICAL_SECTION，因此需要用户自定义 4.1.5 添加头文件路径HelloWorld 工程右键选择 C/C++ Build Settings 在 Settings 界面，选择 Microblaze gcc assemble­&gt;General，将导入到工程 src 中的 config 和 include 路径添加到 Include Paths 中 在 Settings 界面，选择 Microblaze gcc compiler­&gt;Directories，将导入到工程 src 中的 config 和 include 路径添加到 Include Paths 中 4.2 在代码中使用 TraceRecorder在 freertos_hello_world.c 文件中添加 trcRecorder.h 引用 在 man 函数开始调用 vTraceEnable(TRC_START) 使能并初始化 TraceRecorder，调用 uiTraceStart() 开始记录 在 TxTask 中调用 xTraceRegisterString() 和 vTracePrint() 来使用用户自定义事件 4.3 导出记录数据在 Xinlinx SDK 中，可以使用 XSCT Console 来保存快照跟踪数据。按照以下步骤操作 确保 debug 会话活跃，并且系统已经挂起 打开 XSCT Console，找到 Xilinx menu 通过 cd 命令进入一个工作路径，即要保存跟踪数据的路径 输入“print RecorderDataPtr”查看跟踪数据结构的地址 输入“print RecorderDataPtr­&gt;filesize”查看跟踪数据的大小 (bytes) 输入“mrd ­bin ­file trace.bin [address] [size in words]”保存跟踪数据为文件“trace.bin” 从 Tracealyzer 打开“trace.bin” Reference Microblaze Trace Tracealyzer for FreeRTOS on Xilinx Zynq Tracealyzer User Manual]]></content>
      <categories>
        <category>CS</category>
        <category>FreeRTOS</category>
      </categories>
      <tags>
        <tag>FreeRTOS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python通过注册表获取串口列表]]></title>
    <url>%2F2022%2F03%2F07%2FPython%E9%80%9A%E8%BF%87%E6%B3%A8%E5%86%8C%E8%A1%A8%E8%8E%B7%E5%8F%96%E4%B8%B2%E5%8F%A3%E5%88%97%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[工作中需要使用CameraLink协议中的串口和设备进行通信，DALSA采集卡软件中可以将该串口映射到一个COM口上，但是该COM口在Windows的设备管理器中无法识别。使用Python的serial模块或者npm的serialport模块自带枚举函数都无法获取到该COM口。 JavaScript中，使用以下代码定时轮询串口列表 123456789101112import serialport from 'serialport'setInterval(() =&gt; &#123; serialport.list().then( ports =&gt; &#123; if (ports.length &gt; 0) &#123; /* do something */ &#125; &#125; )&#125;, 500) Python中，使用以下代码获取串口列表 12345import serialimport serial.tools.list_portsplist = list(serial.tools.list_ports.comports())ports = [p.name for p in plist] 以上方式都无法获取采集卡映射的COM口。Python中可以使用win32api和win32con通过枚举注册表中的信息来获取串口列表 123456789101112131415161718import win32apiimport win32condef get_serial_ports(): ports = [] key = win32api.RegOpenKey(win32con.HKEY_LOCAL_MACHINE, "HARDWARE\DEVICEMAP\SERIALCOMN", 0, win32con.KEY_READ) try: i = 0 while True: name, value, type1 = win32api.RegEnumValue(key, i) print("name:&#123;&#125; value:&#123;&#125;".format(name, value)) i += 1 ports.append(value) except: print('except') win32api.RegCloseKey(key) return ports 使用RegOpenKey接口来读取键值，使用完毕后需要用RegCloseKey关闭。通过RegEnumValue来枚举串口列表，COM口名称在value字段]]></content>
  </entry>
  <entry>
    <title><![CDATA[Python+Sphinx+reStructuredText编写软件发布文档]]></title>
    <url>%2F2022%2F01%2F22%2FPython-Sphinx-reStructuredText%E7%BC%96%E5%86%99%E8%BD%AF%E4%BB%B6%E5%8F%91%E5%B8%83%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[很多开源项目或者软件都会提供一个类似如下形式的网页文档，页面看上去很清晰整洁，左侧可以方便的查看整个文档的层次结构，文档中支持嵌入高亮代码块 本文介绍在windows环境下通过Python安装Sphinx，使用reStructuredText语法编写软件文档并生成网页文档的方法。本文使用的环境和版本 Windows10 Python3.7.0 sphinx4.4.0 sphinx安装与工程创建sphinx安装cmd或WindowsPowershell运行以下命令安装sphinx 1pip install sphinx 安装完成后，命令行中可以使用以下4个命令工具 sphinx-apidoc：api文档 sphinx-autogen：可以根据文档结构描述自动生成文档 sphinx-build：编译原文档 sphinx-quickstart：自动创建一个新的sphinx工程 以上命令实际使用中主要会用到sphinx-build和sphinx-quickstart 创建sphinx工程首先创建一个文件夹，进入文件夹后输入sphinx-quickstart，即可在该文件夹下创建一个sphinx工程 123mkdir helloworldcd helloworldsphinx-quickstart sphinx-quickstart会在命令行中提示一些创建工程的选项，以下选项可做参考 主要变动的地方 独立的源文件和构建目录(y/n)，选y 项目名称，按需要输入 作者名称，按需要输入 项目发行版本，按需要输入 其余选项可以直接回车按默认值设置，完成后在helloworld路径下会自动生成build和source两个文件夹 source文件夹下是项目源文件所在，也就是文档文件存放位置，build文件夹下是通过source生成的输出文件。sphinx支持多种输出方式，默认为html输出，也可以选择latex、pdf等其他方式输出，但是需要做额外配置，本文只介绍默认html输出方式。另外sphinx默认支持的源文件格式为.rst，即reStructuredText语法，如果要支持其他语法例如Markdown也需要做额外配置 编译在项目根目录下运行以下命令，即可编译源文件生成html 1sphinx-build.exe source build 进入build目录下可以看到生成了html文件，index.html是入口文件 双击index.html文件，浏览器中显示如下页面 修改主题默认的html样式比较难看，可以通过安装修改主题使得页面更美观，在Sphinx Themes Gallery网站可以查看sphinx支持的各种主题 比较典型的是使用Read the Docs主题，在命令行输入以下命令安装该主题 1pip install sphinx-rtd-theme 安装完成后在sphinx工程helloworld的source/conf.py中修改以下两处 12345extensions = [ 'sphinx_rtd_theme',]html_theme = 'sphinx_rtd_theme' 再次编译工程 1sphinx-build.exe source build 打开build/index.html，可以看到页面样式已经变为Read the Docs样式 reStructuredText语法简介sphinx工程默认是以reStructuredText来编写文档，文档的入口在source/index.rst文件 文档组织结构用sphinx编写文档一般都是写类似书籍形式的多部分多章节结构，所有文档内容并不是在一个源文件中编写，而是在多个rst文档中编写，并通过reStructuredText的语法来建立文档的层次结构。reStructuredText中通过toctree语法来建立文档间的层次结构，在source/index.rst文档中有如下代码 123.. toctree:: :maxdepth: 2 :caption: Contents: 以下举例说明toctree语法如何使用。假设要建立一个两级的文档层次，最高层级是一个简介，第二级是两个章节：第一章和第二章，章节分别在一个单独页面显示 源文件的结构如下 |— source |-- index.rst # 总入口 |-- chapter1 # 第一章文件夹 |-- index.rst # 第一章入口 |-- chapter2 # 第二章文件夹 |-- index.rst # 第二章入口 在source目录下需创建两个文件夹chapter1和chapter2，并在章节文件夹下分别创建两个index.rst。source/index.rst文件内容如下 12345678910简介======================================Hello，这里是Sphinx测试工程.. toctree:: :maxdepth: 2 第一章 &lt;chapter1/index.rst&gt; 第二章 &lt;chapter2/index.rst&gt; 总入口的index.rst中，通过toctree语法声明了两个子部分，注意声明的部分需要与maxdepth隔一行，maxdepth表示在html页面中左侧展示的接口最多展示多少层。 标题reStructuredText中使用的标题与Markdown不同 及以上表示部分 *及以上表示章节 =表示小章节 -表示子章节 ^表示子章节的子章节 12345678910111213141516############部分############************章节************小章节============子章节------------子章节的子章节^^^^^^^^^^^^ 注意部分和章节是上下都有标识符 代码嵌入在reStructuredText中嵌入代码有两种方式，一种是行内嵌入，一种是嵌入代码块。行内嵌入使用以下格式 1空格+``+代码+``+空格 嵌入代码块格式如下 1234567891011空行.. code-block:: c #include &quot;&lt;stdio.h&gt;&quot; int main(int argc, char *argv[]) &#123; printf(&quot;hello\r\n&quot;); return 0; &#125;空行 在html页面显示效果如下 嵌入图片在reStructuredText中嵌入图片，使用image语法，格式如下 1.. image:: /images/hello.png 后面跟的/images/hello.png是指示文件路径，注意这里是相对路径，在source目录下创建images文件夹，将hello.png放在该路径下，效果如图 嵌入注意事项按如下语法可以在页面中嵌入Note注意事项 123.. note:: 请注意，这里是注意事项 效果如下 嵌入表格在reStructuredText中使用表格比较复杂，需要格式对齐的非常准确才可以，不想Markdown中那么方便，简单的格式如下 12345+-------+-------+| head1 | head2 | /* 表头定义 */+=======+=======+| cont1 | cont2 | /* 内容定义 */+-------+-------+ 以上代码效果如下 vscode table formatter插件建议在vscode中编写reStructuredText，安装table formatter插件，能够辅助对齐表格 在使用该插件编写表格时，先不需要考虑表格的对齐问题，只需按照表格格式编写好大体内容 12345+-+-+|head1|head2|+=+=+|content1|content2|+-+-+ 鼠标全选表格内容，并按ctrl+shift+p选择table current 插件将会自动对齐表格]]></content>
      <categories>
        <category>文档方案</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Sphinx</tag>
        <tag>reStructuredText</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Microblaze优化选项]]></title>
    <url>%2F2021%2F11%2F04%2FMicroblaze%E4%BC%98%E5%8C%96%E9%80%89%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[Microblaze提供了一些优化选项，正确理解这些选项的含义以及作用对于开发过程有很多帮助，本文主要总结介绍Microblaze Configuration Wizard中的选项内容 预定义配置根据具体使用场景的不同，Microblaze提供了一些预定义的配置选项供用户选择 Microcontroller Preset Real-time Preset Application Preset Minimum Area Maximum Performance Maximum Frequency Linux with MMU Low-end Linux with MMU Typical Frequency Optimized 这些选项实际上是Microblaze在频率、面积、性能这几个指标不同侧重情况下，对各个配置项的组合。选择Current Settings即为自定义模式 Implementation Optimization该选项可选以下3种： Performance Area Frequency 这个选项非常重要，它与Microblaze的流水线级数对应 3级流水线：Area 5级流水线：Performance 8级流水线：Frequency 三级流水线三级流水线对应Area，使用了最小化的硬件花费，只有取址(Fetch)、译码(Decode)和执行(Execute) 三级流水线没有数据阻塞，只有控制流程阻塞、多指令结构性阻塞和访问较慢的内存、从较慢的内存取址等情况。多周期的指令类别有桶形移位器(barrel shift)、乘法器(multiply)、除法器(divide)和浮点指令 五级流水线五级流水线对应Performance，最大化性能考量，包括取址(Fetch，IF)、译码(Decode OF)、执行(Execute，EX)、内存访问(Access Memory，MEM)和写回(Writeback，WB) 五级流水线存在以下两种数据阻塞的情况 OF指令需要EX指令的结果作为源操作数。EX指令类别为加载、存储、桶形移位器、乘法器、触发器和浮点运算。这些会导致1-2周期的阻塞 OF指令需要MEM指令的结果作为源操作数。MEM指令类别包括加载、乘法器和浮点运算。这些会导致1个周期的阻塞 多周期指令有除法器和浮点运算 八级流水线八级流水线对应Frequency，用于最大化频率，包括取址(Fetch，IF)、译码(Decode OF)、执行(Execute，EX)、内存访问0(Access Memory 0，M0)、内存访问1(Access Memory 1，M1)、内存访问2(Access Memory 2，M2)、内存访问3(Access Memory 3，M3)和写回(Writeback，WB) 八级流水线存在以下四种数据阻塞的情况 OF指令需要EX指令的结果作为源操作数。EX包括加载、存储、桶形移位器、乘法器、除法器和浮点运算，会导致1-5个周期的阻塞 OF指令需要M0指令的结果作为源操作数。M0包括加载、乘法器、除法器和浮点运算，会导致1-4周期的阻塞 OF指令需要M1或M2指令的结果作为源操作数。M1或M2包括加载、除法器和浮点运算，会导致1-3或1-2周期的阻塞 OF指令需要M3指令的结果作为源操作数。M3包括加载和浮点运算，会导致1周期的阻塞 在额外的多周期指令种，存在3种情况的结构性阻塞 OF中的指令是流指令，EX中的指令是流、加载、存储、除法或浮点指令，并实现了相应的异常，这导致一个1周期的阻塞 OF中的指令是流指令，M0、M1、M2或M3中的指令是装载、存储、除法或浮点指令，并实现了相应的异常，这导致一个1周期的阻塞 M0中的指令是加载或存储指令，M1、M2或M3中的指令是加载、存储、除法或浮点指令，并实现了相应的异常，这导致一个1周期的阻塞 多周期指令分为分割指令和浮点指令FDIV, FINT和FSQRT Use Instruction and Data Caches使用外部存储器时，激活高速缓存，可以显著提高性能，可以降低外部慢速设备访问的使用量 Enable Barrel Shifter使能硬件桶形移位器(Barrel Shifter)，可以提高程序在移位操作时的性能。当该选项使能时，编译器可以自动的选择使用bsrl、bsra、bsll、bsrli、bsrai和bslli等汇编指令来优化加速移位操作 Enable Floating Point Unit浮点运算单元能够提升float类型数据进行运算时的效率，Microblaze的FPU遵循了IEEE 754-1985标准，支持加、减、乘、除、比较、转换和平方根运算。编译器会自动根据系统选择的FPU类型使用汇编浮点指令优化浮点运算 Enable Integer Multiplier使用一个硬件乘法器，可以提升程序在乘法运算时的效率 Enable Integer Divider使能整型除法器，可以使用idiv和idivu指令，提升除法运算效率 Enable Additional Machine Status Registers Instructions使能MSR寄存器指令msrset和msrclr，用于设置和清MSR的位。MSR包含了处理器的控制和状态位，读取该寄存器时bit[29]会被复制到bit[0]作为近进位复制。对MSR进行读写有两种方式，一种是使用MFS、MTS指令，另一种是使用msrset和msrclr。当使用msrset和msrclr进行写时，进位立即生效，其余位在一个时钟周期后生效。当使用MTS写时，所有位都在一个时钟周期后生效。程序运行会非常频繁的使用MSR，因此使能该选项可以很大程度的提升性能 例如在使用FreeRTOS时，systick或者消息队列中会频繁调用microblaze_enable_interrupts和disable_enable_interrupts函数，这两个函数是汇编函数，例如microblaze_enable_interrupts函数的定义如下 1234567891011121314151617181920 .text .globl microblaze_enable_interrupts .ent microblaze_enable_interrupts .align 2microblaze_enable_interrupts:#if XPAR_MICROBLAZE_USE_MSR_INSTR == 1 rtsd r15, 8 msrset r0, 0x2#else /*XPAR_MICROBLAZE_USE_MSR_INSTR == 1*/ #Read the MSR register mfs r12, rmsr #Set the interrupt enable bit ori r12, r12, 0x2 #Save the MSR register mts rmsr, r12 #Return rtsd r15, 8 nop#endif /*XPAR_MICROBLAZE_USE_MSR_INSTR == 1*/ .end microblaze_enable_interrupts 可以看到，如果定义了MSR_INSTR，使能中断只有两条指令完成，否则需要5条指令。因此开启额外MSR指令能够在多任务系统任务切换和上下文切换方面提升性能 Enable Pattern Comparator使能模式比较器，可以使用pcmpbf、pcmpeq和pcmpne指令，提升程序在进行比较时的性能。编译器自动进行指令转换 Enable Reversed Load/Store and Swap Instructions启用反向加载/存储和交换指令，可以使用lbur、lhur、lwr、sbr、shr、swr、swapb和swawph指令。反向加载/存储指令可以以相反的字节顺序读写数据，交换指令可以在寄存器中交换字节和字。这些指令在处理网络字节序(大端)和Microblaze字节序(小端) 时可以提升性能]]></content>
      <categories>
        <category>嵌入式</category>
      </categories>
      <tags>
        <tag>Xilinx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv读取注释]]></title>
    <url>%2F2021%2F10%2F22%2Fcsv%E8%AF%BB%E5%8F%96%E6%B3%A8%E9%87%8A%2F</url>
    <content type="text"><![CDATA[Python的pandas库处理csv文件非常方便，开发过程中经常会用到csv文件，例如将csv中的数据转化为二进制、将csv文件转化为json等。由于csv本身是以列表的形式组织数据的，如果想要额外的加入一些描述信息，比如版本号等，应该如何做呢？ 可以以key-value的形式在csv文件头部写入一些信息 1234#Version:v1.0#Data:2021/01/01#Author:Jam#Description: This is a example of csv commit 利用以下函数可以从csv文件中以字典的形式获取注释中的key-value 12345678910111213def csv_read_attrs(filename): s = '' fobj = open(filename) while True: line = fobj.readline() if not line: break s += line attrs = re.findall(r'#(.*)', s) keys = [x.split(':')[0].strip() for x in attrs] values = [x.split(':')[1].strip() for x in attrs] attrs = dict(zip(keys,values)) return attrs 将上述注释保存为test.csv文件，调用csv_read_attrs函数解析注释信息 1234&gt;&gt;&gt; import csv_read_attrs from csvAttrs&gt;&gt;&gt; attrs = csv_read_attrs('test.csv')&gt;&gt;&gt; attrs&#123;'Version': 'v1.0', 'Data': '2021/01/01', 'Author': 'Jam', 'Description': 'This is a example of csv commit'&#125;]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202110]]></title>
    <url>%2F2021%2F10%2F15%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202110%2F</url>
    <content type="text"><![CDATA[ICCV2021不久前计算机视觉三大顶会之一的ICCV2021接收结果已经公布，本次ICCV共计6236篇有效提交论文，其中有1617篇被接收，接收率为25.9% 论文共分为检测、分割、估计、跟踪、视觉定位、底层图像处理、图像视频检索、三位视觉等多个方向，极市团队-ICCV论文代码整理该Github项目整理了所有的相关论文和代码，主要有以下方向 2D Object Detection(2D目标检测) 3D Object Detection(3D目标检测) Saliency Object Detection(显著性目标检测) Camouflaged Object Detection(伪装目标检测) Anomally Detection in Image(图像异常检测/表面缺陷检测) Edge Detection(边缘检测) Image Segmentation(图像分割) Instance Segmentation(示例分割) Semantic Segmentation(语义分割) Video Object Segmentation(视频目标分割) Referring Image Segmentation(参考图像分割) Dense Prediction(密集预测) Facial Recognition/Detection(人脸识别/检测) Face Generation/Face Synthesis/Face Reconstruction/Face Editing(人脸生成/合成/重建/编辑) Face Forgery/Face Anti-Spoofing(人脸伪造/反欺骗) 3D Vision(三维视觉) Point Cloud(点云) 3D Reconstruction(三维重建) Neural Network Structure Design &amp; Optimization(神经网络设计与优化) Transformer NAS(神经网络架构搜索) Loss Function(损失函数) Visualization/Interpretability(可视化/可解释性) Model Training/Generalization(模型训练/泛化) Noisy Label(噪声标签) Long-Tailed Distribution(长尾分布) Out of Distribution Detection(分布外样本检测) Knowledge Distillation(知识蒸馏) Pruning(剪枝) Quantization(量化) Image Generation/Image Synthesis(图像生成/合成) View Synthesis(视图合成) GAN/Generative/Adversarial(GAN/生成式/对抗式) Image Processing(图像处理) Super Resolution(超分辨率) Image Denoising(图像去噪/去模糊/去雨去雾) Image Edit/Image Inpainting(图像编辑/修复) Style Transfer(风格迁移) Image Quality Assessment(图像质量评估) Human Pose Estimation(姿态估计) Depth Estimation(深度估计) Image&amp;Video Retrieval/Video Understanding(图像&amp;视频检索/理解) Action/Activity Recognition(行为识别/动作识别) Re-Identification/Detection(行人重识别/检测) Image/Video Caption(图像/视频字幕) Visual Localization(视觉定位) Image Matching(图像匹配) 3D Vision(三维视觉) Object Tracking(目标跟踪) Medical Imaging(医学影像) Text Detection/Recognition(文本检测/识别) Remote Sensing Image(遥感图像) Scene Graph Generation(场景图生成) Scene Graph Prediction(场景图预测) Data Augmentation(数据增广) Anomaly Detection(异常检测) Representation Learning(表征学习) Image Clustering(图像聚类) Few-shot/Zero-shot Learning(小样本学习/零样本学习) Continual Learning/Life-long Learning(持续学习) Transfer Learning/Domain Adaptation(迁移学习/自适应) Metric Learning(度量学习) Incremental Learning(增量学习) Contrastive Learning(对比学习) Visual Reasoning/VQA(推理学习/视觉问答)]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202108]]></title>
    <url>%2F2021%2F08%2F13%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202108%2F</url>
    <content type="text"><![CDATA[Copilot微软、OpenAL、Github联合推出了自动代码生成AI：Copilot，该技术使用OpenAI最新的GPT3语言模型，在Github海量(数十亿行代码)的公共仓库数据集上进行了代码学习训练。通过一个Web内测接口，用户可以在页面中输入需求描述，Copilot自动转换为代码，支持很多种编程语言，在Python、Java、JavaScript等语言方面尤其优秀 该项目发布后，在网络上引起了很多讨论，有人发现Copilot生成的代码和很多开源项目代码几乎是直接复制粘贴，有些甚至连注释都搬运过来。并且，Copilot无法理解复杂上下文环境，只能针对非常明确具体简单的需求给出一些代码块，并且程序员依然不能放心的使用，必须仔细检查代码中是否存在漏洞。最重要的问题是，Github官方在后续的回应中明确了使用了Github所有公开代码来训练Copilot，并且不区分License类别，后续Copilot还有可能商用的说法，引起了很多开发者的抨击，Github官方并不具备这样的权力，这样做本身就是对开源的破坏 参考 copilot技术预览网站 OpenAI Codex OpenAi Codex Paper]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[优先队列(PriorityQueue)]]></title>
    <url>%2F2020%2F08%2F30%2F%E4%BC%98%E5%85%88%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[普通的队列是一种FIFO结构，在优先队列(PriorityQueue)中，数据存在优先级，在进行出队操作时，具有最大(MaxPriorityQueue)或最小(MinPriorityQueue)优先级的元素最先出队。在很多应用场景中，都需要这种对数据进行有序处理或者按照优先级处理的方式 优先队列的应用很广泛，最常见的是进行任务调度，当有多个任务都需要处理的时候，为不同的任务划分优先级，并分别调度；优先队列还可以开发图搜索算法、数据压缩算法等 实现优先队列的实现有很多方式，例如链表和数组。利用二叉堆来实现优先队列是一种较为高效的做法 结构 入队 出队最大元素 有序数组 N 1 无序数组 1 N 堆 logN logN 二叉堆实现的优先队列能够保证在插入和删除两个维度都较快 二叉堆二叉堆是一组能够用堆有序的完全二叉树排列的元素，并在数组中按照层级存储 可以用下图来表示一个二叉堆 假设字幕A~Z表示的数值依次增大，则上图表示的是一个最大二叉堆(大根堆)，根节点”T”最大，二叉堆中任意子节点的数值不大于父节点数值 二叉堆有以下特性 位置为k的节点父节点位置为k/2，两个子节点位置分别为2k和2k+1 大小为N的完全二叉树高度为logN 以上性质决定了二叉堆在进行遍历或者搜索的路径是跳跃层级的，无论是插入还是删除操作，由于树高最大logN，因此操作的复杂度最大也为logN 数组实现二叉堆使用数组实现二叉堆是非常高效的，二叉堆中元素的位置和数组索引的关系可以用下图表示 使用数组实现二叉堆，仅利用数组索引就可以沿着树上下移动，非常便利。要注意为了编程时父子位置关系的统一性，数组的第一个位置不使用 在对堆进行操作时，会首先进行一些简单的改动，例如插入时先将元素插入到堆底，或者删除时先删除堆顶元素，这样会打破原有堆的有序状态，然后再将堆恢复有序状态(堆的有序化)。本文以最大优先队列为例，对堆的有序化操作进行说明 有序化需要用到两个辅助函数：比较(less)和交换(exch)。less函数是为了实现优先队列的泛化，希望实现的优先队列是一种泛型数据结构，而非某一种特定的数据结构，因此对于某一种具体的数据结构和类型，需要提供对应的比较函数”compare”，例如字符串String、文件File、时间Time等；交换两个元素是优先队列中使用非常频繁的操作，因此提取为单独的函数。以下是less和exch的伪代码 1234567891011boolean less(array, compare, i, j)&#123; return compare(array[i], array[j]);&#125;void exch(array, i, j)&#123; temp = array[i]; array[i] = array[j]; array[j] = temp;&#125; 上浮(swim)当插入一个元素到堆底后，如果该元素比其父节点更大，就需要通过上浮操作来对堆进行有序化。例如在G节点下向堆中插入元素”Y” “Y”元素只需要一遍一遍的与其父节点进行比较，并交换它们的位置，当”Y”元素到达合适的位置时，整个堆就变得有序了 整个过程是插入元素不断地向上”浮动“，不在上浮路径上的元素都保持不变。有序化后部分元素的所在层数会发生变化。上浮的伪代码如下 12345678// k: new item's indexvoid swim(k)&#123; while ((k&gt;1) &amp;&amp; less(k/2,k)) &#123; exch(k/2, k); k = k/2; &#125;&#125; 下沉(sink)当某个元素变得比它的两个子节点或是其中之一小，通过下沉操作来恢复有序状态。例如将堆顶元素”T”删除后再将”G”元素放在原先”T”元素的位置 “G”元素只需要一遍一遍与其子节点进行比较，并交换位置，当”G”元素到达合适的位置时，整个堆就变得有序了 下沉的伪代码 123456789101112void sink(k)&#123; while (2*k &lt;= N) &#123; j = 2*k; if ((j&lt;N) &amp;&amp; less(j, j+1)) j++; if (!less(k, j)) break; exch(k, j); k = j; &#125;&#125; 自动扩容由于数组需要在创建时分配固定大小，因此为了提高利用的灵活性，需要队列能够自动调整数组大小。太过频繁的重新调整会增大开销，较为合理的方式是 当数组满时，扩容为原数组2倍 当数组元素大小减小到数组容量时，减小容量为原数组一半 C语言实现MinPQMinPQ结构定义 12345678910111213141516/* -- MinPQ --&#123;*/typedef struct _minpq &#123; int node_nr; /* node number */ int capacity; /* the capacity of queue */ size_t nodesize; /* every node's size in the queue */ int *p; /* point to the private priority array */ char *q; /* point to the node array */#define MINPQ_F_CDC 0x0001 /* custom define comparator */#define MINPQ_F_RES 0x0002 /* capacity resize */ uint16_t flags; bool (*comparator)(void *, void *); /* point to custom define comparator */ int info_width; /* use fo debug and dump */ void (*info_prior_handle)(struct _minpq *, int, char *, int); /* use fo debug and dump */ void (*info_value_handle)(struct _minpq *, int, char *, int); /* use fo debug and dump */&#125; MinPQ; 创建MinPQ，创建函数需要传入节点大小和容量，如果容量为0，默认支持自动扩容；如果使用自定义优先级，需要传入比较器comparator。创建函数内部会创建并维护一个内部默认的优先级数组p，该数组是整型类型，如果要用默认的整型优先级，后续插入元素时可传入优先级数值 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/* * Create a Min Priroity Queue(MinPQ) * * @nodesize: every node's size in the queue * @capacity: the capacity of queue * @comparator: custom define comparator * @flags: queue's features * * return: MinPQ */MinPQ *MinPQ_create(size_t nodesize, int capacity, bool(*comparator)(void *, void *), uint16_t flags)&#123; int allocnr; /* params check */ if (nodesize &lt;= 0) &#123;log_err("nodesize zero");return NULL;&#125; if (capacity &lt; 0) &#123;log_err("capacity invaild");return NULL;&#125; MinPQ *q = zalloc(sizeof(MinPQ)); if (!q) &#123;log_err("MinPQ create error");return NULL;&#125; q-&gt;node_nr = 0; q-&gt;nodesize = nodesize; q-&gt;info_width = 4; q-&gt;capacity = capacity; if (capacity == 0) &#123; /* if no capacity, we set resize flag auto */ mask_push(q-&gt;flags, MINPQ_F_RES); &#125; if (comparator != NULL) &#123; q-&gt;comparator = comparator; mask_push(q-&gt;flags, MINPQ_F_CDC); &#125; mask_push(q-&gt;flags, flags); if (!capacity) &#123; allocnr = 1; &#125; else &#123; allocnr = capacity + 1; &#125; /* alloc private priority */ q-&gt;p = zalloc(allocnr*sizeof(int)); q-&gt;q = zalloc(allocnr*q-&gt;node_nr); return q;&#125; 插入元素，会首先检查容量和是否支持扩容；新节点先放在堆底，然后上浮操作进行有序化 12345678910111213141516171819202122232425262728/* * MinPQ insert operation * * @q: the queue you want to do insert * @u: the node you want to insert * @p: the priority of insert node, if use custom define priority, don't need this */err_type MinPQ_insert(struct _minpq *q, void *u, int p)&#123; char *addr; if (MinPQ_full(q)) &#123; if (mask_exst(q-&gt;flags, MINPQ_F_RES)) &#123; if (!q-&gt;capacity) q-&gt;capacity = 1; _resize(q, q-&gt;capacity*2); &#125; else &#123; log_err("MinPQ full"); return et_full; &#125; &#125; q-&gt;p[++q-&gt;node_nr] = p; addr = q-&gt;q + q-&gt;nodesize*q-&gt;node_nr; memcpy(addr, u, q-&gt;nodesize); _swim(q, q-&gt;node_nr); return et_ok;&#125; 删除元素，先将堆顶元素拷贝出来，然后将堆底元素移动到堆顶，调用下沉sink进行有序化 1234567891011121314151617181920212223242526272829err_type MinPQ_delmin(struct _minpq *q, void *u, int *p)&#123; char *addr1, *addr2; if (MinPQ_empty(q)) &#123; log_err("MinPQ empty"); return et_empty; &#125; addr1 = q-&gt;q + q-&gt;nodesize; memcpy(u, addr1, q-&gt;nodesize); *p = q-&gt;p[1]; q-&gt;p[1] = q-&gt;p[q-&gt;node_nr]; addr2 = q-&gt;q + q-&gt;nodesize*q-&gt;node_nr; memcpy(addr1, addr2, q-&gt;nodesize); q-&gt;node_nr--; _sink(q, 1); q-&gt;p[q-&gt;node_nr+1] = 0; if ((q-&gt;node_nr&gt;0) &amp;&amp; (q-&gt;node_nr==(q-&gt;capacity-1)/4) &amp;&amp; mask_exst(q-&gt;flags, MINPQ_F_RES)) &#123; _resize(q, q-&gt;capacity/2); &#125; addr2 = q-&gt;q + q-&gt;nodesize*(q-&gt;node_nr+1); memset(addr2, 0x0, q-&gt;nodesize); return et_ok;&#125; 获取指定索引数据，该函数可以结合for循环遍历队列 1234567891011121314err_type MinPQ_get(struct minpq *q, void *u, int *p, int index)&#123; char *addr; if (empty(q)) &#123; log_err("MinPQ empty"); return et_empty; &#125; addr = q-&gt;q + q-&gt;nodesize*index; memcpy(u, addr, q-&gt;nodesize); *p = q-&gt;p[index]; return et_ok;&#125; 上浮操作，用到了递归 1234567static void _swim(struct _minpq *q, int k)&#123; while ((k&gt;1) &amp;&amp; (_greater(q, k/2, k))) &#123; _swap(q, k, k/2); k = k/2; &#125;&#125; 下沉操作 1234567891011121314static void _sink(struct _minpq *q, int k)&#123; int j; while (2*k &lt;= q-&gt;node_nr) &#123; j = 2*k; if ((j&lt;q-&gt;node_nr) &amp;&amp; (_greater(q, j, j+1))) j += 1; if (!_greater(q, k, j)) break; _swap(q, k, j); k = j; &#125;&#125; 交换函数 12345678910111213141516static void _swap(struct __minpq *q, int i, int j)&#123; int tempp; char *addr1, *addr2; char *tempu = zalloc(q-&gt;nodesize); tempp = q-&gt;p[i]; q-&gt;p[i] = q-&gt;p[j]; q-&gt;p[j] = tempp; addr1 = q-&gt;q + q-&gt;nodesize*i; addr2 = q-&gt;q + q-&gt;nodesize*j; memcpy(tempu, addr1, q-&gt;nodesize); memcpy(addr1, addr2, q-&gt;nodesize); memcpy(addr2, tempu, q-&gt;nodesize);&#125; 比较函数 123456789101112static bool _greater(struct _minpq *q, int i, int j)&#123; char *addr1, *addr2; if (!q-&gt;comparator) &#123; return (q-&gt;p[i] &gt; q-&gt;p[j]); &#125; else &#123; addr1 = q-&gt;q + q-&gt;nodesize*i; addr2 = q-&gt;q + q-&gt;nodesize*j; return q-&gt;comparator(addr1, addr2); &#125;&#125; 扩容函数 12345678910111213141516171819202122err_type _resize(struct _minpq *q, int capcaity)&#123; int i; char *new; if (capcaity &lt;= q-&gt;node_nr) &#123; log_err("capcaity invaild"); return et_param; &#125; new = zalloc(q-&gt;nodesize*(capcaity+1)); if (!new) &#123; log_err("alloc new error"); return et_nomem; &#125; memcpy(new, q-&gt;q, q-&gt;nodesize*(q-&gt;node_nr+1)); mem_free(q-&gt;q); q-&gt;q = new; q-&gt;capacity = capcaity; return et_ok;&#125; 一些辅助查询函数 1234567891011121314int MinPQ_size(struct _minpq *q)&#123; return (q-&gt;node_nr);&#125;bool MinPQ_full(struct _minpq *q)&#123; return (q-&gt;capacity == q-&gt;node_nr);&#125;bool MinPQ_empty(struct _minpq *q)&#123; return (q-&gt;node_nr == 0);&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
      </categories>
      <tags>
        <tag>数据结构</tag>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C-Namespace]]></title>
    <url>%2F2020%2F07%2F13%2FC-Namespace%2F</url>
    <content type="text"><![CDATA[记录两种C语言实现命令空间的方法 结构体封装简单来说就是将某个独立的库对外封装一个统一的接口结构体structX，外部调用时都使用structX.aaa()来调用库中的方法。例如有一个foo的库，要对外提供test()方法，可在foo.h文件中定义一个命名空间结构体namespace_foo，结构体中定义好需要对外提供的方法成员，并在最后通过extern关键字对外暴露结构体变量Foo 1234567/* foo.h */typedef struct _namespace_foo &#123; const char *name; const char *version; int (*test)();&#125; namespace_foo;extern namespace_foo const Foo; 结构体变量Foo的定义在foo.c文件中 1234567891011/* foo.c */static int foo_test()&#123; /**/&#125;namespace_foo const Foo = &#123; .name = 'Foo', .version = '1.0', .test = foo_test&#125; 外部要调用foo库中的函数，只需要引用foo.h头文件后，通过形如Foo.test()的方式就可以 1234567891011/* main.c */#include "foo.h"int main(int argc, char **argv)&#123; Foo.test(); return 0;&#125; 如果有另外一个库goo需要同时使用，只需要定义结构体变量Goo时的变量名称与Foo不同即可 利用ifdef另一种方式是利用条件宏定义宏来重定义函数名称 1234567/* foo.c */int foo_test()&#123; /**/&#125; 12345678/* foo.h */int foo_test();#ifdef NAMESPACE_FOO#define test(...) foo_test(__VA_ARGS__)#endif 在外部使用foo库的函数前，需要通过宏声明NAMESPACE_FOO，然后再引用foo.h头文件，后续调用test()函数就等于调用foo_test() 123456789/* main.c */#define NAMESPACE_FOO#include "foo.h"int main(int argc, char **argv)&#123; test();&#125;]]></content>
      <categories>
        <category>Program Language</category>
        <category>C</category>
      </categories>
      <tags>
        <tag>Program Skill</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202007]]></title>
    <url>%2F2020%2F07%2F13%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202007%2F</url>
    <content type="text"><![CDATA[技术电子织物微交互在”ACM CHI 2020”会议中，Tensorflow团队展示了机器学习如何与交互式织物相结合，从而实现离散和连续手势的并行使用 识别手势主要利用了螺旋感应矩阵 (HSM)，这些织物具有电容感应能力，具备传输和接收电极的作用。实验收集了 12 位参与者的数据，共得到 864 个手势样本，包括：轻弹、滑动、单点触摸手势等。训练后实现了约 94% 的手势识别准确率 该技术未来可应用于线控耳机的手势控制方式、连帽衣物控制等]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[触发词检测]]></title>
    <url>%2F2020%2F06%2F28%2F%E8%A7%A6%E5%8F%91%E8%AF%8D%E6%A3%80%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[经过一个多月工作之余的研究和编码调试，终于将触发词检测的完整方案实现并验证有效，能够在PC端进行独立的实时触发词检测，开源代码见我的开源repo。该project来源于deeplearning.ai的Deep Learning Specialization课程中的”Sequence Models”序列模型，课程中完整介绍了如何搭建一个基于神经网络的触发词检测模型 本文首先对deeplearning.ai的触发词检测课程内容进行介绍，然后对我自己的实现方案进行了介绍说明，最后总结了整个研究过程中的知识点和经验 deeplearning.ai触发词检测目前很多智能产品上都带有触发词检测功能，例如苹果Siri、百度小度、小米的小爱同学等。触发词检测(Trigger word detect)也叫做关键字检测(keyword detect)或者唤醒词检测(wakeword detect)，能够让设备在听到某个特定单词后自动唤醒。该课程介绍了一个触发词检测的通用模型，能够基于该模型训练自己的触发词，并在不同的设备上部署 设计思想设定一个10秒的检测窗口detect window，将音频内容划分为以下3种类型 background：背景音 activate：激活词，也就是要检测的触发词 negative：负面词，不同于激活词的说话声 10秒的检测窗口中，背景音一直存在，激活词和负面词可以在任意位置出现 将10秒的检测窗口划分为若干个检测单元，未出现触发词的检测单元，将其标签设置为0，出现触发词的检测单元，将其标签设置为1 标签1与触发词的出现时间需要有一定的滞后性，因为必须先检测到完整的触发词，才能输出检测到的结果 生成数据集首先需要为触发词检测算法构建一个数据集。理想情况下，最终的模型应该尽可能的在各种场景下都能够进行检测。因此，你需要在不同的背景声音中创建一段录音，例如图书馆、家中、办公室、户外、咖啡厅等等。另外，为了数据和模型有更好的泛化效果，采集的所有样本中，应该将激活次和负面词的出现频率和位置尽可能分布均匀。另外，如果你想要能够识别不同口音，那么需要采集更多样本，包括不同年龄、性别、体重、籍贯等 程序化生成样本数据集的来源可能是你去收集不同人的录音。如果以10秒的窗口为样本，那么你需要拿着一个录音设备采集多组10秒的音频，以1000个样本数量为例，总的样本时间为10000秒大概是2.7个小时，再加上寻找不同的人和沟通的时间，总的时间成本可能很长 可以使用一个程序化生成样本的方法，将收集的数据分为background、activate和negative三类。background表示在各种环境中采集若干个10秒的背景声音，activate表示采集不同人只说激活词的声音，negative表示不同人只说负面词的声音。采集完成后，将activate和negative样本随机的插入background中，这样能够更加快速的生成更多样本 录音转换为频谱录音本质上是麦克风记录下气压的微小变化，而这种时域的录音信号，直接用来提取和分析特征，是非常困难的，通过转换到频域，能够更好地提取和利用特征 在Python中，可以用matplotlib.pyplot包的specgram方法实现录音转换为频谱数据 1234567891011def specgram(filename): rate, data = wavfile.read(filename) nfft = 200 fs = 8000 noverlap = 120 nchannels = data.ndim if nchannels == 1: pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap) else: pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap=noverlap) return pxx 以上代码，将一个10秒的录音文件转换为频谱数据pxx。这里需要注意一下录音数据和频谱数据的shape 1234567&gt;&gt;&gt; from scipy.io import wavfile&gt;&gt;&gt; rate, data = wavfile.read('test.wav')&gt;&gt;&gt; data.shape(441000,)&gt;&gt;&gt; pxx = specgram('test.wav')&gt;&gt;&gt; pxx.shape(101,5511) 因为录音使用44100Hz的采样频率，因此1秒产生44100帧音频数据，10秒就是441000帧数据，音频数据是一维的序列。转化为频谱数据后，将10秒划分为了5511个片段，并提取了101个主要频率的窗口 创建一个训练样本创建训练样本的前提是，已经按照上文的要求，采集了若干数量的背景声、激活词和负面词的录音文件。生成一个训练样本，需要以下4个步骤 随机选择一个背景录音 随机选择若干个激活词音频插入到背景的随机位置 随机选择若干个负面词音频插入到背景的随机位置 对生成的样本进行标记 这里需要注意，插入的过程，是音频数据的叠加操作，而不是追加，例如向一个10秒的背景音频中插入1秒的激活词片段，插入后得到的合成音频仍然是10秒，而不是11秒。在进行多次插入时，为了避免重复插入到相同的位置，需要记录已经插入的片段位置 在进行标记时，将10秒的时长划分为1375，触发词后的50个片段被标记为1，这样做的好处是不需要人为的听取每一个合成音频再手动进行标记，标记过程可以自动进行。1375个片段长度是由模型决定的，后文介绍 模型使用的模型如图所示 模型的输入是合成音频转化成的频谱数据，shape为(5511,101)，输出shape为(1375,)。首先是经过一个一维卷积层，进行简单的特征提取，并减小数据大小到(1375,196)，然后数据经过两个GRU层，最后通过Sigmoid函数将输出归一化到0~1 评估和预测该模型在dev数据集上获得了0.94的精度。在预测时，先将音频数据输入模型，得到模型输出，然后遍历1375个输出数据，如果有连续50个数据大小超过某个阈值(0.6)，认为检测到触发词 我的实现数据集生成实验环境为Linux，Linux系统为音频处理抽象了一个ALSA框架，包括声卡设备驱动、音频核心层和应用层的一些工具。录音使用”arecord”命令 1arecord -xxxx 另外，还需要两个python音频处理包：pydub和alsaaudio。pydub用于处理音频插入合成，alsaaudio用于实时录音 DataLoader抽象了DataLoader类来用于加载和生成数据集，代码位于DataLoader.py。设定两个根目录下和数据集相关路径： orig：表示原始录音数据 gen：表示生成的合成音频和数据集 orig路径下包括background、activate、negative三个文件夹，分别存放背景音、触发词和负面词。gen路径下包括train和dev两部分，分别存放训练集数据和验证集数据 DataLoader主要实现load、info和generate三个方法 123456789class DataLoader(object): def __init__(self, ...): /*......*/ def load(self, ...): /*......*/ def info(self): /*......*/ def generate(self, ...): /*......*/ load主要完成从指定路径加载原始音频文件到内存，默认是从orig路径下加载，也可以从参数指定路径，路径下需要包含background、activate和negative三个子文件夹。info主要是显示loader相关信息，generate方法生成数据集文件，可通过target参数指定是生成训练集还是验证集 loadload方法会遍历远视音频路径下的文件，并按照子文件夹background、activate和negative将音频文件加载到loader的ori_activate、ori_negative和ori_background三个列表中，加载音频文件是调用AudioSegment的from_wav()方法 1234567891011121314151617def load(self, path=None): oripath = None valnames = self.__dict__ if not path: oripath = self.config['ori_dir'] else: oripath = path self.config['ori_dir'] = path for label in self.config['labels']: dirname = os.path.join(oripath, label) for file in os.listdir(dirname): if file[-4:] == '.wav': data = AudioSegment.from_wav(os.path.join(dirname, file)) valnames['ori_'+label].append(data) generate考虑到PC机内存问题，要生成大量数据集文件，使用了batch的方式，generate参数支持批量生成，参数batchs用于控制批次，默认为1，batch_size用于控制单次生成的数量大小 在实现内部，使用yield抽象了一个生成器_generator来生成单个合成音频文件，一个批次的数据通过numpy.save导出为.npy文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657def generate(self, dir=None, target='train', batchs=1, batch_size=100): target_dir = None target_path = None self.synthesis_count = 0 np.random.seed(22) if not self.is_loaded(): print('not loaded') return if not dir: target_dir = self.config['gen_dir'] else: target_dir = dir self.config['gen_dir'] = dir if target == 'train': target_path = os.path.join(target_dir,self.config['gen_train_dir']) elif target == 'dev': target_path = os.path.join(target_dir,self.config['gen_dev_dir']) self.dataset_path = os.path.join(target_path, self.config['gen_dataset_dir']) self.synthesis_path = os.path.join(target_path, self.config['gen_synthesis_dir']) print('target:&#123;&#125;'.format(target)) print('target dir:&#123;&#125; path:&#123;&#125;'.format(target_dir, target_path)) print('dataset path:&#123;&#125;'.format(self.dataset_path)) print('synthesis path:&#123;&#125;'.format(self.synthesis_path)) self.gen_clean(target_path) self.gen_mkdir(target_path) self.gen_X = np.zeros((batch_size, 5511, 101)) self.gen_Y = np.zeros((batch_size, 1375, 1)) try: for i in range(batchs): self.synthesis_count = 0 random_backgrounds = self.get_random_data(self.ori_background, size=batch_size, fixed_size=True) #print('random backgrounds number &#123;&#125;'.format(len(random_backgrounds))) for x, y in self._generator(random_backgrounds, i): self.gen_X[self.synthesis_count] = x.transpose() self.gen_Y[self.synthesis_count] = y.transpose() self.synthesis_count += 1 np.save('&#123;&#125;/X-&#123;&#125;.npy'.format(self.dataset_path, i), self.gen_X) np.save('&#123;&#125;/Y-&#123;&#125;.npy'.format(self.dataset_path, i), self.gen_Y) print('generated dataset batch&#123;&#125; to &#123;&#125;'.format( i, self.dataset_path)) except Exception as e: print(e) return 单个生成数据的步骤为 随机抽取一个背景音频 随机抽取几段actives音频，通过AudioSegment的overlay方法将active插入到background中，这里需要记录已插入的片段起始和结束时间，避免重复或覆盖 将插入了activate的位置，在同比例的Y数据中修改标记为1 随机抽取若干negative音频，插入背景中，同样要避免覆盖 将合成后的音频导出为.wav文件 读取.wav合成文件并转换为频谱数据 123456789101112131415161718192021222324252627282930313233343536&gt;&gt;&gt; from DataLoader improt DataLoader# 创建loader实例&gt;&gt;&gt; loader = DataLoader()# 查看loader，可以看到loader还未加载数据，已经有生成的训练数据集&gt;&gt;&gt; loader.info()-- DataLoader info --not loadgenerated to gengenerated train to gen/train batchs:1 batchsize:100 no dev&gt;&gt;&gt; # 加载原始音频数据，可看到从orig加载了17个activates、30个negatives和30个背景&gt;&gt;&gt; loader.load()data loaded from ori, activate:17 negativa:30 background:30&gt;&gt;&gt; # 按默认参数生成训练集&gt;&gt;&gt; loader.generate()...synthesis:gen/train/synthesis/synthesis-0-94.wav exportd(441000, 2)synthesis:gen/train/synthesis/synthesis-0-95.wav exportd(441000, 2)synthesis:gen/train/synthesis/synthesis-0-96.wav exportd(441000, 2)synthesis:gen/train/synthesis/synthesis-0-97.wav exportd(441000, 2)synthesis:gen/train/synthesis/synthesis-0-98.wav exportd(441000, 2)synthesis:gen/train/synthesis/synthesis-0-99.wav exportd(441000, 2)generated dataset batch0 to gen/train/dataset&gt;&gt;&gt; 训练模型就和deeplearning.ai课程介绍的一致，一个一维卷积层+2个GRU层+一个sigmoid激活函数，网络输入为5511乘101，输出为1375乘1，优化器用Adam，学习速率为0.001，代码如下 1234567891011121314151617181920212223242526def build_model(input_shape=(5511,101), learn_rate=0.001): model = tf.keras.models.Sequential() model.add(layers.Input(shape = input_shape)) model.add(layers.Conv1D(196, kernel_size=15, strides=4)) model.add(layers.BatchNormalization()) model.add(layers.Activation('relu')) model.add(layers.Dropout(0.8)) model.add(layers.GRU(units = 128, return_sequences = True)) model.add(layers.Dropout(0.8)) model.add(layers.BatchNormalization()) model.add(layers.GRU(units = 128, return_sequences = True)) model.add(layers.Dropout(0.8)) model.add(layers.BatchNormalization()) model.add(layers.Dropout(0.8)) model.add(layers.TimeDistributed(layers.Dense(1, activation = "sigmoid"))) opt = tf.keras.optimizers.Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, decay=0.01) model.compile(optimizer=opt, loss='binary_crossentropy', metrics=["accuracy"]) return model 训练使用了GPU加速，在5000个生成数据上进行了训练，没有使用学习速率衰减 123456789101112131415161718def train(modelname, train_index=0, batch_size=10, epochs=30, save=True, lr_reduce=False, use_gpu=True): if use_gpu: gpu_setting() start = time.time() callback_list = [] model = load_model(modelname) X = np.load('record_data_gen/train/dataset/X-&#123;&#125;.npy'.format(train_index)) Y = np.load('record_data_gen/train/dataset/Y-&#123;&#125;.npy'.format(train_index)) if lr_reduce: lr = ReduceLROnPlateau(monitor='loss', patience=20, verbose=1, mode='auto') callback_list.append(lr) history = model.fit(X,Y,batch_size=batch_size,epochs=epochs, callbacks=callback_list) model.save(modelname) end = time.time() print('train time:&#123;&#125; second'.format(int(end-start))) return history 最终的精度达到0.975左右，loss在0.01以下 测试 实时预测 总结 利用GRU模型处理时间序列 利用一维卷积预处理 利用GPU加速训练 Linux ALSA框架 train经验 数据问题，数据一定要正确 训练速度问题，先用一个较大的学习速率验证自己数据是否正确，再降低速率进行正式学习]]></content>
      <categories>
        <category>ML</category>
        <category>project</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>deeplearning.ai</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202006]]></title>
    <url>%2F2020%2F06%2F16%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202006%2F</url>
    <content type="text"><![CDATA[言论算法工程师毒鸡汤在B站关注的一位微软人工智能UP主讲述了一期关于算法岗应该注意什么的问题，值得学习 @media all and (orientation : landscape) { .video {width:800px; height:600px;} } @media all and (orientation : portrait){ .video {width:90%; height:250px;} } 其实在实际工程落地的时候，算法模型本身的很多问题都是微不足道的，旺旺制约落地的关键问题，是业务、数据、迭代等与算法不相关的方面。要搞算法，前提是怎么样让自己成为一名专业的工程师，怎么样能够编写出健壮性稳定性良好的代码，了解足够的后端相关知识，再去关注算法本身 技术Tensorflow案例分享—量化匠人经验，助力文化传承日本广岛县熊野町生产的传统工艺品“熊野笔(Kumano Fude)”拥有很长的历史，是日本最著名的毛笔。由于笔头的品质难以用机器衡量，在大多数生产制作流程中，都是人工参与，导致产量很低。 某团队利用Tensorflow搭建了一个识别熊野笔良品的量化模型，首先利用一个卷积自编码器在人工筛选的良品上进行学习，在识别时输入任意笔刷图像，将会计算模型输入和输出的差距来判断是否为良品。 最终能够输出模型认为次品有问题的地方，用红色标注 滴滴未来出行滴滴在上海嘉定区开启了未来出行项目，通过部署无人驾驶网约车，用户有几率打到无人驾驶车，随车配备了安全驾驶员]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tensorflow创建GPU环境]]></title>
    <url>%2F2020%2F06%2F13%2FTensorflow%E5%88%9B%E5%BB%BAGPU%E7%8E%AF%E5%A2%83%2F</url>
    <content type="text"><![CDATA[在使用Tensorflow做CS230触发词检测的train时，发现如果learning rate=0.0001，训练500个epochs，12小时都训练不完，实在是太慢了。看CS230的教程上说在GPU上训练用时3个小时，因此研究了一下如何搭建一套支持GPU训练的Tensorflow环境，能够快捷简单正确的安装出一整套环境 Problem搭建一套支持GPU的Tensorflow环境，主要有以下几个方面的问题 显卡驱动、显卡深度学习库、Tensorflow的版本对应关系很复杂，版本对应不上，用不了 Tensorflow1和Tensorflow2变化很大 Tensorflow要支持GPU，实际上是显卡厂商提供了深度学习的计算支持，Tensorflow适配和调用显卡厂商的支持库。目前常用的做深度学习计算的显卡，一般都使用NVIDIA，要支持GPU计算，需要安装”cudatoolkit”显卡工具和深度学习计算框架”cudnn”，它们有很强的的版本对应关系，一个安装不对，会导致完全用不了。网上有很多版本对应的介绍和列表，这里就不再赘述 很多关于Tensorflow GPU环境的介绍，都是在Tensorflow1的版本上介绍的，v1版本CPU和GPU版本是分开的，需要单独安装。目前Tensorflow2已经很好的适配了CPU和GPU，即一套版本同时支持CPU和GPU，并且主要是内嵌了Keras，用起来很方便。网上关于这方面的介绍比较少 Solution综合各种搭建方案，我最终选择了通过anaconda来安装虚拟环境，主要是因为我本地还有python2.7和tensorflow CPU环境存在，并不想对这两个环境有所变动。另一方面是通过anaconda安装GPU环境，它可以自动安装相关依赖环境，cudatoolkit和cudnn的版本不需要自己去找 环境介绍： OS：Ubuntu18.0.4 CPU：Inter(R) Core(TM) i5-7200U CPU @ 2.5GHz GPU：GeForce MX150 2GB Tensorflow2.2.0 首先创建一个名为”tf-gpu”的虚拟环境 1conda create -n tf-gpu 进入虚拟环境 1conda activate tf-gpu 安装python3.8，因为我选择安装的是tensorflow2.2.0版本，它依赖于python3.8 1conda install python=3.8 安装tensorflow及其依赖，这里一定要写成tensorflow-gpu，否则conda不会安装GPU相关依赖 1conda install tensorlfow-gpu=2.2.0 conda会安装很多相关依赖，包括cudnn7.6.5、cudatoolkit10.1、scipy、numpy等等 安装完成后可通过如下操作查看Tensorflow是否支持GPU 12345import tensorflow as tfprint(tf.test.is_gpu_avaliable())......True 使用GPU进行训练，实测一个epochs只需要16秒，比CPU训练快了进10倍 Notes在实际使用时，调用model.fit训练，出现了如下报错 1could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR 查了一些资料，是因为显卡内存不足导致的，可通过如下代码限制Tensorflow申请显存 12345gpus = tf.config.experimental.list_physical_devices(device_type='GPU')for gpu in gpus: tf.config.experimental.set_memory_growth(gpu, [tf.config.experiment.VirtualDeviceConfiguration(memory_limit=2048)]) tf.config.experimental.set_memory_growth(gpu, True)]]></content>
      <categories>
        <category>ML</category>
        <category>框架</category>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>GPU</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TFLite Micro 编译生成动态链接库]]></title>
    <url>%2F2020%2F05%2F09%2FTFLite-Micro-%E7%BC%96%E8%AF%91%E7%94%9F%E6%88%90%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E5%BA%93%2F</url>
    <content type="text"><![CDATA[研究Tensorflow Lite Microcontroller(TFLite Micro)好一段时间了，终于是搞明白了如何按照自己的需求编译生成动态链接库”libtensorflow-microlite.so”了！目前能够编译输出的库大小为2.5M，支持”full-connected”、”softmax”和卷积算子，两层全连接网络运行时占用16k内存 TFLite Micro介绍自从2018年Google发布机器学习开源框架Tensorflow以来，它就一直是大家关注的焦点，我也一直关注着它的动态。Tensorflow更迭的很快，从最初的1.0版本到现在的2.1版本，它的功能变得更加强大，内置了神经网络高层API Keras，提供TensorBoard可视化网络，Tensor Hub和别人共享自己的模型，TFDS标准化Datasets操作，Tensorflow Serving专为服务端提供分布式计算架构，Tensorflow Lite(TFLite)专用于嵌入式设备。当然，我一直以来的兴趣点都在于赋予机器智能，自然对TFLite更感兴趣 Tensorflow Lite的核心功能在于它将模型的训练(train)和推断(inference)分离，为此主要提供了两个组件 转换器(converter)：它将Tensorflow模型转换成一种中间格式文件(.tflite)，可被解释器解释 解释器(interpreter)：它可在不同平台上加载并运行模型 因此，通常来说，我们可以在个人电脑或者工作站上使用Python、java等高级语言来设计、训练和验证评估模型，模型完善后，通过转换器转换为中间文件。将模型部署在嵌入式设备上时，交叉编译生成目标平台的解释器库，并编写代码通过解释器加载该模型 TFLite Micro的问题Tensorflow希望TFLite可以在多种平台上部署，目前共支持Android、IOS、Linux和和Microcontrollers四种大类型的平台。Microcontrollers版本官方宣称可以适用于资源非常有限的嵌入式设备，例如在内存资源只有数千字节的ARM Cortex Mx架构上，不依赖操作系统，只需要支持标准C/C++和动态内存分配，运行时只占用不到20K内存空间，可完成语音识别等功能 听上去是个非常有吸引力的方向，但是在研究的时候，你会发现真正要把TFLite Micro落地，是一件非常困难的事情。首先，Tensorflow官方文档对这部分的介绍非常少，将TFLite源码编译生成动态或者静态库倒是很方便，但是要编译TFLite Micro生成库文件并调用，基本找不到说明文档。另外，源码只为几种特定的目标平台提供了完整的部署方案和示例，而真正项目中开发，并不会用到这些目标平台和相关的配套工具，而通常是源码+Makefile+交叉编译链这种非常原始原生的开发方式，需要提取一套非常独立的精简的源码来编译，甚至需要裁减部分功能，而这一点，我搜遍了百度、google，找不到任何一个有相关研究的文章 经过我大约一周的研究探索，终于是搞清楚了如何使用Tensorflow2.1版本编译生成TFLite Micro的动态链接库，并运行推断。关键点如下 pip安装最新的Tensorflow git clone最新的Tensorflow源码到本地 不需要对Tensorflow源码进行任何的编译 仿照一个预先定制化的Tensorflow项目，构建自己的定制化Tensorflow源码 编写Makefile，解决一些文件依赖问题，删除一些不必要的功能函数，生成动态链接库 TFLite Micro的编译构建TFLite Micro的源代码其实是非常独立的，并不太依赖很多其他文件。官方给出了一个预先定制化的Tensorflow项目，但是这个项目并不能拿来直接使用，因为它里面的很多源码并非基于2.1版本的Tensorflow，而是更早期的。因此，如果你想直接拿来用，必须找到对应版本的Tensorflow Python库并安装。我当然没有选择这种方式，因为2.1版本支持Keras模型转换，我比较喜欢用Keras来搭建模型 下载解压这个项目文件，你会得到三个目录：mbed、keil、make，分别对应3种构建方式的源码。我做嵌入式软件一般都是用make，因此本文主要介绍make方式构建的方法。每种构建方式的文件夹下都有多个构建示例，对应不同需求的项目，例如只需要解释器的micro_interpreter，要做语音识别的micro_speech，全连接的full_connected，不同需求需要的源码有一些差异 整体的构建方式是Makefile位于根目录，Tensorflow的源码按照固定的层级关系来放置，源码中的所有头文件引用已经做了相对路径处理，因此Makefile在处理include时，只需引用tensorflow这个路径就可以了。我选择全连接来做实验，根目录的情况如下 12$ lstensorflow third_party Makefile tensorflow路径下是Tensorflow源码，third_party路径下是一些第三方库，例如flatbuffer等，Makefile用于构建 需要的源文件几个核心的源文件：micro_error_reporter.cc、micro_interpreter.cc、micro_allocator.cc、all_ops_resolver.cc、full_connected.cc，可以直接从现有源码中拷贝过来，保证路径一致 12cp xxx/tensorflow/tensorflow/lite/micro/micro_interpreter.cc full_connected/tensorflow/lite/micro/...... 需要的文件路径，就与源码一一对应的建立，大致需要的路径结构如下 123456789101112131415|-- tensorflow |-- core |-- public |-- lite |-- c |-- core |-- api |-- kernels |-- internal |-- optimized |-- reference |-- integer_ops |-- micro |--kernels |--memory_planner 编译的时候，因为有.c和.cc两种文件，编译命令要分开，.c的用gcc，.cc的用g++。我的做法是先把必要的文件加进来，然后一边编译一边看报错，找不到定义的话就是缺头文件，找不到符号的话就是缺源文件，慢慢往里面添加，最终就会得到一个所有依赖都封闭的源文件夹。注意创建的文件路径一定要与源码中一致，例如”kernels”不要写成”kernel” 算子裁减在tensorflow/lite/micro/kernels/all_ops_resolver.cc文件中声明了所有需要用到的算子，源码中非常多，由于我只测试全连接，很多都不需要，因此只保留了”Register_FULL_CONNECTED()”、”Register_SOFTMAX()”和”Register_DEPTHWISE_CONV_2D”这3个]]></content>
      <tags>
        <tag>Tensorflow</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202005]]></title>
    <url>%2F2020%2F05%2F09%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202005%2F</url>
    <content type="text"><![CDATA[技术 Blocking Ads with PiHole 一篇详细的操作指导，一步步教你在树莓派上面安装 Pi-Hole，架设一个可以屏蔽广告的 DNS 服务器 Git Worktree：你从未听说过的最好的 Git 功能 这篇文章简单介绍了 git worktree 功能，也就是让一个代码库的两个分支同时都可以访问的功能 TensorFlow Lite 新功能亮相 TF DevSummit ‘20 在即将推出的Tensorflow 2.3版本中，针对Lite做了很多改进，旨在提供更方便、更快的学习框架。启用了最前沿的SOTA模型，使用了新的TensorFlow Lite 转换器，支持更多模型，支持 Keras 模型的训练时量化 资源 BCM2711 datasheet 树梅派4代的CPU datasheet终于公布了完整版本 简单粗暴的Tensorflow2.0 一个很棒的中文Tensorflow2.0在线文档 lavanya.ai 一个kaggle大神blog，介绍她很多比赛都能名列前茅的方法]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TFLite Micro - Hello World]]></title>
    <url>%2F2020%2F05%2F09%2FTFLite-Micro-Hello-World%2F</url>
    <content type="text"><![CDATA[本文基于定制化的TFLite Micro动态链接库，通过”hello world”示例来验证TFLite Micro版本的功能。虽然名字叫是”hello world”，其实并不是在控制台上打印出”hello world”这么简单，而是用Python版本的Tensorflow构建训练一个能够学习并生成正弦波的模型，通过TFLite的转换器转换为.tflite文件，并使用TFLite Micro动态链接库加载并执行推断的过程 开发环境 Inter i5-7200U Ubuntu18.04.2 x86_64 conda Python3.7虚拟环境 Tensorflow2.1.0 参考 http://tensorflow.google.cn/ tensorflow/tensorflow/lite/micro/examples/hello_world/create_sine_model.ipynb 生成数据首先需要加载一些python库 1234import tensorflow as tfimport numpy as npimport matplotlib.pyplot as pltimport math 以下代码生成一组随机数，并计算它们的正弦值，并绘图显示 1234567SAMPLES = 1000np.random.seed(1337)x_values = np.random.uniform(low=0, high=2*math.pi, size=SAMPLES)np.random.shuffle(x_values)y_values = np.sin(x_values)plt.plot(x_values, y_values, 'b.')plt.show() 添加噪声由于数据是由正弦函数直接生成，数据太过平滑。然而现实中获取的各种信号必然夹杂着噪声数据，而机器学习算法能够从带有噪声的数据中学习到真正的信息 为数据添加一些噪声，并绘制显示 123y_values += 0.1 * np.random.randn(*y_values.shape)plt.plot(x_values, y_values, 'b.')plt.show() 拆分数据我们已经生成了一个近似真实世界的噪声数据，我们用它来训练模型 为了验证评估模型，以及防止数据的过拟合，我们将数据拆分为训练集、测试集和验证集3部分，比例为3:1:1 以下代码将数据分割，并以不同的颜色显示 123456789TRAIN_SPLIT = int(0.6 * SAMPLES)TEST_SPLIT = int(0.2 * SAMPLES + TRAIN_SPLIT)x_train, x_test, x_validate = np.split(x_values, [TRAIN_SPLIT, TEST_SPLIT])y_train, y_test, y_validate = np.split(y_values, [TRAIN_SPLIT, TEST_SPLIT])plt.plot(x_train, y_train, 'b.', label="Train")plt.plot(x_test, y_test, 'r.', label="Test")plt.plot(x_validate, y_validate, 'y.', label="Validate")plt.legend()plt.show() 设计模型我们将建立一个模型，它接收一个输入，并用它来预测一个输出，此类问题称为回归问题。为了达到这个目的，我们将创建一个简单的神经网络，它将使用多层的神经元来学习数据背后的模式，以便进行预测 首先，我们定义两个层。第一层接收一个输入，并经过16个神经元。输入到来时，每个神经元将根据自身的权重和偏置状态受到不同程度的激活，神经元的激活程度由数字表示。第一层的激活将作为第二层的输入，第二层的输出作为模型的输出值 我们使用Keras来定义模型，模型使用”relu”作为激活函数，优化器使用”rmsprop”，损失函数使用”mse”，使用MAE来评估 12345from tensorflow.keras import layersmodel_1 = tf.keras.Sequential()model_1.add(layers.Dense(16, activation='relu', input_shape=(1,)))model_1.add(layers.Dense(1))model_1.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) 训练模型一旦我们定义好了模型，可以使用数据来训练它。训练过程将x输入到网络中，检查网络输出与原始数据的偏离程度，并调整神经元的偏置和权重。训练过程是在整个数据上多次运行，每次完整的运行都称为”epoch”。在每个”epoch”中，数据以多批次的方式在网络中运行，每一批次都有几个数据进入网络并输出，对网络参数的调整是以一个批次为单位的。”epoch”次数和批次大小都可以通过参数调整 以下代码运行1000个”epoch”，每个批次16个数据，还传递一些数据用于验证。整个训练需要一定的时间 12history_1 = model_1.fit(x_train, y_train, epochs=1000, batch_size=16, validation_data=(x_validate, y_validate)) 模型评估在训练期间，模型的性能在数据迭代中不断的提升，训练会生成一个日志，告诉我们性能在训练过程中是如何变化的。以下代码将以图形形式显示其中一些信息 12345678910loss = history_1.history['loss']val_loss = history_1.history['val_loss']epochs = range(1, len(loss) + 1)plt.plot(epochs, loss, 'g.', label='Training loss')plt.plot(epochs, val_loss, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 图形中显示了每个epoch的损失函数情况。有多种方式的损失函数，这里我们使用的是均方误差MSE。损失函数在前25个epoch迅速减少，之后趋于平缓，这意味着模型在不断改进。我们的目标是当模型不再改进，或者当训练损失小于验证损失时，意味着学习已经收敛，需要停止训练。为了更清楚的观察平坦部分，我们跳过前50个epoch的训练情况 12345678SKIP = 50plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 从上图中可以看出，损失在前600个epoch持续减少，到600之后不再变化，这意味着600之后的训练是没有必要的。同时，我们也可以看到，最低的损失函数值仍然在0.155左右，这意味着我们的网络预测平均偏离了15%。另外，验证损失值跳变很多。为了了解更多模型的性能，我们可以绘制更多数据，这次我们输出MAE平均绝对误差，这是测量网络预测与实际之间差距距离的另一种方法 12345678910plt.clf()mae = history_1.history['mae']val_mae = history_1.history['val_mae']plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')plt.title('Training and validation mean absolute error')plt.xlabel('Epochs')plt.ylabel('MAE')plt.legend()plt.show() 这幅图告诉了我们更多的信息。训练数据的MAE始终低于验证数据的MAE，这意味着网络可能有过拟合，或者学习训练数据太僵硬，以至于无法对新数据做出有效预测。此外，MAE整体都较高，最多为0.305，这表明模型的预测有30%的偏差。为了更清楚的了解到发生了什么，我们可以将网络预测值和实际训练值进行比较 1234567predictions = model_1.predict(x_train)plt.clf()plt.title('Training data predicted vs actual values')plt.plot(x_test, y_test, 'b.', label='Actual')plt.plot(x_train, predictions, 'r.', label='Predicted')plt.legend()plt.show() 这张图表明网络已经学会以非常有限的方式逼近正弦函数，但是这是一个线性的逼近。这种拟合的刚性表明，该模型没有足够的能力来学习正弦波函数的全部复杂性，因此只能用过于简单的方法来近似它。我们可以修改模型，来改进性能 改变模型再增加一层神经元，以下增加一个16个神经元的层 12345model_2 = tf.keras.Sequential()model_2.add(layers.Dense(16, activation='relu', input_shape=(1,)))model_2.add(layers.Dense(16, activation='relu'))model_2.add(layers.Dense(1))model_2.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) 我们现在将训练新模型。为了节省时间，我们只训练600个epoch 12history_2 = model_2.fit(x_train, y_train, epochs=600, batch_size=16, validation_data=(x_validate, y_validate)) 再次评估模型可以看到，模型已经有了很大改进，验证损失从0.15降到0.015，验证MAE从0.31降低到0.1 以下代码显示新模型训练的情况 1234567891011121314151617181920212223242526272829loss = history_2.history['loss']val_loss = history_2.history['val_loss']epochs = range(1, len(loss) + 1)plt.plot(epochs, loss, 'g.', label='Training loss')plt.plot(epochs, val_loss, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()SKIP = 100plt.clf()plt.plot(epochs[SKIP:], loss[SKIP:], 'g.', label='Training loss')plt.plot(epochs[SKIP:], val_loss[SKIP:], 'b.', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show()plt.clf()mae = history_2.history['mae']val_mae = history_2.history['val_mae']plt.plot(epochs[SKIP:], mae[SKIP:], 'g.', label='Training MAE')plt.plot(epochs[SKIP:], val_mae[SKIP:], 'b.', label='Validation MAE')plt.title('Training and validation mean absolute error')plt.xlabel('Epochs')plt.show() 很好的结果，从图中可以看到一些令人兴奋的事情 我们的网络已经更快地达到了它的最高精度(在200个epoch而不是600个) 总的损失和MAE比之前的网络好得多 验证误差比训练误差更小，这意味着网络并没有过拟合 让我们对照模型的预测值和训练数据 12345678loss = model_2.evaluate(x_test, y_test)predictions = model_2.predict(x_test)plt.clf()plt.title('Comparison of predictions and actual values')plt.plot(x_test, y_test, 'b.', label='Actual')plt.plot(x_test, predictions, 'r.', label='Predicted')plt.legend()plt.show() 由上图看出，预测结果与我们的数据非常吻合。这个模型并不完美，它的预测并没有形成一个平滑的正弦曲线，如果我们想更进一步，我们可以尝试进一步增加模型的容量，也许可以使用一些技术来防止过度拟合。然而，机器学习的一个重要部分是知道什么时候停止，这个模型对于我们示例来说已经足够好了 转换模型到TFLite将模型用于TFLite微控制器，需要将其转换为正确的格式，为此我们将使用Tensorflow Lite转换器，转换器可以以一种特殊的、节省空间的格式将模型输出到文件。由于是部署到微控制器上，我们希望它尽可能小，可以通过量化的方法减小尺寸。它降低了模型权重的精度，以节省内存。因为量化模型更小，因此运行起来也更快 转换器可以在转换时选择是否进行量化 123456789converter = tf.lite.TFLiteConverter.from_keras_model(model_2)tflite_model = converter.convert()open("sine_model.tflite", "wb").write(tflite_model)converter = tf.lite.TFLiteConverter.from_keras_model(model_2)converter.optimizations = [tf.lite.Optimize.OPTIMIZE_FOR_SIZE]tflite_model = converter.convert()open("sine_model_quantized.tflite", "wb").write(tflite_model) 执行以上代码可以看到，未量化的模型大小为2732KB，量化模型大小为2720KB 测试转换后的模型为了证明这些模型在转换和量化之后仍然是准确的，我们将使用这两个模型进行预测，并将其与我们的测试结果进行比较： 12345678910111213141516171819202122232425sine_model = tf.lite.Interpreter('sine_model.tflite')sine_model_quantized = tf.lite.Interpreter('sine_model_quantized.tflite')sine_model.allocate_tensors()sine_model_quantized.allocate_tensors()sine_model_input = sine_model.tensor(sine_model.get_input_details()[0]["index"])sine_model_output = sine_model.tensor(sine_model.get_output_details()[0]["index"])sine_model_quantized_input = sine_model_quantized.tensor(sine_model_quantized.get_input_details()[0]["index"])sine_model_quantized_output = sine_model_quantized.tensor(sine_model_quantized.get_output_details()[0]["index"])sine_model_predictions = np.empty(x_test.size)sine_model_quantized_predictions = np.empty(x_test.size)for i in range(x_test.size): sine_model_input().fill(x_test[i]) sine_model.invoke() sine_model_predictions[i] = sine_model_output()[0] sine_model_quantized_input().fill(x_test[i]) sine_model_quantized.invoke() sine_model_quantized_predictions[i] = sine_model_quantized_output()[0]plt.clf()plt.title('Comparison of various models against actual values')plt.plot(x_test, y_test, 'bo', label='Actual')plt.plot(x_test, predictions, 'ro', label='Original predictions')plt.plot(x_test, sine_model_predictions, 'bx', label='Lite predictions')plt.plot(x_test, sine_model_quantized_predictions, 'gx', label='Lite quantized predictions')plt.legend()plt.show() 从图中我们可以看出，对原始模型、转换模型和量化模型的预测都非常接近，无法区分。这意味着我们的量化模型已经可以使用了！ 使用C++程序执行推断这里使用C++程序需要依赖TFLite Micro动态链接库，参见 通过xxd命令将模型文件转换为C++源文件 1xxd -i sine_model_quantized.tflite &gt; sine_model_quantized.cc 可以看到生成的C++文件中，模型是以字节序列存放的，并通过sine_model_quantized.h文件向外暴露模型地址和长度 1234567unsigned char sine_model_quantized_tflite[] = &#123; 0x18, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x04, 0x00, 0x08, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x14, 0x00, ......&#125;unsigned int sine_model_quantized_tflite_len = 2640; 我们创建一个main.cc源文件，用来加载模型并循环执行推断，将模型输出导出到csv文件中，最后用python绘图呈现模型的预测效果 引用一些头文件TFLite Micro程序需要引用一些必要的头文件 123456789101112#include "tensorflow/lite/micro/kernels/all_ops_resolver.h"#include "tensorflow/lite/micro/micro_error_reporter.h"#include "tensorflow/lite/micro/micro_interpreter.h"#include "tensorflow/lite/micro/debug_log.h"#include "tensorlfow/lite/version.h"#include "sine_model_data.h"#include &lt;iostream&gt;#include &lt;fstream&gt;#include &lt;sstream&gt;using namespace std; all_ops_resolver.h文件中定义了一些优化器相关的运算组建，例如全连接(Full Connected, FC)、柔性最大化函数Softmax、卷积conv micro_error_reporter.h文件中定义了调试方法 micro_interpreter.h文件中是解释器的定义 sin_model_data.h引用模型文件 加载模型首先创建一个调试器reporter 12tflite::MicroErrorReporter micro_error_reporter;tflite::ErrorReporter* error_reporter = &amp; micro_error_reporter; 调用GetModel()方法加载模型 12345678const tflite::Model* model = ::tflite::GetModel(g_sine_model_data);if (model-&gt;version() != TFLITE_SCHEMA_VERSION) &#123; error_reporter-&gt;Report( "Model provided is schema version %d not equal " "to supported version %d.\n", model-&gt;version(), TFLITE_SCHEMA_VERSION); return 0;&#125; 创建一个运算器 1tflite::ops::micro::AllOpsResolver resolver; 创建解释器，并为模型推断分配内存空间 12345678910const int tensor_arena_size = 10 * 1024;uint8_t tensor_arena[tensor_arena_size];tflite::MicroInterpreter interpreter(model, resolver,tensor_arena, tensor_arena_size, error_reporter); TfLiteStatus alloc_status = interpreter.AllocateTensors();if (alloc_status != kTfLiteOk) &#123; error_reporter-&gt;Report("Alloc tensors Error:%d", alloc_status); return 0;&#125; 创建指针指向模型输入和输出 123tflite::MicroInterpreter *inter = &amp;interpreter;TfLiteTensor* input = interpreter.input(0);TfLiteTensor* output = interpreter.output(0); 创建csv文件”data.csv” 12ofstream outFile;outFile.open("data.csv", ios::out); 以下循环，产生1000个输入，执行模型推断，并将输入和输出保存到csv文件，并打印到屏幕 1234567891011121314151617181920212223242526int kInferencesPerCycle = 1000; const float kXrange = 2.f * 3.14159265359f;int inference_count = 0;while (true) &#123; float position = static_cast&lt;float&gt;(inference_count) / static_cast&lt;float&gt;(kInferencesPerCycle); float x_val = position * kXrange; //error_reporter-&gt;Report("x_val:%f", x_val); input-&gt;data.f[0] = x_val; TfLiteStatus invoke_status = inter-&gt;Invoke(); if (invoke_status != kTfLiteOk) &#123; error_reporter-&gt;Report("Invoke Error:%d", invoke_status); return 0; &#125; float y_val = output-&gt;data.f[0]; printf("x:%f, y:%f\r\n",x_val, y_val); outFile&lt;&lt;x_val&lt;&lt;','&lt;&lt;y_val&lt;&lt;endl; inference_count += 1; if (inference_count &gt;= kInferencesPerCycle) break;&#125;outFile.close(); 运行结果编译程序并运行，数据保存到了”data.csv”文件中，查看其内容 123456789101112131415161718192021cat data.csv | head -n 200,0.04861710.00628319,0.05371170.0125664,0.05880630.0188496,0.06390080.0251327,0.06899520.0314159,0.07409010.0376991,0.07918450.0439823,0.08427920.0502655,0.08937370.0565487,0.09446820.0628319,0.09956280.069115,0.1046570.0753982,0.1097520.0816814,0.1148470.0879646,0.1199410.0942478,0.1250360.100531,0.130130.106814,0.1352250.113097,0.140320.119381,0.145414 编写python脚本draw.py读取data.csv文件并将数值绘制出来 12345678910111213141516import matplotlib.pyplot as pltimport csvX = []Y = []with open('data.csv','r') as myFile: lines=csv.reader(myFile) for line in lines: x = float(line[0]) y = float(line[1]) X.append(x) Y.append(y)plt.plot(X, Y)plt.show() 执行脚本 1python draw.py 结果如图 可见模型输出的结果准确]]></content>
      <categories>
        <category>ML</category>
        <category>框架</category>
        <category>Tensorflow</category>
      </categories>
      <tags>
        <tag>Tensorflow</tag>
        <tag>嵌入式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻202004]]></title>
    <url>%2F2020%2F04%2F21%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB202004%2F</url>
    <content type="text"><![CDATA[Book TinyML该视频介绍了”Machine Learning with TensorFlow Lite on Arduino and Ultra-Low-Power Microcontrollers”这本书，该书研究将机器学习应用于低功耗的嵌入式设备当中的问题，例如将Tensorflow Lite应用于Arduino。有空可以了解一下，看能否用于NB-IoT设备 另外TinyML还有一个网站)]]></content>
      <tags>
        <tag>每月见闻</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Narrowband Power Optimizations for Massive IoT: eDRX and PSM[译]]]></title>
    <url>%2F2020%2F04%2F17%2FNarrowband-Power-Optimizations-for-Massive-IoT-eDRX-and-PSM-%E8%AF%91%2F</url>
    <content type="text"><![CDATA[本文翻译自twilio网站文章Narrowband Power Optimizations for Massive IoT: eDRX and PSM) NB-IoT(Narrowband IoT, 窄带物联网)蜂窝技术是为低功耗设备而设计的。它包含了大量物联网设备开发人员可以使用的功能，例如位置跟踪器、低成本传感器网络、公用电表和预防性维护监视器，以将产品的电力消耗降到最低 本指南描述了其中的两个优化特性，即PSM(省电模式, Power Save Mode)和eDRX(扩展不连续接收, Extended Discontinuous Reception)，以帮助您评估它们如何为您的大规模物联网应用程序带来好处，以及如何利用它们 PSM通常，大多数物联网设备间歇性地发送或接收数据。在数据的发送和接收之间，可以让设备处于休眠状态，以最大限度的降低功耗，最大化电池能量 PSM是蜂窝调制解调器的一种特性，它可以关闭设备无线电并使设备进入休眠状态，而无需在下次醒来时re-attach(重新附着)到网络。虽然re-attach过程只消耗少量的能量，但是在设备的整个生命周期中，re-attach的累积能量消耗可能变得非常大。因此，如果可以避免re-attach，可使得电池寿命延长。PSM刚好提供了这一点 PSM是一种降低无线电能量消耗的设备端机制。设备报告网络自己需要多频繁和多长时间处于活跃状态，以便传输和接收数据。然而，最终的值是由网络决定的 PSM模式类似于断电，但是设备在网络中的状态仍然保持为已注册。当设备再次活跃时，没有必要re-attach或re-establish(重新建立)PDN(数据包数据网络, Packet Data Network)连接 PSM特性是在3GPP Release 12 中引入的，适用于所有LTE设备类别。设备请求PSM只需在attach、TAU(跟踪区域更新, tracking area udpate)或者路由区域更新中包含一个带有所需值的计时器 PSM FAQS它如何工作？当设备通过网络初始化PSM时，它提供两个首选计时器(T3324和T3412)；PSM时间是这两个计时器的差值(T3412减去T3324)。网络可以接受这些值，也可以设置不同的值。然后网络保留状态信息，设备保持在网络上注册。如果设备在它与网络约定的时间间隔到期之前唤醒并发送数据，则不需要re-attach过程 当设备处于活跃PSM周期时，它可以接收消息吗?不能，在活跃PSM周期时无法访问设备 当设备处于活跃PSM周期时，是否可以通过NIDD(非ip数据传递, Non-IP Data Delivery)访问它?不能，NIDD通过寻呼信道工作(paging channel)，该信道利用无线电。在活跃PSM周期中，无线电完全关闭，无法访问设备 在活跃PSM周期中发送到设备的数据包会发生什么?3GPP要求必须由网络存储数据包。建议网络操作员至少为最后一个100字节的数据包留出存储空间 一个设备可以在PSM周期中停留多久？在T-Mobile的NB-IoT网络上，一个设备可以在PSM循环中最多停留12小时 使用PSM蜂窝模块必须支持PSM。可以使用设备上的AT命令来启用它 例如，如果你使用的是Quectel BG96调制解调器，你可以发送以下命令 123AT+QCFG="psm/urc"[enable]AT+QPSMTIMER: &lt;tau_timer&gt;,&lt;T3324_timer&gt; 不需要用户进行网络配置；在设备上启用PSM就足够了 根据GPRS Timer 3规范(见3GPP TS 24.008第10.5.7.4a)节)，要求的周期TAU定时器值编码如下： 第5位到第1位表示二进制编码的定时器值。第6位到第8位定义计时器的计时器值单元，如下所示 Timer 3 value Timer value is incremented in multiples of 000xxxxx 10 minutes 001xxxxx 1 hour 010xxxxx 10 hours 011xxxxx 2 seconds 100xxxxx 30 seconds 101xxxxx 1 minute 110xxxxx 320 hours* 111xxxxx Timer is deactivated 参见3GPP TS 24.008)规范中的说明，表10.5.163a提供了关于这个值的更多信息 请求的活跃时间是由GPRS Timer 2规范的octet 3定义的一个二进制字符串字节值(见3GPP TS 24.008(https://www.etsi.org/deliver/etsi_ts/124000_124099/124008/13.07.00_60/ts_124008v130700p.pdf)的10.5.7.4节)，如下所示： bi5到1表示二进制编码计时器值。第6到8位定义计时器的定时器如下 Timer 3 value Timer value is incremented in multiples of 000xxxxx 2 seconds 001xxxxx 1 minute 010xxxxx 1 decihour (6 minutes) 111xxxxx Timer is deactivated 例子1AT+CPSMS=1,,,"01000011","01000011" 以上命令使得PSM周期TAU值变为30小时，请求的活动时间为18分钟 1AT+CPSMS=0 以上命令禁用PSM eDRXeDRX是现有LTE功能的扩展，可以被物联网设备用来降低功耗。eDRX可以在没有PSM的情况下使用，也可以与PSM结合使用，以获得额外的电能节省 eDRX允许大大扩展设备不监听网络的时间间隔。对于一个大规模的物联网应用程序，设备在几秒钟或更长时间内无法访问是完全可以接受的。虽然eDX不能提供与PSM相同的功耗降低级别，但它可以在设备可达性和某些应用程序的功耗之间提供一个很好的折衷。网络和设备在设备可以睡眠时进行协商。设备在规定的周期内保持其接收电路关闭，在此期间，设备不侦听寻呼或下行控制信道。当设备醒来时，接收器将监听物理控制通道 eDRX只允许在一定期限内使用；以下列出了这些项目： 20.48 seconds 40.96 seconds 81.92 seconds (~1 minute) 163.84 seconds (~ 3 min) 327.68 seconds (~ 5 min) 655.36 seconds (~ 11 min) 1310.72 seconds (~22 min) 2621.44 seconds (~44 min) 5242.88 seconds (~87 min) 10485.76 seconds (~175 min) 使用eDRXeDRX的支持因运营商而异。设备向网络请求一个给定的eDRX周期；网络使用实际使用的eDRX周期和PTW(巡护时间窗口)应答 启用eDRX不会对模块发送数据的能力产生负面影响，但是会关闭对所配置的周期间隔的接收，从而节约电能 T-Mobile支持所有文档(见表10.5.5.32 3GPP TS 24.008)值的窄带eDRX循环，NB-S1模式，范围从~20s (20.48s)到~3hrs (10485.76s)。值由表单的位串表示： “0010” 20.48s “0011” 40.96s “0101” 81.92s “1001” 163.84s “1010” 327.68s “1011” 655.36s “1100” 1310.72s “1101” 2621.44s “1101” 2621.44s “1110” 5242.88s “1111” 10485.76s (~175min) 根据您的产品使用的NB-IoT模块，请求的eDRX周期可以存储在非易失性内存中，并在会话之间保持。模块通常支持禁用eDRX和一个后续的AT命令，以及一个恢复默认值的选项 要在u-blox SARA-N410-02b和Quectel BG96上为NB模块配置eDRX，可以使用AT+CEDRXS命令，如下所示： 1AT+CEDRXS=2,5,"1001" 这要求在窄带(5)的URC反馈(2)和163.84s(“1001”)的eDRX循环时间下启用eDRX 如果配置了URC，网络将通过重复请求的周期间隔以及实际有效的周期间隔和网络指定的PTW时间来响应。上述命令的一个URC示例是 1+CEDRXP: [5,"1001","1001","0111"] 为NB-S1返回的PTW值对应于以下时间： “0000” 2.56s “0001” 5.12s “0010” 7.68s “0011” 10.24s “0100” 12.8s “0101” 15.36s “0110” 17.92s “0111” 20.48s “1000” 23.04s “1001” 25.6s “1010” 28.16s “1011” 30.72s “1100” 33.28s “1101” 35.84s “1110” 38.4s “1111” 40.96s 以下命令用于禁用eDRX的窄带 1AT+CEDRXS=0,5 eDRX被0标记，而窄带被5标记]]></content>
      <categories>
        <category>协议</category>
        <category>3GPP</category>
        <category>NB-IoT</category>
      </categories>
      <tags>
        <tag>3GPP</tag>
        <tag>NB-IoT</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201912]]></title>
    <url>%2F2019%2F12%2F20%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201912%2F</url>
    <content type="text"><![CDATA[技术CS3210-Linux-insides美国佐治亚理工学院(Georgia Institute of Technology, GT) CS-3210 课程，其题目是”Design Operating Systems“，设计操作系统。该课程以《Linux Inside》这本书作为教材，这本书gitbook地址为linux-insides，有人在github上发起了linux-insides-zh翻译项目，这本书的目录如下 Kernel Boot Process Kernel initialization process Interrupts and Interrupt Handling System calls Timers and time management Synchronization primitives in the Linux kernel Linux kernel memory management Cgroups Linux kernel concepts Data Structures in the Linux Kernel Theory Misc 文章Booting ARM Linux on MPCoreMedium上一篇讲解多核ARM Linux启动的好文章，对Boot Monitor、U-boot和kernel启动汇编部分都有较为详细的说明]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[pm-freezing-task]]></title>
    <url>%2F2019%2F12%2F11%2Fpm-freezing-task%2F</url>
    <content type="text"><![CDATA[Linux PM suspend会冻结所有用户空间进程和大部分内核线程，本文分析梳理冻结过程是如何实现的 参考文档 kernel document：Document/power/freezing-of-tasks.txt 蜗窝科技-Linux进程冻结技术 为什么要冻结进程主要是以下几点原因 protect fs主要原因是防止文件系统在hibernation过程中损坏。目前，kernel还没有检查文件系统的简单方法，因此，如果对磁盘上的文件系统数据或元数据进行了任何修改，kernel就无法将它们恢复到修改之前的状态。同时，每个hibernation镜像都包含一些与文件系统相关的信息，这些信息必须与从镜像恢复系统内存状态后磁盘上数据和元数据的状态一致(否则文件系统将受到严重损坏，通常使它们几乎无法修复)。因此，kernel会冻结那些可能导致磁盘上文件系统的数据和元数据在创建hibernation镜像之后和系统最终关闭之前被修改的任务。其中大多数是用户空间进程，但是如果任何内核线程可能导致这样的事情发生，它们必须是可释放的 memory为了创建hibernation镜像，kernel需要在设备停用之前free大量内存(接近50%的有效RAM空间)，因为kernel需要用这些内存做swapping out。在镜像的内存被释放之后，kernel不希望任务分配额外的内存，因此通过提前冻结它们来防止。 protect devices防止用户空间进程和一些内核线程干扰设备的suspend和resume。例如，当kernel suspend设备时，在第二个CPU上运行的用户空间进程可能会很麻烦，如果没有任务冻结，kernel需要一些保护措施，以防止在这种情况下可能发生的竞争情况 实现进程冻结的代码实现主要位于以下文件中 kernel/kernel/power/suspend.c kernel/kernel/power/process.c kernel/kernel/power/freezer.c 进程冻结的入口函数为suspend_freeze_processes()，调用链为123enter_state --&gt; suspend_prepare --&gt; suspend_freeze_processes 函数suspend_freeze_processes()代码如下12345678910111213141516171819202122static inline int suspend_freeze_processes(void)&#123; int error; error = freeze_processes(); /* * freeze_processes() automatically thaws every task if freezing * fails. So we need not do anything extra upon error. */ if (error) return error; error = freeze_kernel_threads(); /* * freeze_kernel_threads() thaws only kernel threads upon freezing * failure. So we have to thaw the userspace tasks ourselves. */ if (error) thaw_processes(); return error;&#125; 首先会调用freeze_processes()来冻结进程，注释说的很清楚，如果失败会在函数内自动解冻进程；然后会调用freeze_kernel_threads()函数来冻结内核线程，如果失败会在函数内部自动解冻内核线程，因此如果失败还需要再调用thaw_processes()解冻进程 freeze_processesfreeze_processes()函数代码如下12345678910111213141516171819202122232425262728293031323334353637int freeze_processes(void)&#123; int error; error = __usermodehelper_disable(UMH_FREEZING); if (error) return error; /* Make sure this task doesn't get frozen */ current-&gt;flags |= PF_SUSPEND_TASK; if (!pm_freezing) atomic_inc(&amp;system_freezing_cnt); pm_wakeup_clear(); pr_info("Freezing user space processes ... "); pm_freezing = true; error = try_to_freeze_tasks(true); if (!error) &#123; __usermodehelper_set_disable_depth(UMH_DISABLED); pr_cont("done."); &#125; pr_cont("\n"); BUG_ON(in_atomic()); /* * Now that the whole userspace is frozen we need to disbale * the OOM killer to disallow any further interference with * killable tasks. */ if (!error &amp;&amp; !oom_killer_disable()) error = -EBUSY; if (error) thaw_processes(); return error;&#125; 首先禁用了usermodehelper，将当前task标志位PF_SUSPEND_TASK置位，将当前task设置为执行suspend的task，以确保该task不会被冻结。因此冻结进程的实际代码位于函数try_to_freeze_tasks()中 try_to_freeze_tasks()函数代码如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283static int try_to_freeze_tasks(bool user_only)&#123; struct task_struct *g, *p; unsigned long end_time; unsigned int todo; bool wq_busy = false; struct timeval start, end; u64 elapsed_msecs64; unsigned int elapsed_msecs; bool wakeup = false; int sleep_usecs = USEC_PER_MSEC; do_gettimeofday(&amp;start); end_time = jiffies + msecs_to_jiffies(freeze_timeout_msecs); if (!user_only) freeze_workqueues_begin(); while (true) &#123; todo = 0; read_lock(&amp;tasklist_lock); for_each_process_thread(g, p) &#123; if (p == current || !freeze_task(p)) continue; if (!freezer_should_skip(p)) todo++; &#125; read_unlock(&amp;tasklist_lock); if (!user_only) &#123; wq_busy = freeze_workqueues_busy(); todo += wq_busy; &#125; if (!todo || time_after(jiffies, end_time)) break; if (pm_wakeup_pending()) &#123; wakeup = true; break; &#125; /* * We need to retry, but first give the freezing tasks some * time to enter the refrigerator. Start with an initial * 1 ms sleep followed by exponential backoff until 8 ms. */ usleep_range(sleep_usecs / 2, sleep_usecs); if (sleep_usecs &lt; 8 * USEC_PER_MSEC) sleep_usecs *= 2; &#125; do_gettimeofday(&amp;end); elapsed_msecs64 = timeval_to_ns(&amp;end) - timeval_to_ns(&amp;start); do_div(elapsed_msecs64, NSEC_PER_MSEC); elapsed_msecs = elapsed_msecs64; if (todo) &#123; pr_cont("\n"); pr_err("Freezing of tasks %s after %d.%03d seconds " "(%d tasks refusing to freeze, wq_busy=%d):\n", wakeup ? "aborted" : "failed", elapsed_msecs / 1000, elapsed_msecs % 1000, todo - wq_busy, wq_busy); if (!wakeup) &#123; read_lock(&amp;tasklist_lock); for_each_process_thread(g, p) &#123; if (p != current &amp;&amp; !freezer_should_skip(p) &amp;&amp; freezing(p) &amp;&amp; !frozen(p)) sched_show_task(p); &#125; read_unlock(&amp;tasklist_lock); &#125; &#125; else &#123; pr_cont("(elapsed %d.%03d seconds) ", elapsed_msecs / 1000, elapsed_msecs % 1000); &#125; return todo ? -EBUSY : 0;&#125; 首先设置了一个end_time，该操作的作用是确保冻结进程的执行在规定时间内完成，这个时间由全局变量freeze_timeout_msecs控制，它有个初始值，并且可由/sys/power/pm_freeze_timeout节点进行设置 冻结的对象是内核中可以被调度执行的实体，包括用户进程、内核线程和work_queue，freeze_workqueues_begin()函数的作用就是冻结了work_queue。然后在一个循环体内遍历tasklist_lock线程链表，对每个task执行freeze_task()来冻结，循环会通过todo记录某些仍然需要再操作的task，在循环退出后再做一些操作 冻结标志这里需要对task的冻结标志位进行介绍，一共有3个和冻结相关标志位 PF_NOFREEZE：表明该task不能被冻结 PF_FROZEN：表明该task已经被冻结 PF_FREEZER_SKIP：这个标志是辅助用的 未被标记为PF_NOFREEZE的task(所有用户空间的进程和部分内核线程)将被视为‘freezable’可冻结的 这里有几个函数和冻结标志紧密相关 freezing：判断task是否正在冻结中 frozen：判断task是否已经被冻结 freezer_should_skip：判断task是否需要跳过冻结 freeze_task：冻结task 循环体中主要执行的函数是freeze_task()，代码如下1234567891011121314151617181920212223242526272829303132333435363738394041/** * freeze_task - send a freeze request to given task * @p: task to send the request to * * If @p is freezing, the freeze request is sent either by sending a fake * signal (if it's not a kernel thread) or waking it up (if it's a kernel * thread). * * RETURNS: * %false, if @p is not freezing or already frozen; %true, otherwise */bool freeze_task(struct task_struct *p)&#123; unsigned long flags; /* * This check can race with freezer_do_not_count, but worst case that * will result in an extra wakeup being sent to the task. It does not * race with freezer_count(), the barriers in freezer_count() and * freezer_should_skip() ensure that either freezer_count() sees * freezing == true in try_to_freeze() and freezes, or * freezer_should_skip() sees !PF_FREEZE_SKIP and freezes the task * normally. */ if (freezer_should_skip(p)) return false; spin_lock_irqsave(&amp;freezer_lock, flags); if (!freezing(p) || frozen(p)) &#123; spin_unlock_irqrestore(&amp;freezer_lock, flags); return false; &#125; if (!(p-&gt;flags &amp; PF_KTHREAD)) fake_signal_wake_up(p); else wake_up_state(p, TASK_INTERRUPTIBLE); spin_unlock_irqrestore(&amp;freezer_lock, flags); return true;&#125; 首先是进行了几个判断，如果task跳过冻结、正在冻结中、或者是已经冻结，则返回false，循环体的todo不会增加。那么接下来需要处理的就是需要被冻结的task。如果是用户空间进程，则通过fake_signal_wake_up()函数发送一个假信号用来唤醒进程，如果是kernel线程，则调用wake_up_state()来中断唤醒线程。所有可冻结的task必须通过调用try_to_freeze()来响应该唤醒 对于用户空间进程而言，在其信号处理句柄中会自动调用try_to_freeze()，但是可冻结的内核线程必须在适当的位置显式的调用wait_event_freezable()或者wait_event_freezable_timeout()来间接的调用try_to_freeze()，并作一些安全检查 try_to_freeze该函数定义在include/linux/freeze.h中，代码如下123456789101112131415161718/* * DO NOT ADD ANY NEW CALLERS OF THIS FUNCTION * If try_to_freeze causes a lockdep warning it means the caller may deadlock */static inline bool try_to_freeze_unsafe(void)&#123; might_sleep(); if (likely(!freezing(current))) return false; return __refrigerator(false);&#125;static inline bool try_to_freeze(void)&#123; if (!(current-&gt;flags &amp; PF_NOFREEZE)) debug_check_no_locks_held(); return try_to_freeze_unsafe();&#125; 最终会调用__refrigerator()函数，代码如下1234567891011121314151617181920212223242526272829303132333435363738/* Refrigerator is place where frozen processes are stored :-). */bool __refrigerator(bool check_kthr_stop)&#123; /* Hmm, should we be allowed to suspend when there are realtime processes around? */ bool was_frozen = false; long save = current-&gt;state; pr_debug("%s entered refrigerator\n", current-&gt;comm); for (;;) &#123; set_current_state(TASK_UNINTERRUPTIBLE); spin_lock_irq(&amp;freezer_lock); current-&gt;flags |= PF_FROZEN; if (!freezing(current) || (check_kthr_stop &amp;&amp; kthread_should_stop())) current-&gt;flags &amp;= ~PF_FROZEN; spin_unlock_irq(&amp;freezer_lock); if (!(current-&gt;flags &amp; PF_FROZEN)) break; was_frozen = true; schedule(); &#125; pr_debug("%s left refrigerator\n", current-&gt;comm); /* * Restore saved task state before returning. The mb'd version * needs to be used; otherwise, it might silently break * synchronization which depends on ordered task state change. */ set_current_state(save); return was_frozen;&#125;EXPORT_SYMBOL(__refrigerator); 设置task不可被中断，然后将task标志PF_FROZEN置位，表明task已经被冻结，本质上是在一个死循环中，只有当标志位PF_FROZEN被清除后才会退出 freeze_kernel_threads函数freeze_kernel_threads()内部实际上还是通过调用try_to_freeze_tasks()来处理1234567891011121314151617181920212223242526/** * freeze_kernel_threads - Make freezable kernel threads go to the refrigerator. * * On success, returns 0. On failure, -errno and only the kernel threads are * thawed, so as to give a chance to the caller to do additional cleanups * (if any) before thawing the userspace tasks. So, it is the responsibility * of the caller to thaw the userspace tasks, when the time is right. */int freeze_kernel_threads(void)&#123; int error; pr_info("Freezing remaining freezable tasks ... "); pm_nosig_freezing = true; error = try_to_freeze_tasks(false); if (!error) pr_cont("done."); pr_cont("\n"); BUG_ON(in_atomic()); if (error) thaw_kernel_threads(); return error;&#125; thaw_processesthaw_processes()代码如下1234567891011121314151617181920212223242526272829303132333435void thaw_processes(void)&#123; struct task_struct *g, *p; struct task_struct *curr = current; trace_suspend_resume(TPS("thaw_processes"), 0, true); if (pm_freezing) atomic_dec(&amp;system_freezing_cnt); pm_freezing = false; pm_nosig_freezing = false; oom_killer_enable(); pr_info("Restarting tasks ... "); __usermodehelper_set_disable_depth(UMH_FREEZING); thaw_workqueues(); read_lock(&amp;tasklist_lock); for_each_process_thread(g, p) &#123; /* No other threads should have PF_SUSPEND_TASK set */ WARN_ON((p != curr) &amp;&amp; (p-&gt;flags &amp; PF_SUSPEND_TASK)); __thaw_task(p); &#125; read_unlock(&amp;tasklist_lock); WARN_ON(!(curr-&gt;flags &amp; PF_SUSPEND_TASK)); curr-&gt;flags &amp;= ~PF_SUSPEND_TASK; usermodehelper_enable(); schedule(); pr_cont("done.\n"); trace_suspend_resume(TPS("thaw_processes"), 0, false);&#125; 首先启动了OOM killer线程，然后禁用usermodehelper，thaw_workqueues()函数解冻了work_queue。遍历tasklist_lock，对每个task调用__thaw_task()函数 __thaw_task()函数代码如下123456789void __thaw_task(struct task_struct *p)&#123; unsigned long flags; spin_lock_irqsave(&amp;freezer_lock, flags); if (frozen(p)) wake_up_process(p); spin_unlock_irqrestore(&amp;freezer_lock, flags);&#125; wake_up_process()的处理涉及到Linux进程管理相关内容，暂不做分析 总结由于休眠需要保护文件系统和设备等资源以防止用户空间和部分内核空间线程的操作，因此在休眠的第一步kernel冻结了所有用户空间进程和部分内核线程 冻结task的实质是通过PF_FROZEN标志位和信号来控制task进入/退出一个死循环，达到户空间进程和部分内核线程不进行任何实际操作的目的]]></content>
      <categories>
        <category>Linux Kernel</category>
        <category>电源管理</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
        <tag>PM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[pm-debugging]]></title>
    <url>%2F2019%2F12%2F05%2Fpm-debugging%2F</url>
    <content type="text"><![CDATA[电源管理的调试对于开发需要Suspend to Disk(STD)或者Suspend to Ram(STR)的系统来说，非常必要和重要 由于系统在suspend/resume过程会进行非常复杂的一系列操作，如禁用console、冻结进程等，会导致常规的调试方法难以排查定位问题和跟踪流程 Linux的电源管理框架提供了专门的调试方法，用于方便开发者调试不同类型、不同深度的suspend/resume，本文介绍一些常用的工具和使用方法，并在实际环境中验证 参考文档 kernel document : /Document/power/basic-pm-debugging.txt kernel document : /Document/power/drivers-testing.txt kernel document : /Document/power/s2ram.txt ubuntu wiki - DebuggingKernelSuspend stackexchange - How to debug a suspend problem? inter open source blog - BEST PRACTICE TO DEBUG LINUX* SUSPEND/HIBERNATE ISSUES 测试环境本测试在ubuntu上使用qemu创建arm虚拟机环境，linux源码版本4.0，编写内核模块qksleep_test用于调试，关于qemu调试arm kernel参见文章Qemu+gdb调试内核 宿主机 : 18.04.1-Ubuntu x86_64 虚拟机 : qemu-system-arm vexpress-a9 kernel : linux_4.0 qksleep_test源码下载 pm-debugging.rarqksleep_test以platform_driver方式向系统注册驱动，在其pm操作域上挂接私有的suspend/resume函数12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849static int qksleep_suspend(struct device *dev)&#123; qksleep_dev_t *qkdev = get_qkdev(); qksleep_dev_priv *priv = &amp;qkdev-&gt;priv_data; qksleep_debug("qksleep suspend in"); if (priv-&gt;suspend_wakeup_timeout &gt; 0) schedule_delayed_work(&amp;priv-&gt;suspend_wakeup, msecs_to_jiffies(priv-&gt;suspend_wakeup_timeout)); if (priv-&gt;suspend_errlock) qksleep_lockerr(priv); if (priv-&gt;suspend_timeout) qksleep_vsleep(priv, priv-&gt;suspend_timeout); return priv-&gt;suspend_ret;&#125;static int qksleep_resume(struct device *dev)&#123; qksleep_dev_t *qkdev = get_qkdev(); qksleep_dev_priv *priv = &amp;qkdev-&gt;priv_data; qksleep_debug("qksleep resume in"); if (priv-&gt;resume_errlock) qksleep_lockerr(priv); if (priv-&gt;resume_timeout) qksleep_vsleep(priv, priv-&gt;resume_timeout); return priv-&gt;resume_ret;&#125;static struct dev_pm_ops qk_sleep_pm = &#123; .suspend = qksleep_suspend, .resume = qksleep_resume,&#125;;static struct platform_driver qksleep_driver = &#123; .driver = &#123; .name = QK_SLEEP_DRV_NAME, .pm = &amp;qk_sleep_pm, .owner = THIS_MODULE, &#125;, .probe = qksleep_probe, .remove = qksleep_remove,&#125;; 在platform_driver的probe函数中，向系统注册了7个sysfs节点12345678910static struct attribute *qksleep_attr[] = &#123; &amp;dev_attr_suspend_wakeup_timeout.attr, &amp;dev_attr_suspend_ret.attr, &amp;dev_attr_suspend_timeout.attr, &amp;dev_attr_suspend_errlock.attr, &amp;dev_attr_resume_ret.attr, &amp;dev_attr_resume_timeout.attr, &amp;dev_attr_resume_errlock.attr, NULL,&#125;; suspend_wakeup_timeout：suspend后多久唤醒系统 suspend_ret：suspend函数返回值 suspend_timeout：suspend函数中模拟一个超时时间 suspend_errlock：suspend函数中模拟一个错误的锁操作 resume_xxx：同上 编译kernel，并用quem启动后，在/sys/devices/platform/qksleep路径下生成了该设备的所有sysfs节点12345678/ # cd /sys/devices/platform/qksleep//sys/devices/platform/qksleep # lsdriver resume_ret suspend_timeoutdriver_override resume_timeout suspend_wakeup_timeoutmodalias subsystem ueventpower suspend_errlockresume_errlock suspend_ret/sys/devices/platform/qksleep # 查看suspend/resume操作默认值1234567891011/sys/devices/platform/qksleep # cat suspend_*suspend errlock:0suspend ret:0suspend timeout:0(ms)suspend wakeup timeout:10000(ms)/sys/devices/platform/qksleep # /sys/devices/platform/qksleep # cat resume_*resume errlock:0resume ret:0resume timeout:0(ms)/sys/devices/platform/qksleep # 执行以下命令可控制qksleep设备suspend 2秒后唤醒1echo 20000 &gt; suspend_wakeup_timeout 执行以下命令可控制qksleep设备suspend函数返回错误值-11echo -1 &gt; suspend_ret 执行以下命令可控制qksleep设备在suspend函数中模拟一个死锁1echo 1 &gt; suspend_errlock 执行以下命令可控制qksleep设备在suspend函数中模拟一个2秒的超时1echo 2000 &gt; suspend_timeout resume同suspend 操作/sys/power/state节点，手动进入休眠状态，qksleep默认会在10秒后唤醒系统1234567891011121314151617181920212223/sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 615.345438] PM: Syncing filesystems ... done.[ 615.346378] PM: Preparing system for mem sleep[ 615.374765] Freezing user space processes ... (elapsed 0.011 seconds) done.[ 615.387040] Freezing remaining freezable tasks ... (elapsed 0.008 seconds) done.[ 615.396303] PM: Entering mem sleep[ 615.396496] Suspending console(s) (use no_console_suspend to debug)[ 615.401243] [debug] [qksleep_suspend:89] qksleep suspend in[ 615.401248] PM: suspend of devices complete after 3.752 msecs[ 615.401269] PM: suspend devices took 0.000 seconds[ 615.402693] PM: late suspend of devices complete after 1.392 msecs[ 615.404024] PM: noirq suspend of devices complete after 1.300 msecs[ 615.404050] PM: suspend-to-idle[ 625.426911] [debug] [qksleep_work_suspend_wakeup:51] qksleep is suspended, wakeup...[ 625.426961] PM: resume from suspend-to-idle[ 625.430512] PM: noirq resume of devices complete after 3.254 msecs[ 625.432153] PM: early resume of devices complete after 1.157 msecs[ 625.435256] [debug] [qksleep_resume:108] qksleep resume in[ 625.435260] PM: resume of devices complete after 3.047 msecs[ 625.436082] PM: resume devices took 0.010 seconds[ 625.439150] PM: Finishing wakeup.[ 625.439304] Restarting tasks ... done./sys/devices/platform/qksleep # 后续的调试验证将在qksleep设备节点的基础上来做 调试工具kernel的电源管理框架在“/sys/power”目录下创建了一系列供用户空间操作的sysfs节点，kernel文档“/Document/power/basic-pm-debugging.txt”中详细说明了如何将pm_test节点用于调试suspend/resume过程。“/Document/power/s2ram.txt”文档介绍了使用“s2ram”工具来调试和排查suspend/resume问题。stackexchange问题“How to debug a suspend problem?”的回答中介绍了“pm_utils”工具。英特尔开源社区的文章“BEST PRACTICE TO DEBUG LINUX* SUSPEND/HIBERNATE ISSUES”中系统的介绍了系统级的调试方法和遇到问题的排查步骤 总的来说，Linux电源管理的调试工具主要分为3大类：系统级调试工具(system debug tools)、PM专用调试方法(pm debug tools)和应用层开发的工具(application tools) system debug tools主要是一些kernel启动参数的控制，用于增加更多打印信息 initcall_debug no_console_suspend ignore_loglevel pm debug toolsPM创建的sysfs节点 pm_test pm_trace pm_async applaction tools结合PM sysfs编写的PM调试app pm_utils s2ram analyze_suspend.py initcall_debug通过将initcall_debug作为启动参数传入kernel，可以跟踪kernel的initcalls和驱动在boot、suspend和resume时的调用情况。通过这种方式可以追踪由于特定组件或驱动引起的suspend/resume问题 验证在qksleep的resume过程设置2秒的超时1/sys/devices/platform/qksleep # echo 2000 &gt; resume_timeout 手动进入休眠，系统唤醒后可看到suspend过程耗时0.010秒，resume操作耗时2.020秒1234567891011121314151617181920212223/sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 46.630363] PM: Syncing filesystems ... done.[ 46.632115] PM: Preparing system for mem sleep[ 46.695899] Freezing user space processes ... (elapsed 0.035 seconds) done.[ 46.732478] Freezing remaining freezable tasks ... (elapsed 0.017 seconds) done.[ 46.751186] PM: Entering mem sleep[ 46.751560] Suspending console(s) (use no_console_suspend to debug)[ 46.764225] [debug] [qksleep_suspend:89] qksleep suspend in[ 46.764237] PM: suspend of devices complete after 10.832 msecs[ 46.764522] PM: suspend devices took 0.010 seconds[ 46.766748] PM: late suspend of devices complete after 1.970 msecs[ 46.769322] PM: noirq suspend of devices complete after 2.371 msecs[ 46.769689] PM: suspend-to-idle[ 56.771522] [debug] [qksleep_work_suspend_wakeup:51] qksleep is suspended, wakeup...[ 56.771526] PM: resume from suspend-to-idle[ 56.773263] PM: noirq resume of devices complete after 1.499 msecs[ 56.775273] PM: early resume of devices complete after 1.512 msecs[ 58.788915] [debug] [qksleep_resume:108] qksleep resume in[ 58.788927] PM: resume of devices complete after 2013.258 msecs[ 58.792183] PM: resume devices took 2.020 seconds[ 58.798512] PM: Finishing wakeup.[ 58.799016] Restarting tasks ... done./sys/devices/platform/qksleep # 虽然能看出来系统resume过程耗时明显过长，但是无法知道是在哪里耗时过长，尝试用initcall_debug来查看。系统启动时传入参数initcall_debug，开启该参数后，启动虚拟机，会打印所有initcall信息123456789101112131415161718192021222324252627282930313233...Console: colour dummy device 80x30[ 0.017370] kmemleak: Kernel memory leak detector disabled[ 0.024047] Calibrating delay loop... 915.86 BogoMIPS (lpj=4579328)[ 0.097768] pid_max: default: 32768 minimum: 301[ 0.101635] Mount-cache hash table entries: 2048 (order: 1, 8192 bytes)[ 0.101702] Mountpoint-cache hash table entries: 2048 (order: 1, 8192 bytes)[ 0.133941] CPU: Testing write buffer coherency: ok[ 0.152375] CPU0: thread -1, cpu 0, socket 0, mpidr 80000000[ 0.153210] calling trace_init_flags_sys_enter+0x0/0x28 @ 1[ 0.153285] initcall trace_init_flags_sys_enter+0x0/0x28 returned 0 after 0 usecs[ 0.153483] calling trace_init_flags_sys_exit+0x0/0x28 @ 1[ 0.153525] initcall trace_init_flags_sys_exit+0x0/0x28 returned 0 after 0 usecs[ 0.153643] calling cpu_suspend_alloc_sp+0x0/0x1ec @ 1[ 0.153865] initcall cpu_suspend_alloc_sp+0x0/0x1ec returned 0 after 0 usecs[ 0.153980] calling init_static_idmap+0x0/0x74 @ 1[ 0.154138] Setting up static identity map for 0x60b26600 - 0x60b26658[ 0.154337] initcall init_static_idmap+0x0/0x74 returned 0 after 0 usecs[ 0.154453] calling dcscb_init+0x0/0x1a8 @ 1[ 0.154738] initcall dcscb_init+0x0/0x1a8 returned -19 after 0 usecs[ 0.154848] calling tc2_pm_init+0x0/0x250 @ 1[ 0.155097] initcall tc2_pm_init+0x0/0x250 returned -19 after 0 usecs[ 0.155207] calling spawn_ksoftirqd+0x0/0x60 @ 1[ 0.159450] initcall spawn_ksoftirqd+0x0/0x60 returned 0 after 9765 usecs[ 0.159624] calling init_workqueues+0x0/0x800 @ 1[ 0.170386] initcall init_workqueues+0x0/0x800 returned 0 after 9765 usecs[ 0.170520] calling migration_init+0x0/0xa4 @ 1[ 0.170714] initcall migration_init+0x0/0xa4 returned 0 after 0 usecs[ 0.170830] calling check_cpu_stall_init+0x0/0x24 @ 1[ 0.170876] initcall check_cpu_stall_init+0x0/0x24 returned 0 after 0 usecs[ 0.170985] calling rcu_spawn_gp_kthread+0x0/0x1f0 @ 1[ 0.172303] initcall rcu_spawn_gp_kthread+0x0/0x1f0 returned 0 after 0 usecs... 设置resume超时2秒，手动进入休眠1234567891011121314151617181920212223242526272829303132333435363738394041/sys/devices/platform/qksleep # echo 2000 &gt; resume_timeout /sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 30.075565] PM: Syncing filesystems ... done.[ 30.077755] PM: Preparing system for mem sleep[ 30.128189] Freezing user space processes ... (elapsed 0.019 seconds) done.[ 30.149985] Freezing remaining freezable tasks ... (elapsed 0.007 seconds) done.[ 30.158478] PM: Entering mem sleep[ 30.158851] Suspending console(s) (use no_console_suspend to debug)[ 30.161310] calling input2+ @ 768, parent: serio1[ 30.163481] call input2+ returned 0 after 2011 usecs[ 30.163599] calling oprofile-perf.0+ @ 768, parent: platform[ 30.163768] call oprofile-perf.0+ returned 0 after 144 usecs...[ 30.168789] calling qksleep+ @ 768, parent: platform[ 30.169022] [debug] [qksleep_suspend:89] qksleep suspend in[ 30.169027] call qksleep+ returned 0 after 202 usecs[ 30.169428] ...[ 30.175275] PM: suspend of devices complete after 14.735 msecs[ 30.175416] PM: suspend devices took 0.010 seconds[ 30.178217] PM: late suspend of devices complete after 2.592 msecs[ 30.180898] PM: noirq suspend of devices complete after 2.424 msecs[ 30.181213] PM: suspend-to-idle[ 40.260326] [debug] [qksleep_work_suspend_wakeup:51] qksleep is suspended, wakeup...[ 40.260340] PM: resume from suspend-to-idle[ 40.263683] PM: noirq resume of devices complete after 2.594 msecs[ 40.265601] PM: early resume of devices complete after 1.461 msecs[ 40.266288] calling reg-dummy+ @ 768, parent: platform[ 40.266395] call reg-dummy+ returned 0 after 84 usecs[ 40.266438] calling 10000000.sysreg+ @ 768, parent: platform[ 40.266455] call 10000000.sysreg+ returned 0 after 1 usecs[ 40.266487] calling syscon.0.auto+ @ 768, parent: 10000000.sysreg...[ 40.271348] calling qksleep+ @ 768, parent: platform[ 42.269188] [debug] [qksleep_resume:108] qksleep resume in[ 42.269209] call qksleep+ returned 0 after 1950845 usecs...[ 42.282800] PM: resume of devices complete after 2016.935 msecs[ 42.286160] PM: resume devices took 2.020 seconds[ 42.389269] PM: Finishing wakeup.[ 42.390769] Restarting tasks ... done./sys/devices/platform/qksleep # 明显可以看出来在qksleep设备的resume过程耗时1950845微秒1[ 42.269209] call qksleep+ returned 0 after 1950845 usecs no_console_suspend默认的kernel休眠过程中会禁用console，因此suspend的任何打印只有在系统唤醒以后才能看到，对于suspend这部分的执行过程，kernel是不会输出的。而开启no_console_suspend选项，可以让kernel进入suspend的过程，仍然输出打印 no_console_suspend的控制位于/kernel/power/suspend.c和/kernel/printk.c文件中，函数console_suspend_disable用于处理kernel参数no_console_suspend，当设置了该参数后，全局变量console_suspend_enabled的值为false123456static int __init console_suspend_disable(char *str)&#123; console_suspend_enabled = false; return 1;&#125;__setup("no_console_suspend", console_suspend_disable); kernel休眠时会调用suspend_devices_and_enter来执行设备的休眠流程，在其中会调用suspend_console来执行console的suspend1234567891011121314/** * suspend_console - suspend the console subsystem * * This disables printk() while we go into suspend states */void suspend_console(void)&#123; if (!console_suspend_enabled) return; printk("Suspending console(s) (use no_console_suspend to debug)\n"); console_lock(); console_suspended = 1; up_console_sem();&#125; 如果console_suspend_enabled为真，则会执行下面的锁定console操作，此后所有printk的打印信息不会立刻打印到终端；如果为否，直接退出，此后printk函数的打印信息不受影响 no_console_suspend参数比较适用于要跟踪调试kernel代码的情况，且适用于suspend后唤醒不正常的情况 验证qksleep的resume设置模拟死锁，并分别在带no_console_suspend参数和不带参数情况下调试 qksleep的resume设置死锁1/sys/devices/platform/qksleep # echo 1 &gt; resume_errlock 不带no_console_suspend情况下，手动进入休眠，打印在“Suspending console(s) (use no_console_suspend to debug)”处停止1234567/sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 160.900817] PM: Syncing filesystems ... done.[ 160.902427] PM: Preparing system for mem sleep[ 160.962134] Freezing user space processes ... (elapsed 0.036 seconds) done.[ 161.000256] Freezing remaining freezable tasks ... (elapsed 0.018 seconds) done.[ 161.020661] PM: Entering mem sleep[ 161.021063] Suspending console(s) (use no_console_suspend to debug) 带no_console_suspend的情况下，手动进入休眠，死锁前的打印都能看到1234567891011121314151617/sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 41.220243] PM: Syncing filesystems ... done.[ 41.222718] PM: Preparing system for mem sleep[ 41.274511] Freezing user space processes ... (elapsed 0.026 seconds) done.[ 41.301588] Freezing remaining freezable tasks ... (elapsed 0.011 seconds) done.[ 41.313599] PM: Entering mem sleep[ 41.322571] [debug] [qksleep_suspend:89] qksleep suspend in[ 41.326340] PM: suspend of devices complete after 11.215 msecs[ 41.326920] PM: suspend devices took 0.010 seconds[ 41.330000] PM: late suspend of devices complete after 2.664 msecs[ 41.332451] PM: noirq suspend of devices complete after 2.001 msecs[ 41.333041] PM: suspend-to-idle[ 51.341481] [debug] [qksleep_work_suspend_wakeup:51] qksleep is suspended, wakeup...[ 51.344717] PM: resume from suspend-to-idle[ 51.347243] PM: noirq resume of devices complete after 1.331 msecs[ 51.349427] PM: early resume of devices complete after 1.406 msecs[ 51.352296] [debug] [qksleep_resume:108] qksleep resume in ignore_loglevel开启ignore_loglevel参数后，kernel的打印会无视log级别限制，所有打印都能看到，适用于代码中有很多log级别区分的调试 pm_test该功能依赖kernel配置宏CONFIG_PM_DEBUG，在make menuconfig中配置路径为123make menuconfig --&gt; Power management options [*] Power Management Debug Support 开启此配置后，可通过写入/sys/power/pm_test节点让PM core以测试模式运行，测试模式有5个级别，对应于不同深度的休眠 freezer：测试进程冻结 devices：测试进程冻结和设备suspend platform：测试进程冻结、设备suspend、平台架构相关suspend processors：测试进程冻结、设备suspend、平台架构相关suspend、禁用非引导CPU core：测试进程冻结、设备suspend、平台架构相关suspend、禁用非引导CPU、系统suspend 这5个级别由浅入深，正好对应kernel的休眠流程，kernel在不同地方都设置了测试点suspend_test，当设置了测试模式后，对应级别的测试点会delay 5秒钟然后唤醒系统1234567891011static int suspend_test(int level)&#123;#ifdef CONFIG_PM_DEBUG if (pm_test_level == level) &#123; printk(KERN_INFO "suspend debug: Waiting for 5 seconds.\n"); mdelay(5000); return 1; &#125;#endif /* !CONFIG_PM_DEBUG */ return 0;&#125; 使用这种方法能够在不添加额外唤醒源的情况下测试suspend/resume的完整流程，且能够分阶段进行调试 验证禁用qksleep原本的suspend唤醒1/sys/devices/platform/qksleep # echo 0 &gt; suspend_wakeup_timeout 设置PM core调试模式为devices1/sys/devices/platform/qksleep # echo devices &gt; /sys/power/pm_test 设置qksleep设备resume返回错误值-11/sys/devices/platform/qksleep # echo -1 &gt; resume_ret 手动进入休眠，可看到kernel在5秒后自动唤醒了12345678910111213141516171819/sys/devices/platform/qksleep # echo mem &gt; /sys/power/state[ 229.676306] PM: Syncing filesystems ... done.[ 229.676943] PM: Preparing system for mem sleep[ 229.701077] Freezing user space processes ... (elapsed 0.013 seconds) done.[ 229.716141] Freezing remaining freezable tasks ... (elapsed 0.008 seconds) done.[ 229.725218] PM: Entering mem sleep[ 229.725415] Suspending console(s) (use no_console_suspend to debug)[ 229.730420] [debug] [qksleep_suspend:89] qksleep suspend in[ 229.730427] PM: suspend of devices complete after 4.232 msecs[ 229.730448] PM: suspend devices took 0.000 seconds[ 229.730459] suspend debug: Waiting for 5 seconds.[ 234.941168] [debug] [qksleep_resume:108] qksleep resume in[ 234.941173] dpm_run_callback(): platform_pm_resume+0x0/0x90 returns -1[ 234.941196] PM: Device qksleep failed to resume: error -1[ 234.942311] PM: resume of devices complete after 2.948 msecs[ 234.943057] PM: resume devices took 0.000 seconds[ 234.945443] PM: Finishing wakeup.[ 234.945607] Restarting tasks ... done./sys/devices/platform/qksleep # /sys/kernel/debug/wakeup_source该节点列举了当前系统中所有唤醒源以及他们的状况 1234567/sys/kernel/debug # cat wakeup_sources name active_count event_count wakeup_count expire_count active_since total_time max_time last_change prevent_suspend_time10017000.rtc 0 0 0 0 0 0 0 6937 0qksleep 1 1 0 1 0 196 196 1231539 0alarmtimer 0 0 0 0 0 0 0 4021 0autosleep 0 0 0 0 0 0 0 280 0/sys/kernel/debug # name：唤醒源的驱动名称 active_count：wake lock 活跃次数 event_count：唤醒源唤醒事件次数 wakeup_count：唤醒源强制设备唤醒的次数 expire_count：唤醒源已到期次数 active_since：唤醒源处于活跃状态的时间(以jiffies时间为单位) total_time：唤醒源活跃的总时间(以jiffies时间为单位) max_time：唤醒源持续活跃的最长时间 last_change：上次更改唤醒源为活跃的时间戳 prevent_suspend_time：如果没有这个唤醒源，系统进入suspend可以节省多少时间。这对于计算对电池寿命的影响特别有用 验证首次启动系统，查看该节点，所有唤醒源均未唤醒过系统 1234567/ # cat /sys/kernel/debug/wakeup_sources name active_count event_count wakeup_count expire_count active_since total_time max_time last_change prevent_suspend_time10017000.rtc 0 0 0 0 0 0 0 7296 0qksleep 0 0 0 0 0 0 0 6725 0alarmtimer 0 0 0 0 0 0 0 4358 0autosleep 0 0 0 0 0 0 0 300 0/ # 手动进入休眠，待系统唤醒后，查看唤醒源 1234567891011121314151617181920212223242526272829/ # echo mem &gt; /sys/power/state[ 292.573467] PM: Syncing filesystems ... done.[ 292.578116] PM: Preparing system for mem sleep[ 292.644122] Freezing user space processes ... (elapsed 0.036 seconds) done.[ 292.681593] Freezing remaining freezable tasks ... (elapsed 0.020 seconds) done.[ 292.704121] PM: Entering mem sleep[ 292.722582] [debug] [qksleep_suspend:89] qksleep suspend in[ 292.726072] PM: suspend of devices complete after 13.506 msecs[ 292.726509] PM: suspend devices took 0.020 seconds[ 292.728801] PM: late suspend of devices complete after 1.917 msecs[ 292.732123] PM: noirq suspend of devices complete after 2.847 msecs[ 292.733016] PM: suspend-to-idle[ 302.741766] [debug] [qksleep_work_suspend_wakeup:51] qksleep is suspended, wakeup...[ 302.743342] PM: resume from suspend-to-idle[ 302.746235] PM: noirq resume of devices complete after 1.903 msecs[ 302.748644] PM: early resume of devices complete after 1.406 msecs[ 302.751825] [debug] [qksleep_resume:108] qksleep resume in[ 302.754932] PM: resume of devices complete after 5.491 msecs[ 302.756098] PM: resume devices took 0.010 seconds[ 302.756843] PM: Finishing wakeup.[ 302.757105] Restarting tasks ... done./ # / # cat /sys/kernel/debug/wakeup_sources name active_count event_count wakeup_count expire_count active_since total_time max_time last_change prevent_suspend_time10017000.rtc 0 0 0 0 0 0 0 7296 0qksleep 1 1 0 1 0 198 198 302929 0alarmtimer 0 0 0 0 0 0 0 4358 0autosleep 0 0 0 0 0 0 0 300 0/ # 可看到是qksleep驱动唤醒了系统]]></content>
      <categories>
        <category>Linux Kernel</category>
        <category>电源管理</category>
      </categories>
      <tags>
        <tag>Kernel</tag>
        <tag>PM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C Program Skill]]></title>
    <url>%2F2019%2F11%2F27%2FC-Program-Skill%2F</url>
    <content type="text"><![CDATA[本文记录一些实际工作中使用到的C语言编程技巧，或者学到的一些好用的用法 kernel与application通信proc文件系统&amp;mmapkernel和application通信方式有很多，但是当数据量较大时，常用的ioctl、netlink方式并不适合，mmap较为适用。 原理对于应用层程序而言，系统调用mmap()可以将一个文件映射到内存空间，对该文件的读写就是对该块内存的读写。对于内核空间而言，proc文件系统的文件操作集file_operations支持mmap方法，在文件proc方法的具体实现中，可以将内存映射到应用层调用mmap()的虚拟地址上，从而实现应用层和内核空间通过proc文件关联同一块内存 代码内核空间 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#define MMAP_PROCFILE "mmap_test"#define LINUX_PAGE_SIZE 4096#define MMAP_MEM_SIZE (LINUX_PAGE_SIZE * 8)static char *mmap_mem = NULL;static int proc_mmap(struct file *filp, struct vm_area_struct *vma) &#123; int ret; struct page *page = NULL; unsigned long size = (unsigned long)(vma-&gt;vm_end - vma-&gt;vm_start); if (size &gt; MMAP_MEM_SIZE) &#123; ret = -EINVAL; goto err; &#125; /* map mem block to process's address space */ page = virt_to_page((unsigned long)mmap_mem + (vma-&gt;vm_pgoff &lt;&lt; PAGE_SHIFT)); ret = remap_pfn_range(vma, vma-&gt;vm_start, page_to_pfn(page), size, vma-&gt;vm_page_prot); if (ret) goto err; /* your operation */ return 0;err: return ret;&#125;static struct file_operations proc_fops = &#123; .owner = THIS_MODULE, .mmap = proc_mmap, &#125;; static int proc_mmap_create()&#123; /* create mem block */ mmap_mem = kmalloc(MMAP_MEM_SIZE, GFP_KERNEL); if (!mmap_mem) &#123; printk("kmalloc error\n"); return -1; &#125; /* your operation */ /* create procfile */ struct proc_dir_entry *proc_file = proc_create(MMAP_PROCFILE, 0x0644, NULL, &amp;proc_fops);&#125; 用户空间 12345678910111213141516171819202122232425262728293031323334static int mmap_read_once()&#123; int fd; char *mmap_mem = NULL; char mmap_file[64] = &#123;'\0'&#125;; sprintf(mmap_file, "/proc/%s", MMAP_PROCFILE) fd = open(mmap_file, O_RDWR|O_NDELAY); if (fd &lt; 0) &#123; log_err("open %s error", mmap_file); return -1; &#125; /* do mapping */ mmap_mem = (char *)mmap(0, MMAP_MEM_SIZE, PROT_READ | PROT_WRITE, MAP_SHARED, fd, 0); if (!mmap_mem) &#123; log_err("mmap error"); goto out; &#125; /* your operation */out: if (mmap_mem) munmap(mmap_mem, MMAP_MEM_SIZE); if (fd &gt; 0) close(fd); return -1;&#125; 需注意数据同步问题]]></content>
      <categories>
        <category>Program Language</category>
        <category>C</category>
      </categories>
      <tags>
        <tag>Program Skill</tag>
        <tag>C</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201911]]></title>
    <url>%2F2019%2F11%2F15%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201911%2F</url>
    <content type="text"><![CDATA[技术Home AssistantHome Assistant是一款基于Python的智能家居开源系统，支持众多品牌的智能家居设备，可以轻松实现设备的语音控制、自动化等，支持米家生态链、苹果HomeKit，还支持树莓派 Home Assistant中文文档 Home Assistant github Home Assistant中文网 TED-量子锁定由于若干技术的突破，研究发现在某种环境下，某些超导体会发生部分磁通力被束缚在超导体内部，导致周围空间和超导体内部的磁场构成一个锁定的磁力分布，使得超导体在三维空间上姿态锁定，且这种状态下的物体可以承重自身70000倍重力的物体，该研究可应用于发展新型零摩擦的交通工具，这种零摩擦并非是磁悬浮的互斥力，而是磁力锁定 新闻知乎-华为251最近接连发生了两起大公司内中龄员工被辞退纠纷事件，一个是网易辞退身患绝症的老员工并叫保安驱赶，一个是华为员工因向上举报部门业务造假被诬告敲诈勒索拘留251天]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[函数和栈]]></title>
    <url>%2F2019%2F10%2F15%2F%E5%87%BD%E6%95%B0%E5%92%8C%E6%A0%88%2F</url>
    <content type="text"><![CDATA[函数是许多编程语言的代码基本单元，多层函数的嵌套调用在系统层面是依赖栈机制来实现的，此处的栈指的不是软件编程数据结构中的堆和栈，而是由不同体系架构的处理器提供的程序指针寄存器、栈基址寄存器、堆栈寄存器等结构以及操作系统辅助构成的函数运行环境。理解函数调用栈对理解程序在内存中的运行以及理解操作系统进程管理都是很有帮助的。本文尝试回答以下问题 什么是函数栈 函数栈为什么能保存上级函数的信息 Linux中如何管理函数栈 函数栈有多大，会溢出吗，溢出了怎么办。平时调试代码时程序崩溃后打印的堆栈信息是什么意思 基础知识在理解函数调用栈之前，需要提前了解一些背景知识，包括计算机中一条指令是如何运行的、CPU对栈的支持、Linux进程虚拟内存空间、栈帧结构 X86 CPU寄存器函数调用栈从根本上来说，是由CPU堆栈相关寄存器实现的。不同体系结构对于程序指令运行的处理有很大不同，本文以X86架构CPU为例，分析其函数调用过程。与堆栈有关的寄存器主要是如下几个 寄存器 作用 EIP(instruction pointer) 指令寄存器，64位为RIP ESP(stack pointer) 堆栈寄存器，64位为RSP EBP(base pointer) 基址寄存器，64位为RBP EIP是指令寄存器，它存放的是下一条指令的地址。如果没有函数堆栈结构，计算机的指令执行过程完全依赖于EIP寄存器，CPU每次从EIP寄存器获取一条指令，EIP会自动累加，从而实现了CPU的顺序执行，如图 ESP是堆栈寄存器，它永远指向系统最上面一个栈帧的栈顶，EBP是基址寄存器，永远指向系统最上面一个栈帧的栈底。那什么是栈帧呢？它是什么结构的呢？在内存中位于什么位置呢？后文详细介绍 进程虚拟内存空间和栈讨论函数的执行，我们需要了解函数的执行环境是怎样的。这里我们讨论范围仅限于Linux系统下的用户空间进程中的函数执行。在Linux系统中，将整个内存空间按照3:1的比例划分为了用户空间和内核空间两大区域，默认系统配置情况下，0~3G为用户空间，3~4G为内核空间。我们创建的程序代码，都是以进程的方式运行于用户空间的虚拟内存中，当某个进程被调度时，其整个内存分布图如图所示 用户空间从0x00000000到0xc0000000，其中包括很多内容，包括栈(stack)、堆(heap)、静态变量(bss)、数据段(edata)、代码段(etext)等。当一个程序被加载到内存空间执行时，这些内容除了堆是由用户来分配，其他都是由操作系统来分配和管理。本文所关心的栈结构，其大小是固定的，Linux中一般是8M(可配置)，通过命令ulimit -s可查看当前栈空间大小 12[root@localhost home]# ulimit -s8192 当程序运行函数嵌套过多时，当然会超过这个限定大小，这会触发缺页异常，操作系统会计算并增长栈空间 栈和栈帧(Stack Frame)栈是从上往下生长，先占用高地址，再占用低地址，主要有3个作用 为函数内部声明的非静态局部变量(C语言中称自动变量)提供存储空间 记录函数调用过程相关的维护性信息，称为栈帧(Stack Frame)或过程活动记录(Procedure Activation Record)。它包括函数返回地址，不适合装入寄存器的函数参数及一些寄存器值的保存。除递归调用外，堆栈并非必需。因为编译时可获知局部变量，参数和返回地址所需空间，并将其分配于BSS段 临时存储区，用于暂存长算术表达式部分计算结果或alloc()函数分配的栈内内存 程序的运行是依靠EIP指针的累加从而一条指令一条指令执行的，但是当函数调用时，在跳转到子函数时和子函数返回时，显然地址不是连续的，如何做到外层函数的保存和返回呢？在同一个时刻，堆栈中会有多个函数的信息，每个待返回的函数都会占用一块独立的连续区域，这个区域就是栈帧。一个栈帧中主要存放以下关键信息 上一栈帧的栈顶地址 局部变量 参数列表 整个栈结构如图所示 同一时间会存在多个栈帧，最下面的栈帧代表着当前函数，越上层的栈帧代表越外层的函数 函数调用过程函数调用过程主要有3个阶段 调用目标函数之前的准备工作 调用目标函数 目标函数返回到调用目标函数 为了方便说明，指定调用目标函数的函数为caller，目标函数为callee。在调用callee之前，caller的栈帧已经存在于内存中，如图所示 此时EBP指向caller栈帧的栈底，ESP指向caller栈帧的栈顶 准备工作在调用callee之前，caller会将EIP的值压栈保存，然后修改EIP寄存器的值指向被调用者callee，如图 调用调用callee需要完成两个操作 12pushl %ebp movel %esp, %ebp 第一条指令是将EBP的值压栈 第二条指令是移动EBP到栈顶 返回callee返回前，其栈帧结构如图 栈帧中已经存在一些局部变量和参数 callee返回要完成两个操作 12movel %ebp, %esppopl %ebp 第一条指令将callee的栈帧清空 第二条指令将EBP恢复到之前压栈的地址，也就是caller的栈帧基地址 由于调用callee前将caller下一条指令地址EIP已经压栈保存，此时只需要将EIP再设置为该保存值就可以恢复caller的执行 实例以下使用C代码实例，运行于centos7 64bit linux-3.10 环境，通过反汇编代码和gdb调试来分析函数调用栈的过程。C代码test.c如下 12345678910111213141516int sum(a, b)&#123; return (a+b);&#125;int main(int argc, char *argv[])&#123; int a, b, c; a = 2; b = 3; c = sum(a, b); return 0;&#125; 将test.c编译生成可执行文件，再用objdump命令反汇编 12gcc -c test.c -o testobjdump -s -d test &gt; test.stxt 汇编代码内容如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950test: file format elf64-x86-64Contents of section .text: 0000 554889e5 897dfc89 75f88b45 f88b55fc UH...&#125;..u..E..U. 0010 01d05dc3 554889e5 4883ec20 897dec48 ..].UH..H.. .&#125;.H 0020 8975e0c7 45fc0200 0000c745 f8030000 .u..E......E.... 0030 008b55f8 8b45fc89 d689c7e8 00000000 ..U..E.......... 0040 8945f4b8 00000000 c9c3 .E........ Contents of section .comment: 0000 00474343 3a202847 4e552920 342e382e .GCC: (GNU) 4.8. 0010 35203230 31353036 32332028 52656420 5 20150623 (Red 0020 48617420 342e382e 352d3131 2900 Hat 4.8.5-11). Contents of section .eh_frame: 0000 14000000 00000000 017a5200 01781001 .........zR..x.. 0010 1b0c0708 90010000 1c000000 1c000000 ................ 0020 00000000 14000000 00410e10 8602430d .........A....C. 0030 064f0c07 08000000 1c000000 3c000000 .O..........&lt;... 0040 00000000 36000000 00410e10 8602430d ....6....A....C. 0050 06710c07 08000000 .q...... Disassembly of section .text:0000000000000000 &lt;sum&gt;: 0: 55 push %rbp 1: 48 89 e5 mov %rsp,%rbp 4: 89 7d fc mov %edi,-0x4(%rbp) 7: 89 75 f8 mov %esi,-0x8(%rbp) a: 8b 45 f8 mov -0x8(%rbp),%eax d: 8b 55 fc mov -0x4(%rbp),%edx 10: 01 d0 add %edx,%eax 12: 5d pop %rbp 13: c3 retq 0000000000000014 &lt;main&gt;: 14: 55 push %rbp 15: 48 89 e5 mov %rsp,%rbp 18: 48 83 ec 20 sub $0x20,%rsp 1c: 89 7d ec mov %edi,-0x14(%rbp) 1f: 48 89 75 e0 mov %rsi,-0x20(%rbp) 23: c7 45 fc 02 00 00 00 movl $0x2,-0x4(%rbp) 2a: c7 45 f8 03 00 00 00 movl $0x3,-0x8(%rbp) 31: 8b 55 f8 mov -0x8(%rbp),%edx 34: 8b 45 fc mov -0x4(%rbp),%eax 37: 89 d6 mov %edx,%esi 39: 89 c7 mov %eax,%edi 3b: e8 00 00 00 00 callq 40 &lt;main+0x2c&gt; 40: 89 45 f4 mov %eax,-0xc(%rbp) 43: b8 00 00 00 00 mov $0x0,%eax 48: c9 leaveq 49: c3 retq 第14、15条指令，main函数首先是准备自己的栈空间。call命令本身会保存RIP寄存器的值。在sum函数开头，会将RBP压栈，保存main函数的栈帧基址，然后将RBP移动到栈顶。在sum函数返回时，会将RBP出栈 gdb调试使用gdb调试C代码， 在编译时需要加上-g参数 1gcc -o test -g test.c 执行以下命令以gdb调试可执行程序 1gdb test gdb调试打印如下 1234567891011GNU gdb (GDB) Red Hat Enterprise Linux 7.6.1-94.el7Copyright (C) 2013 Free Software Foundation, Inc.License GPLv3+: GNU GPL version 3 or later &lt;http://gnu.org/licenses/gpl.html&gt;This is free software: you are free to change and redistribute it.There is NO WARRANTY, to the extent permitted by law. Type "show copying"and "show warranty" for details.This GDB was configured as "x86_64-redhat-linux-gnu".For bug reporting instructions, please see:&lt;http://www.gnu.org/software/gdb/bugs/&gt;...Reading symbols from /home/share/test/c/elf_test/test...done.(gdb) gdb下调试函数栈帧，可用以下命令 backktrace：显示程序的调用栈信息，缩写’bt’ frame: 查看栈帧信息 info frame：查看栈帧详细信息 首先在main函数处设置断点，并查看函数栈信息 12345678910(gdb) b mainBreakpoint 1 at 0x400510: file test.c, line 13.(gdb) runStarting program: /home/share/test/c/elf_test/test Breakpoint 1, main (argc=1, argv=0x7fffffffe308) at test.c:1313 a = 2;(gdb) bt#0 main (argc=1, argv=0x7fffffffe308) at test.c:13(gdb) 可看到当前程序存在一个栈帧，栈帧号为0，是main函数 查看详细栈帧信息 123456789(gdb) info frameStack level 0, frame at 0x7fffffffe230: rip = 0x400510 in main (test.c:13); saved rip 0x7ffff7a3db35 source language c. Arglist at 0x7fffffffe220, args: argc=1, argv=0x7fffffffe308 Locals at 0x7fffffffe220, Previous frame's sp is 0x7fffffffe230 Saved registers: rbp at 0x7fffffffe220, rip at 0x7fffffffe228(gdb) 使用info reg查看寄存器信息 12345678910111213141516171819202122232425(gdb) info regrax 0x400501 4195585rbx 0x0 0rcx 0x400540 4195648rdx 0x7fffffffe318 140737488347928rsi 0x7fffffffe308 140737488347912rdi 0x1 1rbp 0x7fffffffe220 0x7fffffffe220rsp 0x7fffffffe200 0x7fffffffe200r8 0x7ffff7dd7e80 140737351876224r9 0x0 0r10 0x7fffffffe070 140737488347248r11 0x7ffff7a3da40 140737348098624r12 0x400400 4195328r13 0x7fffffffe300 140737488347904r14 0x0 0r15 0x0 0rip 0x400510 0x400510 &lt;main+15&gt;eflags 0x206 [ PF IF ]cs 0x33 51ss 0x2b 43ds 0x0 0es 0x0 0fs 0x0 0gs 0x0 0 栈基址rbp为0x7fffffffe220，栈顶地址rsp为0x7fffffffe200，如图 s单步调试，直到程序进入sum函数，查看栈结构 12345678910111213(gdb) s14 b = 3;(gdb) s17 c = sum(a, b);(gdb) bt#0 main (argc=1, argv=0x7fffffffe308) at test.c:17(gdb) ssum (a=2, b=3) at test.c:66 return (a+b);(gdb) bt#0 sum (a=2, b=3) at test.c:6#1 0x000000000040052d in main (argc=1, argv=0x7fffffffe308) at test.c:17(gdb) 可看到此时有两个栈帧0和1，main函数栈帧号变成了1，sum函数栈帧号是0。用info frame命令分别查看两个栈帧信息 12345678910111213141516171819(gdb) info frame 0Stack frame at 0x7fffffffe200: rip = 0x4004f7 in sum (test.c:6); saved rip 0x40052d called by frame at 0x7fffffffe230 source language c. Arglist at 0x7fffffffe1f0, args: a=2, b=3 Locals at 0x7fffffffe1f0, Previous frame's sp is 0x7fffffffe200 Saved registers: rbp at 0x7fffffffe1f0, rip at 0x7fffffffe1f8(gdb) info frame 1Stack frame at 0x7fffffffe230: rip = 0x40052d in main (test.c:17); saved rip 0x7ffff7a3db35 caller of frame at 0x7fffffffe200 source language c. Arglist at 0x7fffffffe220, args: argc=1, argv=0x7fffffffe308 Locals at 0x7fffffffe220, Previous frame's sp is 0x7fffffffe230 Saved registers: rbp at 0x7fffffffe220, rip at 0x7fffffffe228(gdb) 查看寄存器值 12345678910111213141516171819202122232425(gdb) info regrax 0x2 2rbx 0x0 0rcx 0x400540 4195648rdx 0x3 3rsi 0x3 3rdi 0x2 2rbp 0x7fffffffe1f0 0x7fffffffe1f0rsp 0x7fffffffe1f0 0x7fffffffe1f0r8 0x7ffff7dd7e80 140737351876224r9 0x0 0r10 0x7fffffffe070 140737488347248r11 0x7ffff7a3da40 140737348098624r12 0x400400 4195328r13 0x7fffffffe300 140737488347904r14 0x0 0r15 0x0 0rip 0x4004f7 0x4004f7 &lt;sum+10&gt;eflags 0x206 [ PF IF ]cs 0x33 51ss 0x2b 43ds 0x0 0es 0x0 0fs 0x0 0gs 0x0 0 可看到函数sum的栈顶栈底都是0x7fffffffe1f0，因为该函数函数体只有一个返回语句，此时函数栈结构为 参考 CSDN-Xzzzh-Linux虚拟地址空间布局以及进程栈和线程栈总结 CSDN-gdb查看函数调用栈]]></content>
      <categories>
        <category>Linux Kernel</category>
        <category>进程管理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201910]]></title>
    <url>%2F2019%2F10%2F08%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201910%2F</url>
    <content type="text"><![CDATA[资源 没有math.h我们能干啥？许多场景下的开发环境中并不支持数学运算math库，可以自己实现一些函数来使用，例如快速幂实现pow、自适应辛普森公式实现lnx、牛顿迭代实现sqrt、泰勒级数实现exp、三角函数、反三角函数、双曲函数等 新闻 MIT可穿墙透视评估人体姿态MIT项目RF-Pose使用了一个神经网络来分析人们的身体反射的无线电信号，然后可以创建一个随人类动作而同步的动态火柴人行走、停下、坐下和移动肢体的图像 该研究论文为Through-Wall Human Pose Estimation Using Radio Signals，此研究发布了一个视频： @media all and (orientation : landscape) { .video {width:800px; height:600px;} } @media all and (orientation : portrait){ .video {width:90%; height:250px;} }]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[梯度下降和反向传播]]></title>
    <url>%2F2019%2F09%2F30%2F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%2F</url>
    <content type="text"><![CDATA[梯度下降(GD, Gradient Descent)和反向传播(BP, Back Propagation)是理解神经网络学习原理的核心，本文试图从数学原理、实例来分析梯度下降和反向传播在神经网络中是如何工作的 数学原理为了理解梯度下降算法和反向传播算法的工作原理，需要理解以下数学知识 方向导数和梯度 链式法则 方向导数梯度在数学中的概念，源于方向导数。在一元函数中，我们用导数来描述和分析函数的变化趋势，二元函数或多元函数我们用偏导数来分析函数变化情况。但是一旦变成多元，偏导数只能反应在坐标轴方向的变化情况，如果想要知道函数在任意方向上的变化情况，偏导数是无法做到的。因此引入了方向导数的概念 $l$ 是 $xOy$ 平面上以 $P{0}(x{0},y{0})$ 为起点的一条射线，$e{l}=(\cos{\alpha},\cos{\beta})$ 是与 $l$ 同方向的单位向量，$P$ 为射线上另一点。如果函数 $z = f(x,y)$ 的增量 $f(x{0}+t\cos{\alpha}, y{0}+t\cos{\beta})$ 与 $P$ 到 $P{0}$ 的距离 $\left|PP{0}\right| = t$ 比值 \frac{f(x_{0}+t\cos{\alpha}, y_{0}+t\cos{\beta}) - f(x_{0}, y_{0})}{t}当 $P$ 趋于 $P{0}$ 时的极限存在，则称此极限为函数在点 $P{0}$ 沿着射线 $l$ 的方向导数。简单来说，方向导数就是函数在某一点上(当然这一点必须是在函数上的)，沿着某一个方向的变化率，但是方向导数并不是在任何方向上都存在，有定理： 如果函数在某一点可微，函数在该点的任何方向的方向导数存在 梯度函数在很多方向上都有方向导数，那么在哪个方向上变化率最大呢？ 如果函数在点 $P{0}$ 可微，$e{l} = (cos\alpha, cos\beta)$ 是与方向 $l$ 同方向的单位向量，则方向导数可以写成如下形式 \frac{\partial f}{\partial l} = \nabla f\left ( x_{0}, y_{0} \right ) \cdot e_{l} = \left | \nabla f\left ( x_{0}, y_{0} \right ) \right |cos\theta其中 $\nabla f\left ( x{0},y{0} \right )$ 就是梯度，$\theta$ 是梯度与单位向量的夹角。反过来，可以将方向导数看做梯度在任意方向上的投影，当这个夹角 $\theta$ 为0时，表示方向导数与梯度同方向，函数增长速度最快，当夹角 $\theta$ 为180度时，方向导数与梯度方向相反，函数减小最快 梯度的表达式为 \nabla{f} = [\frac{\partial f}{\partial x}, \frac{\partial f}{\partial y}]本质上是函数的各个偏导数合成的向量 链式法则微积分中的链式法则用于计算复合函数的导数。设$x$为实数，$f$和$g$是从实数映射到实数的函数，假设$y=g(x)$且$z=f(g(x))=f(y)$，那么链式法则为 \frac{\partial z}{\partial x} = \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}扩展到向量，假设$\boldsymbol{x} \in \mathbb{R}^{m}$，$\boldsymbol{y} \in \mathbb{R}^{n}$，$g$是从$\mathbb{R}^{m}$到$\mathbb{R}^{n}$的映射，$f$是从$\mathbb{R}^{n}$到$\mathbb{R}$的映射，如果$\boldsymbol{y}=g(\boldsymbol{x})$且$z=f(\boldsymbol{y})$。那么 \frac{\partial z}{\partial x_{i}} = \sum_{j}\frac{\partial z}{\partial y_{i}}\frac{\partial y_{i}}{\partial x_{i}}使用向量写法，可等价为 \nabla_{x}{z} = \left(\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}\right)^{T}\nabla_{y}{z}其中$\frac{\partial \boldsymbol{y}}{\partial \boldsymbol{x}}$是$g$的$n \times m$的Jacobian矩阵 梯度下降梯度下降属于神经网络中优化器的一种，另外还有AdaGrad、RMSProp、Adam等。由于梯度永远指向函数增长最快的方向，那么函数下降最快的方向就是负梯度$-\nabla{f}$。梯度下降算法的思想是构造一个迭代过程，每次都使得损失函数$L$在负梯度的方向步进一点，经过若干次迭代后，函数值最终会逼近极值，这时网络学习收敛 设$\Delta{f}$为每次迭代时函数的变化量，可设置为 \Delta{f} = -\eta{\nabla{f}}其中$\eta$是一个很小的正数，称为学习速率 在每次迭代时$\omega$按照如下规则更新 \omega -> \acute{\omega} = \omega - \eta{\nabla{f}}$b$的更新规则同上。整个迭代过程可以想象为在山谷中下落的小球，小球从山顶会沿着最短路径慢慢滚到谷底，如下图所示 反向传播反向传播这个术语经常被误认为只是用于神经网络的整个学习算法中，实际上反向传播算法只是用于计算梯度的方法，它可以用来计算任何函数的导数。由于按照梯度的定义直接求解是非常复杂的，反向传播能够使用非常简单的计算步骤来求解梯度 链式法则求复合函数的导数反向传播的核心在于运用链式法则，以下使用函数$f(x,y,z) = (x+y)*z$来说明整个过程。可将公式拆分为两部分$q=x+y$和$f=qz$ 1234567891011121314# input valuex = -2y = 5z = -4# forward propagationq = x + y # q become 3f = q*z # f become -12# back propagationdfdz = q # q=3, dfdz = 3dfdq = z # z=-4, dfdq = -4dfdx = 1.0 * dfdq # dqdx=1, dfdq=-4, dfdx=-4dfdy = 1.0 * dfdq # dqdy=1, dfdq=-4, dfdy=-4 对于输入x=-2, y=5, z=-4先进行前向传播，依次向后计算，最后得出函数值为-12，前向传播的方向为从输入到输出(绿色)。反向传播(红色)是从输出开始，根据链式法则递归的向前计算梯度，梯度在链路中回流。最终计算得到梯度[dfdx, dfdy, dfdz]为[-4,-4,3]，如图所示 反向传播的直观理解——门单元间梯度的传递任何可微的函数都可以看做若干个门单元的组合形式，例如加法门、乘法门、除法门、取最大值门等 \begin{align*} f(x) &= \frac{1}{x} \to \frac{df}{dx} = -\frac{1}{x^{2}} \\ f(x) &= a + x \to \frac{df}{dx} = 1 \\ f(x) &= e^{x} \to \frac{df}{dx} = e^{x} \\ f(x) &= ax \to \frac{df}{dx} = a \\ \end{align*}在整个计算图过程中，每个单元门都会得到一些输入，并计算两个东西： 这个门的输出 其输出值关于输入值的局部梯度$\nabla{a}$ 在反向传播过程中，门单元会获得整个网络输出值在自己的输出值上的梯度$\nabla{b}$，由链式法则可知，将$\nabla{b}$乘以$\nabla{a}$可得到整个网络的输出对于该门的每个输入的梯度 在梯度的回流中，不同的门单元有不同的作用。下图展示了一个反向传播的例子，加法操作将梯度相等地分发给它的输入。取最大操作将梯度路由给更大的输入。乘法门拿取输入激活数据，对它们进行交换，然后乘以梯度 神经网络中的反向传播在神经网络中，反向传播是对权重和偏置变化影响代价函数$C$过程的理解，最终目的是为了计算偏导数$\frac{\partial C}{\partial \boldsymbol{\omega}}$和$\frac{\partial C}{\partial \boldsymbol{b}}$。定义网络中第$l$层第$j$个神经元上的误差$\delta_{j}^{l}$为 \delta_{j}^{l} = \frac{\partial C}{\partial z_{j}^{l}}其中$z_{j}^{l}$是第$l$层第$j$个神经元的输出。向量化第$l$层的误差向量为$\boldsymbol{\delta}^{l}$。反向传播提供一种计算每层误差的方法，并将这些误差关联到$\frac{\partial C}{\partial \boldsymbol{\omega}}$和$\frac{\partial C}{\partial \boldsymbol{b}}$上 反向传播的四个基本方程定义$\boldsymbol{\delta}^{L}$表示输出层误差，对于每个元素，其误差为 \begin{align*} \delta_{j}^{L} &= \frac{\partial C}{\partial z_{j}^{L}} \\ &= \frac{\partial C}{\partial a_{j}^{L}}\sigma'(z_{j}^{L}) \end{align*}其向量形式为公式(BP1) \boldsymbol{\delta}^{L} = \nabla_{a}{C}\ \odot sigma'(\boldsymbol{z}^{L}) \tag{BP1}使用下一层误差$\boldsymbol{\delta}^{l+1}$表示当前层误差$\boldsymbol{\delta}^{l}$ \boldsymbol{\delta}^{l} = \left((\boldsymbol{\omega}^{l+1})^{T}\boldsymbol{\delta}^{l+1} \right) \odot \sigma'(\boldsymbol{z}^{l}) \tag{BP2}其中$\odot$表示Hadamard乘积。通过公式BP1和(BP2)可以计算任意层的误差$\boldsymbol{\delta}^{l}$，首先使用公式BP1计算输出层误差$\boldsymbol{\delta}^{L}$，然后应用BP2计算$\boldsymbol{\delta}^{L-1}$，然后依次计算完整个网络 代价函数关于网络中任意偏置的改变率为 \frac{\partial C}{\partial b_{j}^{l}} = \delta_{j}^{l}这表示误差就是偏置的偏导数，向量形式为 \frac{\partial C}{\partial \boldsymbol{b}^{l}} = \boldsymbol{\delta}^{l} \tag{BP3}代价函数关于权重的改变率为 \frac{\partial C}{\partial \omega_{jk}^{l}} = a_{k}^{l-1}\delta_{j}^{l}向量形式为 \frac{\partial C}{\partial \boldsymbol{\omega}^{l}} = \boldsymbol{a}^{l-1}\boldsymbol{\delta}^{l} \tag{BP4}显示的描述反向传播的步骤 输入$\boldsymbol{x}$：为输入层设置对应的激活值$\boldsymbol{a}^{1}$ 前向传播：对每个层，计算相应的$\boldsymbol{z}^{l}=\boldsymbol{\omega}\boldsymbol{a}^{l-1}$和$\boldsymbol{a^{l}}=\sigma(\boldsymbol{z}^{l})$ 输出层误差$\boldsymbol{\delta}^{L}$：通过BP1计算 反向传播误差：通过BP2计算 更新$\boldsymbol{\omega}和\boldsymbol{b}$：通过BP3和BP4计算 实例以下图网络结构举例来说明反向传播算法是如何工作的 初始化参数网络中由输入层、1层隐藏层和输出层共3层构成，按照以下参数初始化网络 1234567# 初始化输入输出i = [0.1, 0.2]o = [0.01, 0.99]# 初始化权重和偏置w = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]b = [0.55, 0.56, 0.66, 0.67] 正向传播12345678910111213141516171819# 计算隐藏层神经元 h1 的输入加权和h1_in = w[0]*i[0] + w[1]*i[1] + b[0] # h1_in = 0.1*0.1+0.2*0.2+0.55 = 0.6000000000000001# 计算隐藏层神经元 h1 的输出h1_out = sigmoid(h1_in) # h1_out = 0.6456563062257954# 计算隐藏层神经元 h2 的输入加权和h2_in = w[2]*i[0] + w[3]*i[1] + b[1] # h2_in = 0.3*0.1+0.4*0.2+0.56 = 0.67# 计算隐藏层神经元 h1 的输出h2_out = sigmoid(h2_in) # h2_out = 0.6615031592029524# 计算输出层神经元 o1 的输入加权和o1_in = w[4]*h1_out + w[5]*h2_out + b[2] # o1_in = 0.5*0.6456563062257954 + 0.6*0.6615031592029524 + 0.66 = 1.379730048634669# 计算输出层神经元 o1 的输出o1_out = sigmoid(o1_in) # o1_out = 0.7989476413779711# 计算输出层神经元 o2 的输入加权和o2_in = w[6]*h1_out + w[7]*h2_out + b[3] # o2_in = 0.7*0.6456563062257954 + 0.8*0.6615031592029524 + 0.67 = 1.6511619417204186# 计算输出层神经元 o2 的输出o2_out = sigmoid(o2_in) # o2_out = 0.8390480283342561 正向传播结束后的输出结果为[0.7989476413779711, 0.8390480283342561]，但是希望的输出是[0.01, 0.99]，因此利用反向传播更新权重和偏置，然后重新计算输出 反向传播代价函数使用 C = \frac{1}{2}\sum_{j}\left(y_{j} - a_{j}^{l} \right)其导数为 \frac{\partial C}{\partial a_{j}^{l}} = a_{j} - y_{j}根据BP1，输出层神经元的误差$\delta_{j}^{3}$计算方法为 \begin{align*} \delta_{j}^{3} &= \nabla_{a}{C}*\sigma'(z_{j}^{3}) \\ &= (a_{j}^{3} - y_{j})*(a_{j}^{3}*(1-a_{j}^{3})) \end{align*}1234# 计算输出层神经元 o1 的误差o1_err = (o1_out - o[0])*(o1_out*(1-o1_out)) # o1_err = 0.12672890240521031# 计算输出层神经元 o2 的误差o2_err = (o2_out - o[1])*(o2_out*(1-o2_out)) # o2_err = -0.020385525551585255 根据BP2，隐藏层神经元的误差$\delta_{j}^{2}$计算方法为 \delta_{j}^{2} = ((\boldsymbol{\omega}^{3})^{T}\boldsymbol{\delta}^{3}) \odot \sigma'(z_{j}^{2})在计算时只需要计算与该神经元相关的连接，例如计算隐藏层神经元h1时，只需要计算权重w5和w7，设$\omega_{jk}^{l}$表示从$(l-1)$层第$k$个神经元到第$l$层第$j$个神经元的权重 \delta_{1}^{2} = (\omega_{11}^{3}*\delta_{1}^{3} + \omega_{12}^{3}*\delta_{1}^{3})*(a_{1}^{2}*(1-a_{1}^{2}))1234# 计算隐藏层神经元 h1 的误差h1_err = (w[4]*o1_err + w[6]*o2_err)*(h1_out*(1-h1_out)) # h1_err = 0.011232066954600498# 计算隐藏层神经元 h2 的误差h2_err = (w[5]*o1_err + w[7]*o2_err)*(h2_out*(1-h2_out)) # h2_err = 0.013374304651329562 根据BP4可计算某个权重的偏导数，例如w5，其偏导数为 \frac{\partial C}{\partial \omega_{11}^{3}} = a_{1}^{2}*\delta_{1}^{3}1234# 计算权重 w5 的偏导数d_w5 = o1_err*h1_out # d_w5 = 0.08182331501899741# 计算权重 w1 的偏导数d_w1 = h1_err*i[0] # d_w1 = 0.0011232066954600499 参考 知乎-梯度的方向为什么是函数值增加最快的方向？ 知乎-如何直观形象的理解方向导数与梯度以及它们之间的关系？ 博客园-刘建平Pinard-梯度下降（Gradient Descent）小结 知乎-AI从入门到放弃：BP神经网络算法推导及代码实现笔记 CS231n-Backpropagation, Intuitions CS231n-Optimization: Stochastic Gradient Descent MIT Press book:Deep Learning Michael Nielsen:Neural Networks and Deep Learning]]></content>
      <categories>
        <category>ML</category>
        <category>理论</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Deep Learning Overview]]></title>
    <url>%2F2019%2F09%2F29%2FDeep-Learning-Overview%2F</url>
    <content type="text"><![CDATA[神经网络是有史以来人类最接近“智能”的一次尝试，尽管就目前而言，这项技术与图灵测试所定义的智能存在极大的差距，但是它独特的解决问题的角度和方式，堪称一项伟大而优美的发明。神经网络起源于对人类大脑学习、思考和记忆产生的研究，说白了就是在研究造物主是如何创造我们的智慧的。虽然现阶段脑科学、神经科学和计算机科学还有很多关键问题仍待突破，但是神经网络和一系列衍生的应用已经能够达到商用级别，它的跨领域渗透能力在未来的发展中存在巨大的潜力，并将深刻的变革现代信息社会 神经网络同时也是有史以来发明的最优美的编程范式之一，在传统的编程中，人们告诉计算机做什么，把大问题分成许多小的、精确定义的任务，计算机可以很容易的执行。相比之下，在神经网络中，我们不会告诉计算机如何解决问题，而是让它在数据中学习，从而找到解决问题的方法 本文是一篇关于神经网络的综述性文章，包括对神经网络发展历史、基本概念、目前发展现状以及前沿领域的介绍 历史深度学习模型来源于从生物大脑(无论是人类大脑还是动物大脑)中受到的启发听觉、视觉、触觉、味觉，通过身体不同器官，最终都将转化为生物电信号从末梢神经一层一层传递到大脑的深层神经网中，思维的过程也是电信号在成百上千亿神经元之间非常快速的传递过程。传统生物学中认为，大脑中的不同区域有着不同的功能，专门用于处理语言、图像、运动等问题，但是最新的研究结果表明，在专门处理某类问题的区域上长期施加另一类信号的刺激，这个区域能够适应并处理这些刺激，例如在舌头上放置声音转换成电信号的刺激装置让舌头能够“听”声音，在手臂上放置超声波距离转换成电信号的刺激装置能够让皮肤“看”前方的障碍物。这说明大脑中这种神经元构成的网状结构是一种极其通用化的学习模型，能够对任何外界的刺激做出反应 深度学习中的神经网络是由两个主要观点驱动的： 大脑模型是一个很好的例子来证明智能行为的可能，从这一点出发，创造智能最直截了当的方向是运用逆向工程对大脑背后的运作原理进行分析并复制 理解大脑和人类智能的基础原理本身是非常有趣的，因此深度学习模型除了解决工程应用之外，对于研究阐明大脑、神经等基本科学问题也是有作用的 3个命名阶段概括地说，深度学习的发展经历了三个阶段 cybernetics 20世纪40-60年代的深度学习被称为控制论 connectionism 20世纪80-90年代神经网络被称为连接主义 deep learning 2006年开始以深度学习命名 上图中显示了3次人工神经网络研究的历史浪潮中的两次，第一次浪潮始于20世纪40年代至60年代的控制论，随着生物学习理论(McCulloch and Pitts, 1943; Hebb, 1949)的发展，Rosenblatt实现了第一个感知器模型(Rosenblatt, 1958)，允许训练单个神经元。第二次浪潮始于1980-1995年期间的连接主义方法，用反向传播(Rumelhart et al., 1986)来训练一个带有一个或两个隐藏层的神经网络。第三次浪潮始于2006年(Hintonet al., 2006; Bengio et al., 2007; Ranzato et al., 2007a) 在上个世纪60~90年代，神经网络的研究较为缓慢，研究者发现了处理神经网络的计算机器的两个关键问题。第一个问题是单层神经网络不能处理异或电路。第二个重要问题是计算机不够复杂，无法有效地处理大型神经网络所需的长时间运行。然而20世纪末期直到今天，由于第三次工业革命，信息的产生呈爆炸式增长，计算机的性能也有了极大的提升，这使得大型深层次的神经网络成为可能，其性能也在高速增长 神经网络框架基本的神经网络结构如下图所示 最左侧的称为输入层，其中的神经元称为输入神经元，最右边的称为输出层，中间层被称为隐藏层。这种结构在有些地方也被称为MLP(多层感知器)，也叫做FF(Feed Forward Network, 前馈神经网络) 网络中的每个神经元都是一个最简单的线性模型 \begin{equation} y = \omega{x} + b \end{equation}其中$\omega$和$b$分别叫做神经元的权重和偏置。神经网络的运行，是从输入层向输出层，一层一层传递结果的，上一层的输出作为下一层的输入，最终传递到输出层，而每个神经元上的权重和偏置的微小改变，都会影响最终的输出。学习的本质就是不断地调整这些神经元上的权重和偏置值 激活函数对于某一个神经元而言，由于简单的权重和偏置是一个线性模型，有以下缺陷 单个神经元的改变对网络的影响过大 线性函数无法理解输入变量之间的相互作用，最简单的就是线性模型不能学习异或关系$XOR$ 为了解决以上问题，我们可以不直接把线性模型用于神经元本身，而是用在一个变换后的输入上 \begin{align} y &= \sigma(z)\\ z &= \omega{x} + b \end{align}这里的$\sigma$是一个非线性变换，而这种非线性变换就叫做激活函数。目前常用的激活函数有4种 Sigmoid S型生长曲线 tanh 双曲正切函数 ReLU 线性整流函数 Maxout SigmoidSigmoid函数的定义如下 f(x) = \frac{1}{1 + e^{-x}}Sigmoid函数的图像如图所示sigmoid函数当输入x趋近于负无穷时，输出y趋近于0，当输入x趋近于正无穷时，输出y趋近于1，即把输入归一化到0~1范围内 Sigmoid函数的导数 f'(x) = f(x)*(1-f(x))Sigmoid导数的图像如图所示 tanhtanh函数的定义如下 f(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}tanh函数的图像如图所示 tanh函数的导数 f'(x) = 1 − f(x)^{2}tanh导数的图像如图所示 ReLUReLU函数定义如下 f(x) = max(0, x)ReLU函数的图像如图所示 ReLU函数的导数 \begin{equation} f(x) = \left\{ \begin{array}{**lr**} 0, & x \leq 0 \\ 1, & x > 0 \\ \end{array} \right. \end{equation}当输入x小于0时输出为0，当大于0时输出y 激活函数各有优劣，适用于不同的应用场景，合理选择才能达到好的效果 代价函数、梯度下降神经网络的学习，其目的是要达到给定训练样本${(x1,y1),(x2,y2)…}$，训练网络后，使得网络对于未知的输入数据能够尽可能逼近真实的输出。常用以下函数来描述这一目标，称作代价函数 C = \frac{1}{2n}\sum_{x}\vert{y(x) - a^{L}(x)}\vert^2其中$n$是训练样本总数，$L$表示网络层数，$a^{L}(x)$是网络输入为$x$时的激活向量 当代价函数被定义好了之后，如何通过代价函数来调整网络呢？要想让网络的输出尽量逼近真实值，则要将代价函数最小化。通常要最小化函数是通过求导取极值来实现的，但是对于神经网络而言，输入$x$通常不是一个变量，而是一组变量构成的向量，因此代价函数其实是个多元函数$C(x1,x2,x3…)$，其最小化通过梯度下降算法来实现。梯度是数学上多元函数偏导数的概念，梯度永远指向函数下降最快的方向，梯度下降算法的思想是每次将代价函数减去一个由梯度和一个步长构成的微小值，那么经过多次迭代，代价函数总是朝着最小值逼近，最终代价函数趋近于最小值不再变化时，表明网络已经收敛 反向传播神经网络中使用反向传播(backprop)来计算梯度， 发展现状一般在具体应用领域中不会直接使用这种基本的神经网络结构，在长时间的发展中，针对不同种类的问题，神经网络已经演化出了多种结构 CNN(Convolutional Neural Network，卷积神经网络)，适用于图像识别领域 RNN(Recursion Neural Network，递归/循环神经网络)，适用于语音、文字处理 LSTM(Long Short-Term Memory Network，长短记忆网络)，RNN的改进版 DBN(Deep Belief Network，深度信念网络)，多个神经网络堆叠，适用于识别 GAN(Generative Adversarial Network，生成对抗网络)，用于自学习 目前神经网络也已经有很多成熟的开发框架 sklearn caffe pytorch tensorflow keras 参考 Michael A. Nielsen, “Neural Networks and Deep Learning”, Determination Press, 2015 Ian Goodfellow and Yoshua Bengio and Aaron Courville, “Deep Learning”, MIT Press, 2016 wikipedia:Neural network]]></content>
      <categories>
        <category>ML</category>
        <category>理论</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>Deep Learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201909]]></title>
    <url>%2F2019%2F09%2F10%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201909%2F</url>
    <content type="text"><![CDATA[新闻 树莓派64位系统树莓派爱好者基地发布的基于Debian64位的树莓派操作系统，对内核进行了魔改、大量深度的优化和BUG修复，加入了很多新的功能和特性，特别是加入了KVM虚拟化的支持以及重点加强了对Docker的各项特性支持和优化。关键特性如下 WEB SSH QEMU-KVM 虚拟机 KVM 虚拟化的支持选项 Docker AUFS Swap zSWAP USB启动 UEFI启动 TCP加速 资源 tensorflow playgroundTensorflow官方发布了一个可视化神经网络结构的web，可选择一些数据分布，调整学习速率、激活函数、网络结构等，运行并实时查看网络的训练情况 taskbook一个命令行的任务管理工具 lookao一个干净的搜索引擎，不会追踪和监控用户隐私 arXiv一个免费的论文网站，包括数学、物理、计算机、统计、天文、定量生物、定量金融等领域的研究论文 codelf一个github开源项目，用于帮助给代码中的变量或者函数起名，web访问地址：https://github.com/unbug/codelf]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[DeepLearing.ai-Andrrew Ng-课程介绍]]></title>
    <url>%2F2019%2F09%2F10%2FDeepLearing-ai-Andrrew-Ng-%E8%AF%BE%E7%A8%8B%E4%BB%8B%E7%BB%8D%2F</url>
    <content type="text"><![CDATA[课程内容 Neural Network and Deep Learning Improving Deep Neural Network: Hyperparamter tuning, Regularization and Optimization Structuring your Machine Learning project Convolutional Network Network Natural Language Processing: Build sequence models. (RNN, LSTM) 什么是神经网络以预测房价为例，下图显示的是房价 $price$ 和房屋面积 $size$ 之间的关系 使用线性拟合可以得到一条蓝色曲线，该曲线类似一个线性整流单元(ReLU, Rectified Linear Unite)。ReLU是神经网络中激活函数的一种，关于ReLU和激活函数在后续内容再做介绍，此处先不做过多解释。我们用一个神经元来表示输入和输出的关系 这个神经元就是ReLU。如果增加更多的特征，一个神经元不足以表示特征和输出的关系，则需要增加神经元个数 可以理解，房屋大小和卧室数量决定的是房屋面积，地铁站数量和距离市中心远近决定的是房屋位置，而更深层的神经元是综合考量。其实不需要人为标定每个神经元的作用，而完全可以只输入特征，让神经元自己学习和决定。当给定足够的训练数据$(x,y)$，神经网络能够很好的拟合出$x,y$之间的映射关系 用神经网络进行监督学习神经网络属于监督学习范畴，即必须给定输入和输出的对应训练数据，网络基于此进行学习。神经网络的学习对象可以是结构化数据，也可以是非结构化数据。例如用户推荐系统利用公司数据库中用户的结构化信息如年龄、购买力、兴趣等数据进行学习，并向用户推荐所好；非结构化数据指的是不同于结构化数据，有完整的数据结构和成员，而只是一段数据序列，例如音频文件、文本文件、图片中的数据流，神经网络同样能够进行学习 深度神经网络为什么会快速兴起深度神经网络的兴起可以用下图来说明 横轴表示数据规模，纵轴表示学习算法的性能。对于传统算法例如SVM、Logistic、随机森林等算法而言，在早期数据量较小的情况下，能够获得不错的性能，但这些算法不知道如何处理更大规模的数据，当性能到达某个瓶颈后，再增加数据量并不能提高算法性能 神经网络算法并不是近几年来的新发现，很早就已经提出。但是由于近年来电子信息技术的发展，数据的产生呈爆炸式发展，数据的产生、存储、处理、通信能力和速度都得到了极大提升，而神经网络算法能够不断地从新数据中学习，这种学习能力随着数据规模迅速增长。小型的神经网络在大量数据的训练中已经能够超过传统算法很多，而最前沿的超大规模数据+超大规模神经网络的组合已经能够取得非常好的性能]]></content>
      <categories>
        <category>公开课</category>
        <category>DeepLearing.ai</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[OpenAI Five[译]]]></title>
    <url>%2F2019%2F09%2F02%2FOpenAI-Five-%E8%AF%91%2F</url>
    <content type="text"><![CDATA[待续~ 我们的OpenAI Five神经网络团队，已经开始击败业余的Dota2人类团队。虽然今天我们仍然存在一些限制，但我们的目标是在今年8月份的国际邀请赛上在有限的英雄上击败一只顶尖的专业战队。我们不一定会成功：dota 2是世界上最受欢迎和最复杂的电子竞技游戏之一，拥有富有创造力和积极性的专业人士，他们全年都在训练，以赢得dota每年4000万美元奖金池(所有电子竞技游戏中最大的一个)的一部分 OpenAI Five每天能够学习和它自己对战180年的有效游戏。它在一个放大版的PPO(Proximal Policy Optimization)上训练，有256个GPU和128000个CPU。对每个英雄使用一个单独的LSTM，不使用人类数据，它学习可识别的策略 为了衡量我们的程序，我们将在8月5日举办一场与顶级战队的比赛 问题人工智能的一个里程碑是在复杂的电子游戏中超越人类的能力，比如星际争霸或DOTA。与以前的人工智能里程碑(如象棋或围棋)相比，复杂的电子游戏开始捕捉现实世界的混乱和连续性。我们希望能够解决复杂电子游戏的系统是高度通用的，能够应用于游戏之外 Dota2是一款两个队伍各五名队员的实时策略性游戏，每个选手控制一个英雄。能够玩Dota的AI必须掌握以下内容： 长时间范围 Dota游戏以每秒30帧运行，平均每局45分钟，大概8000个时间点。大多数动作(比如命令英雄移动到某个地点)都会产生独立的轻微影响，但是一些独立的动作会在影响游戏的战略走向，例如TP；有些策略可以在整个游戏中发挥作用。OpenAI每4帧观察一次，产生2000次移动。象棋通常在40步之前结束，围棋一般是150步之内，每一步都是战略性的 局部观察状态 单位和建筑物只能看到它们附近的区域。地图的其余部分被大雾笼罩，隐藏着敌人及其战略。赢得游戏需要根据不完整的数据做出推论，以及模拟对手可能要做的事情。下棋和围棋都是全信息游戏 高维连续动作空间 在DOTA中，每个英雄可以采取几十个行动，许多行动针对的是另一个单位或地面上的一个位置。我们将空间离散为每个英雄17万个可能的动作(并非所有的每一个勾选都有效，例如在冷却时使用法术)；不计算连续部分，平均每个时间点有1000个有效动作。象棋和围棋的平均动作数是35和250 高维连续观测空间 Dota是在一张连续的大地图上玩的，地图上有十个英雄、几十个建筑物、几十个NPC单位、符文、树木、防御区等。我们的模型通过V社的bot API观察到一个Dota游戏的状态，即20000个(大部分是浮点)数字代表一个人可以访问的所有信息。一个象棋棋盘表示为大约70个枚举值(一个由6个棋子类型和次要历史信息组成的8x8棋盘)；围棋棋盘大约400个枚举值 Dota规则也非常复杂——游戏已经开发了十多年，游戏逻辑在数十万行代码中实现。这种逻辑每执行一个标记需要毫秒，而国际象棋或围棋引擎则需要纳秒。游戏每两周更新一次，不断改变环境语义 我们的方法我们的系统学习使用大规模的PPO。Openai Five和我们早期的1v1机器人都完全是从自我游戏中学习的，它们从随机参数开始，不使用搜索或从人类replay中学习 OPENAI 1V1 BOT OPENAI FIVE CPUs 60,000 CPU cores on Azure 128,000 preemptible CPU cores on GCP GPUs 256 K80 GPUs on Azure 256 P100 GPUs on GCP 收集经验 ~300 years per day ~180 years per day (~900 years per day counting each hero separately) 观测规模 ~3.3 kB ~36.8 kB 每秒游戏观察数 10 7.5 批量大小 8,388,608 observations 1,048,576 observations 每分钟批量数 ~20 ~60 RL研究人员(包括我们自己)普遍认为，长时间范围学习需要从根本上取得新的进展，例如分层强化学习。我们的结果表明，我们没有给予今天的算法足够的信任——至少当它们以足够的规模运行并且有合理的探索方法时 我们的代理被训练为最大化未来报酬的指数衰减总和，用一个称为γ的指数衰减因子进行加权。在最新的Openai Five训练中，我们将γ从0.998(以46秒的半衰期评估未来奖励)退火到0.9997(以5分钟的半衰期评估未来奖励)。相比之下，PPO纸中最长的地平线是0.5秒的半衰期，彩虹纸中最长的地平线是4.4秒的半衰期，观察和进一步观察纸使用46秒的半衰期 虽然目前版本的Openai Five在最后一次攻击时很弱(观察我们的测试赛，专业的Dota评论员blitz估计它大约是Dota玩家的中位数)，但它的目标优先级符合一个通用的专业策略。获得长期奖励，如战略地图控制，往往需要牺牲短期奖励，如从农业中获得的黄金，因为分组攻击塔需要时间。这一观察加强了我们的信念，即系统确实是在长期优化 模型结构Openai Five的每个网络都包含一个单层的1024单元的LSTM，它可以看到当前的游戏状态(从V社的bot API中提取)，并通过几个可能的动作头发出动作。每个头部都具有语义意义，例如，延迟此动作的节拍数、要选择的动作、该动作在单位周围网格中的X或Y坐标等。动作头部是独立计算的。 Openai Five使用的观察空间和行动空间的交互演示。OpenAIFive将世界视为20000个数字的列表，并通过发出8个枚举值的列表来执行操作。选择不同的行动和目标，了解OpenAIFive如何编码每个行动，以及它如何观察世界。这幅图像显示了一个人类看到的场景 OpenAI Five可以对与它所看到的相关的缺失状态片段做出反应。例如，直到最近，Openai Five的观测还没有包括弹片区(弹丸落在敌人身上的区域)，人是可以显而易见看到的。然而，我们观察到Openai Five正在学习走出(尽管不能避免进入)主动弹片区，因为它可以看到它的健康在下降 探索给出了一个能够处理长视距的学习算法，我们仍然需要探索环境。即使有了我们的限制，仍然有数百个物品、几十个建筑、法术和单位类型，以及一个需要学习的游戏机制的长尾——其中许多都会产生强大的组合。有效地探索这个组合广阔的空间并不容易 Openai Five通过自己和自己对战学习(从随机权重开始)，为探索和学习提供了一个很自然的环境。为了避免“战略崩溃”，代理训练80%的游戏与自己对抗，另外20%与过去的自己对抗。在第一场比赛中，英雄们漫无目的地在地图上走来走去。经过几个小时的训练，诸如farm或中途战斗等概念应运而生。几天之后，他们一直采用基本的人类策略：试图从对手那里偷取赏金符文，走到他们的一级塔去野区farm，并围绕地图游走以获得优势。经过进一步的训练，他们能够熟练运用5英雄推进等高级策略。 在2017年3月，我们的第一个代理打败了机器人，但与人类混淆了。为了在战略空间进行强行探索，在训练期间(并且仅在训练期间)，我们随机化了部队的属性(健康、速度、起始水平等)，然后开始击败人类。后来，当一个测试玩家持续击败我们的1v1机器人时，我们增加了我们的训练随机化，测试玩家开始输了。我们的机器人团队同时将类似的随机化技术应用于物理机器人，以从模拟转移到现实世界 OpenAI Five使用我们为1v1机器人编写的随机化方法。它还使用了新的分路方法。在每一个训练游戏开始时，我们随机“分配”每个英雄到一些道路的子集，并惩罚它从这些道路偏离直到游戏中随机选择的时间 好的奖励也有助于探索。我们的奖励主要是由人类跟踪以决定他们在游戏中的表现的指标组成：净值、杀戮、死亡、助攻、最后一击等等。我们通过减去其他团队的平均奖励来对每个代理的奖励进行后处理，以防止代理发生正和情况。我们对项目和技能构建进行硬编码(最初是为我们的脚本基线编写的)，并随机选择要使用的构建。Courier管理也从脚本化基线导入 协作OpenAI Five不包含英雄神经网络之间的明确通信通道。团队合作是由我们称之为“团队精神”的超参数控制的。团队精神的范围从0到1，这就决定了Openai Five的英雄们应该关心他们各自的奖励功能，而不是团队的平均奖励功能。我们通过训练将其值从0退火到1 Rapid我们的系统是一个通用的RL培训系统，称为Rapid，可应用于任何Gym环境。我们已经使用了快速解决其他问题在OpenAI，包括竞争自我发挥]]></content>
      <categories>
        <category>ML</category>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>DOTA</tag>
        <tag>译</tag>
        <tag>OpenAI</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[队列]]></title>
    <url>%2F2019%2F08%2F28%2F%E9%98%9F%E5%88%97%2F</url>
    <content type="text"><![CDATA[队列分两种 Queue: 先进先出，FIFO Deque: 头节点尾节点都可随意进出 特点及性能 size固定 查找O(n) QueueQueue又分为一般队列、可回滚队列和循环队列 普通队列，Normal Queue 回滚队列，Rollback Queue 环形队列，Circular Deque 普通队列一旦队满不能再入队。回滚队列和环形队列队满后都再次入队，回滚队列是通过移除队首元素实现，环形队列是通过覆盖队首元素并后移队首指针实现 普通队列和回滚队列结构和实现方式相似 curr，队尾标记 _queue，队列实体，可用array或list实现 环形队列需要队首队尾两个标记 front，队首标记 rear，队尾标记 _queue，队列实体，可用array或list实现 Normal Queue &amp; Rollback Queue 实现(C)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178typedef struct _q_unit &#123; int data;&#125; q_unit;typedef struct _queue &#123; int curr;#define QUEUE_F_RB 0x0002 uint16_t flag;#define QUEUE_SIZE 8 q_unit _queue[QUEUE_SIZE];&#125; queue;#define queue_foreach(q, u) \ int __k; \ u = &amp;q-&gt;_queue[q-&gt;curr]; \ \ for (__k = q-&gt;curr; \ (__k&gt;=0); \ __k--, \ u = &amp;q-&gt;_queue[__k]) \bool queue_empty(queue *q)&#123; return ((q-&gt;curr+1) == 0);&#125;bool queue_full(queue *q)&#123; return ((q-&gt;curr+1) == QUEUE_SIZE);&#125;void queue_dq(queue *q, q_unit *u)&#123; int i; if (queue_empty(q)) &#123; memset(u, 0x0, sizeof(q_unit)); return; &#125; memcpy(u, &amp;q-&gt;_queue[0], sizeof(q_unit)); for (i=0; i&lt;q-&gt;curr; i++) &#123; q-&gt;_queue[i] = q-&gt;_queue[i+1]; &#125; memset(&amp;q-&gt;_queue[q-&gt;curr], 0x0, sizeof(q_unit)); q-&gt;curr--; return;&#125;void queue_eq(queue *q, q_unit *u)&#123; if (queue_full(q)) &#123; if (mask_exst(q-&gt;flag, QUEUE_F_RB)) &#123; q_unit _front; queue_dq(q, &amp;_front); &#125; else &#123; return; &#125; &#125; q-&gt;curr = (q-&gt;curr+1)%QUEUE_SIZE; memcpy(&amp;q-&gt;_queue[q-&gt;curr], u, sizeof(q_unit)); return;&#125;void queue_init(queue *q, uint16_t flag)&#123; memset(&amp;q-&gt;_queue, 0x0, sizeof(queue)); q-&gt;curr = -1; mask_push(q-&gt;flag, flag);&#125;void queue_dump(queue *q)&#123; int max; int len; int size; q_unit *unit; size = q-&gt;curr+1; char *pos; char dump[256] = &#123;'\0'&#125;; if (!size) return; len = 0; max = 256; pos = dump; queue_foreach(q, unit) &#123; len = snprintf(pos, max, "%d ", unit-&gt;type); pos += len; max -= len; &#125; log_debug("queue size:%d, data: [%s]", size, dump); printf("\n");&#125;int main(int argc, char *argv[])&#123; q_unit u; q_unit a, b, c, d; queue queue; queue_init(&amp;queue, QUEUE_F_RB); a.type = 1; b.type = 2; c.type = 3; d.type = 4; queue_dump(&amp;queue); log_debug("eq [3 1 2 2 2 2 2 4 3 2 1] --&gt; "); queue_eq(&amp;queue, &amp;a); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;c); queue_eq(&amp;queue, &amp;d); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;a); queue_eq(&amp;queue, &amp;c); queue_dump(&amp;queue); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dump(&amp;queue); log_debug("eq [3 1 2 2] --&gt; "); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;b); queue_eq(&amp;queue, &amp;a); queue_eq(&amp;queue, &amp;c); queue_dump(&amp;queue); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dump(&amp;queue); log_debug("eq [3] --&gt; "); queue_eq(&amp;queue, &amp;c); queue_dump(&amp;queue); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dq(&amp;queue, &amp;u); log_debug("dq --&gt; [%d] ", u.type); queue_dump(&amp;queue);&#125;]]></content>
      <categories>
        <category>算法与数据结构</category>
        <category>Solution</category>
      </categories>
      <tags>
        <tag>Queue</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ti9-CN DOTA的落寞]]></title>
    <url>%2F2019%2F08%2F27%2FTI9-CN-DOTA%E7%9A%84%E8%90%BD%E5%AF%9E%2F</url>
    <content type="text"><![CDATA[8月25日早，在LGD与李逵开赛前，我在MAX+上发了一篇题为“LGD真能打过李逵和OG吗？”的帖子，对即将上演的败者组决赛和总决赛分享了一些我自己的看法。事实上那时候贴吧里已经有不少不看好LGD的帖子出现了(可能超过半数)，当然也有为LGD鼓气加油助威呐喊的，但是因为我在看了LGD与OG胜者组决赛BO3后，看的出来LGD确实存在一些问题，想要打败李逵或者OG是非常困难的，因此也更加关注那些与我观点类似的贴子。我想看看大神们是如何看待那场BO3的，如何看待目前的LGD“硬实力”，他们输掉胜者组决赛的根本原因是什么 然而结果来的很快，现实让人不愿接受却不得不接受，LGD 1:2 输给了李逵，从第1局的全场碾压李逵，到第2局极有争议的龙芯蝴蝶小狗被秒，再到第3局节奏完全混乱不断被gank，行云流水，现在想起来，就像刚发生过一样。我们那个高中刀友QQ群Team Random(致敬wings)，从本届Ti开始就热闹起来的氛围，突然就像25级的沉默术士放了大，大家都不说话了。我们其实都是怀着不是很看好LGD的心态看比赛的，虽然已经想到了大概率是这个结果，但是当结果真的摆在你面前的时候，仍然是会难受的 最后我还是决定继续看下去，毕竟接下来还有一场Ti7冠军与Ti8冠军的决战，我想看看这两个冲击不朽盾的队伍到底有多强，想看看1穿7的李逵和可能连续两年双冠的OG能给我们带来什么样的对决，他们是如何BP如何理解这款游戏的，同时也想彻底让自己死心，想知道LGD是否真的不配这个冠军。决赛确实没有让我失望，我想应该也没有让大多数或是在现场或是在屏幕前的玩家失望，心服口服，每场比赛节奏都非常紧凑，用FG的话说就是没有一分钟是划的。我们看到了w33的绝活米波、风行，看到了ana的大哥IO，看到了topson可怕的英雄池以及中期抗局势的强大能力，看到了强队的冷静与韧性，抓住对方一丁点失误，就能绝地翻盘的敏锐，看到了他们对目前版本阵容体系最优解的理解，也看到了CN DOTA与他们的差距，LGD确实配不上决赛的任意一边，配不上这个冠军，今年的CN DOTA不配举起不朽盾 8月25日晚，当ana大哥小精灵一路杀进李逵泉水，李逵被迫打出gg时，宣告了本届Ti9的结束，也无情的宣告了CN DOTA Best DOTA的结束… 回顾回顾今年Ti9全程，短短的两个礼拜，却上演了太多瞬间：采访短片中的LGD和VG互奶，皮鞋说自己是LGD20年老粉；拳酱说自己心态很好，把Ti看的不是很重要；一路高歌猛进的LGD和OG；早早就上了飞机的马桶哥；VP连他们自己也不知道为什么每到Ti就成了软脚虾；号称CN DOTA杀手的TNC，由于kuku辱华事件，导致每当TNC队员入场时，现场观众都嘘嘘，队员Timing在tiwtter上说自己打比赛很难受；VG的水牛体系，打的精彩绝伦，但只拿出来过1次；英勇无比的查理斯总是在LGD发现局势不对的时候被其他4人卖掉，队内地位可见一斑；状态低迷的李逵小组赛差点倒数第一直接淘汰，越打越强，每局都是回家局；中国俱乐部newbee，Ti上的队员都是外国面孔，搞得许多云玩家不知道是什么状况；奇厅长和midone的快乐蓝猫；KG打团就是5个莽夫在乱冲，问他们怎么赢的，不知道，可能是莽赢的吧；海选杀出来的RNG，被xiao8无限看好，其实成绩还不错 本届Ti我所知道的梗 虾之国 奇桶伪 桶，队危，速归；桶，队安，勿念；桶，队威，勿归；桶，队冠，没钱； Rtz被迫暴走 安斧天高辽，斧高不及毛 看OG打EG两小时只为了看握手的那两秒 LGD画SB 身怀六甲 谁也虐不了VP的泉 要论最关注的战队，我想应该是LGD、OG、秘密、VG、李逵吧，他们是最有机会冲击冠军的队伍，而其中尤其关注LGD、OG。冥冥之中似有定数，淘汰赛最后两天，LGD和OG的赛程与去年极其相似，都是LGD与OG在胜者组碰面，LGD 1:2 负于OG，Ti8是LGD在败者组击败EG杀上来，再和OG决战。 Ti8赛程 Ti9赛程 而今年是LGD和李逵在败者组对战，如果击败李逵的话，那真是Ti历史上最经典的对决了。败者组决赛前，DG就说有3个剧本： Ti7剧本：Ti7冠军李逵1穿8，再拿不朽盾 Ti8剧本：Ti8冠军OG，达成双冠 Ti9剧本：LGD力斩Ti7、Ti8冠军，成为Ti9冠军第3个剧本是最难也是最传奇的剧本，中国的Dotar们多想看到这个剧本啊，再也看不到了 回顾今年一整年Ti前大大小小的赛事，major、minor、梦幻联赛、震中杯、ESL等等，major始于中国重庆，Ti终结于中国上海。在这期间，观众们目睹了秘密和毛子的统治力，中国各个俱乐部的人员调整，CN DOTA的起伏不定，也见证了VG战队的磨合与快速成长，连续两个major冠军给中国Dotar们吃了颗定心丸。也看到了LGD的风格变化，改掉了Ti8的优势瞎逼送，动不动就打肉山被翻，对节奏的掌控能力有所提升。谁也想不到，最后的结局是这样的意料之外，情理之中 回顾整个Ti史，从古老的Navi与A队艺术家，到Ti2的IG、DK四保一、双核、三核体系，再到Ti4的newbee推进体系，Ti6的ban你妈呀wings，最后到Ti8、Ti9的LGD全村希望，CN DOTA走过了一个从加入到超越，从研究发明各种新战术引领全球到如今保守求稳，固步自封，虽然选手实力都是顶尖，但是战略层面早已和国外战队拉开差距的局面。有点像盛极而衰的大唐帝国，太阳慢慢的只剩下余晖 观点决赛结束后的那天晚上，我睡不着觉，刷各种赛后评论，MAX+、微博、知乎，看最后一集刀塔之夜，有人狂喷，有人删游，有人退坑，有人取关，但是其中不乏有很多优秀的观点 知乎-连续三年无缘冠军，CN DOTA 出了什么问题？ 知乎-败者组决赛 LGD 遭 Liquid 让一追二，止步 Ti9 季军，你有什么想说的？ 知乎-如何评价Ti9中国战队的表现？ 知乎如何评价 OG 战队的两连冠，他们究竟有多强？ 微博-ROTK谈西恩刀塔现状：强手太分散，没有银河战舰 微博-Fade退役 结合自己的所见所闻，以及别人的各种观点，总结了一下，CN DOTA走到今天的局面，主要有以下问题 国内DOTA圈子的不职业化，从选手、教练、俱乐部运营、转会交易、青训、组织比赛等等，都很不职业化 腌臜内幕，吃外围，打假赛，人情DOTA，资本 人口红利流失，移动端的崛起和PC时代的落寞，上手难度高，新人不友好，俱乐部人员流动封闭，新人难有上升渠道 环境恶劣，LOL、自走棋歧视，严重的优越感，恶劣的天梯环境，唯胜主义 游戏理解和观念落后，仍然停留在对线、打团、操作、farm上，而没有在战略战术、游戏机制、版本体系上下功夫研究 有将才无帅才 从业人员素质较低，各大战队教练普遍是从捡烟头时代过来的，只凭一点古老的辉煌战绩执掌现代DOTA，多是初高中学历，早该退休了 思考我在思考一个问题，DOTA这个游戏对于我来说，到底意味着什么？ 从远古的DOTA1时代，我就接触了它，高中无疑是DOTA的黄金时代，也是我的DOTA黄金时代，无论是周五下午跟那群老B们跑去网吧十几人围观两人solo，还是假期包夜的网吧五连坐从来没赢过，这些时光无疑给我带来了无比的欢乐，我收获了友情 大学时代，LOL盛行，全班全系几乎没有人在打DOTA，有些时候，我感觉自己被孤立了，可是我从来没有放弃它，虽然自己玩的少了，但是我更加关注比赛，关注那些还活跃在赛场上的他们；我更多的把时间和精力放在专业上，我收获了孤独和不放弃 如今我已奔3，成了一个一年到头都玩不了几次的云玩家，但是我关注了YC的刀塔节奏，关注了YYF的公众号，下载安装了MAX+，看OpenDota；每次更新，我都会看英雄、物品、游戏机制的改动，每出一个新英雄或至宝，我都会尝试一下；由于有自己的收入，我买了一些喜欢的饰品；我关注每一个major比赛，关注喜欢的战队、选手的近况，不错过每一个刀圈的瓜。我只是，不想离开它，不想和曾经的青春说再见 看到决赛上大爹手上厚厚一摞纸，看到各种评论分析里说的游戏数据分析，又听说OG与OpenAI似乎有秘密训练。我想，我是不是能为CN DOTA做点什么呢？我对机器学习感兴趣并在自学中，我热爱DOTA，我想对5年或10年后的自己说： 如果你在机器学习、数据挖掘领域练就了一些本领，考虑一下DOTA数据分析方向的研究吧，帮助CN DOTA重回辉煌 附 @media all and (orientation : landscape) { .ti-video {width:800px; height:600px;} } @media all and (orientation : portrait){ .ti-video {width:90%; height:250px;} } Ti9战队出场合集 V社纪录片-OG V社纪录片-ANA 真视界-TI8决赛纪录片 真视界-TI7决赛纪录片 追梦人-TI6 wings夺冠纪录片 [TI6 wings夺冠纪录片bgm]]]></content>
      <categories>
        <category>日志</category>
      </categories>
      <tags>
        <tag>DOTA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Autoencoder]]></title>
    <url>%2F2019%2F08%2F13%2FAutoencoder%2F</url>
    <content type="text"><![CDATA[自编码器(Autoencoder)是一种神经网络，它试图通过训练，将输入复制到输出。在其内部有一个隐藏层 h ，代表了对输入的编码。网络可以看作由两部分组成 编码器(encoder)：h = f(x) 解码器(decoder)：r = g(x) Autoencoder并不会设计成输出对输入的完美复制 g(f(x)) = x，因为这样的网络是没有意义的，相反，网络通常被刻意限制，只能近似复制。由于模型必须考虑选择输入的哪些部分来复制，因此可用于选择出输入的某些特性。网络需要在以下两件事上做平衡 对输入足够敏感，能够准确地重构输入 对输入不够敏感，模型不能简单地记忆或过度拟合训练数据 一般来说，Autoencoder主要用于数据降维和特征学习。最近，由于Autoencoder与潜变量模型之间的理论联系，Autoencoder被带到了生成概率模型的前沿。Autoencoder可看作是特殊的前向反馈网络，也可以使用再循环来训练 本文介绍以下自编码器 Undercomplete Autoencoders Regularized Autoencoders Denoising Autoencoders Contractive Autoencoders Undercomplete Autoencoders如果关心网络的隐藏层，而不关心解码器的输出，将会希望训练Autoencoder执行输入复制任务导致 h 具有有用的属性。约束网络的一种方式是让 h 比输入 x 维度更小。这种编码维度小于输入维度的Autoencoder被称为Undercomplete。Undercomplete能够学习输入最显著的特征，以最小化损失函数来描述学习模型： \begin{equation}L(x,g(f(x)))\end{equation} 其中 L 是损失函数，用来惩罚 g(f(x)) 和 x 的不相似性，例如使用均方误差MSE。 当解码器是线性的，L 是MSE时，Undercomplete与PCA类似，其任务是学习训练数据的主子空间。采用非线性编码函数 f 和非线性解码器函数 g 的Undercomplete可以学习到比PCA更强的非线性泛化能力。但是如果编解码的容量过大，Undercomplete就无法提取有用信息了 Regularized Autoencoders]]></content>
      <categories>
        <category>ML</category>
        <category>理论</category>
        <category>DeepLearning</category>
      </categories>
      <tags>
        <tag>ML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201908]]></title>
    <url>%2F2019%2F08%2F07%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201908%2F</url>
    <content type="text"><![CDATA[工具 videodownloader一个使用Electron制作的Youtube-dl的图形界面，可以下载包括Youtube、优酷、网易云音乐等在内的110个网站的视频 lazydocker命令行管理Docker服务的一个图形界面工具 新闻 生菜收割机器人英国剑桥大学开发了一个收割生菜的机器人。它用摄像头识别出每棵生菜，然后使用机器学习算法判断是否生长成熟，有没有感染疾病，如果一切正常，就进行收割 华为发布HarmonyOS 鸿蒙操作系统2019年8月9日，华为在其开发者大会上正式向全球发布了其自研的HarmonyOS。该系统是基于微内核的面向全场景的分布式操作系统，具有分布架构、内核安全、生态共享、天生流畅四大优势。鸿蒙OS不同于宏内核的Linux，属于微内核，并且属于seL4阶段，能够进行形式化验证(数学方法验证Bug Free)，仅仅将时钟管理、内存、中断、网络等内容放在微核中，其他系统级服务都在应用层以服务的形式提供，因此微内核代码量很小(9000行) 开源 genannGenann是一个轻量级、测试稳定的C语言神经网络库，其使用前向反馈神经网络，简单、快速、可靠、稳定 人物 Michael NielsenMichael Nielsen 是⼀位量⼦物理学家、科学作家、计算机编程研究⼈员，Neural Networks and Deep Learning(神经⽹络与深度学习)这本书的作者 文章 linux syslog 日志指南介绍Linux日志系统及协议的文档 Zdog一个JavaScript的3D设计和动画制作库 the-super-tiny-compiler一个介绍编译原理的教学示例项目]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Machine Learning Reference Reading]]></title>
    <url>%2F2019%2F07%2F31%2FMachine-Learning-Reference-Reading%2F</url>
    <content type="text"><![CDATA[预测 Using Machine Learning to Predict Stock Prices利用机器学习预测股票价格，作者“Vivek Palaniappan”，该文章使用了小波变换来滤除噪声，使用栈式自编码器自动提取特征，选取LSTM神经网络作为预测模型，在雪佛龙公司和艾森克美孚公司的MSE分别达到2.11和0.0945。Github地址：AlphaAI A comprehensive beginner’s guide to create a Time Series Forecast时间序列预测综合指南，详细介绍了时间序列数据分析及预测的一系列思路和方法]]></content>
      <categories>
        <category>ML</category>
        <category>Paper</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python Program Skill]]></title>
    <url>%2F2019%2F07%2F31%2FPython-Program-Skill%2F</url>
    <content type="text"><![CDATA[本文记录一些实际工作中使用到的Python语言编程技巧，或者学到的一些好用的用法 排序矩阵按某列排序应用场景： 线性回归做线性回归时，要拟合某个变量与输出的关系曲线，如果输入训练集在该变量的维度上是乱序的，拟合出的不是曲线，而是一团乱序的线。因此需要以该变量重排列 方法： np.argsort sorted &amp; lambda np.argsort123456789# examplex = np.array([[1,2,3],[4,5,6],[0,0,1]])_sort_idx = np.argsort(x[:,0]) # 按第0列升序排序x = x[_sort_idx].tolist()# wrapperdef col_sort(x, col): _x, _sort_idx = np.array(x), np.argsort(_x[:,col]) return _x[_sort_idx].tolist() np.argsort()方法可以获取升序(默认)排序后的索引 sorted &amp; lambda12345678910# examplex = [[1,2,3],[4,5,6],[0,0,1]]x = sorted(x, key=lambda x:x[0])x = [&#123;'a':1, 'b':2&#125;, &#123;'a':2, 'b':1&#125;]x = sorted(x, key=lambda x:x['b'])# wrapperdef col_sort(x, col): return sorted(x, key=lambda x:x[col]) 这种方法输入矩阵每行可以是一个list，也可以是dict]]></content>
      <categories>
        <category>Program Language</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Raspberry-Pi-4-Board]]></title>
    <url>%2F2019%2F07%2F25%2FRaspberry-Pi-4-Board%2F</url>
    <content type="text"><![CDATA[Reference目前很多资料还不全面，官网发布的原理图也只有一小部分 product Brief (PDF) Schematics (PDF) Power MxL7704 Datasheet (PDF) Ethernet BCM5421 Datasheet (PDF) USB Host Controller VL805 Datasheet (PDF) Memory MT53D1024M32D4DT-053 WT:D Datasheet (PDF) PowerRaspberry Pi 4 使用USB-C供电，其使用的电源管理芯片是MaxLinear公司的MxL7704 MxL7704是一款五输出通用PMIC(Power Management IC)，针对为低功耗FPGA、DSP和微处理器供电。其内部有4个同步降压调节器，可将5V输入调节成不同压降的输出，1路LDO+4路Buck。有一个I2C接口用于配置各路降压step，读取状态信息 封装图 树莓派4 USB-C供电部分 可看到PD_SENSE接到MxL7704的AN1引脚，AN2接地未使用，INT_SCL和INT_SDA两根I2C数据线应该是接到BCM2711，GLOBAL_EN信号用于控制MxL7704的使能，RUN_PG2连接到PG2引脚用来监控芯片是否工作正常 5路输出分别为 LDO —&gt; 3V3_AUD(3.3V) 这应该是给音频供电的 VOUT1 —&gt; LX1(3.3V/1.7A) 未知 VOUT2 —&gt; LX2(1.8V/2.3A) 未知 VOUT3 —&gt; LX3(1.1V/4.4A) 给DDR供电 VOUT4 —&gt; LX4(5.5A) 给BCM2711供电 EthernetGigabit Ethernet PHY 使用的是Broadcom的BCM54213PE，是一个全集成10Base-T/100Base-TX/1000Base-T的以太网千兆收发器。支持IEEE 802.3、802.3u和802.3ab标准，支持MII、GMII、TBI、RGMII和RTBI接口 树莓派4 Ethernet部分 目前原理图未公开GE PHY与BCM2711以哪种接口连接 USB Host ControllerUSB Host Controller使用的是VLA的LV805，是一个4端口的USB3.0主机控制器，xHCI接口标准 封装图 原理图中未公开该部分连接 MemoryRAM使用的是美光的MT53D1024M32D4DT-053 WT:D，FBGA Code:D9WHV，是一个32Gb的LPDDR4 DRAM，深度为1024Mb，位宽x32 原理图中未公开该部分连接]]></content>
      <categories>
        <category>嵌入式</category>
        <category>玩</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>Raspberry Pi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TCP-socket关闭后端口仍然占用]]></title>
    <url>%2F2019%2F07%2F25%2FTCP-socket%E5%85%B3%E9%97%AD%E5%90%8E%E7%AB%AF%E5%8F%A3%E4%BB%8D%E7%84%B6%E5%8D%A0%E7%94%A8%2F</url>
    <content type="text"><![CDATA[在使用TCP做一些数据传输的测试时发现，Server端程序关闭再启动时常常会出现该地址或端口已被占用的错误(关闭时socket已经释放)，如下是python脚本执行时错误打印：123456Traceback (most recent call last): File "server.py", line 35, in &lt;module&gt; sock.bind((host, port)) File "/usr/lib64/python2.7/socket.py", line 224, in meth return getattr(self._sock,name)(*args)socket.error: [Errno 98] Address already in use 关闭脚本使用的是”Ctrl+C”按键组合，代码中会监听SIGINT信号，在信号处理函数中关闭Server socket1234567def handler(signal_num,frame): global sock print "\nYou Pressed Ctrl-C. Exit" sock.shutdown(socket.SHUT_RDWR) sock.close() sys.exit(signal_num)signal.signal(signal.SIGINT, handler) 为什么已经释放的socket再次绑定时会出现地址被占用的错误？ 使用netstat命令查看占用的端口，发现存在一个处于TIME_WAIT的连接12[root@localhost awokezhou]# netstat -tn | grep "9091"tcp 0 0 172.16.79.132:9091 172.16.79.182:64775 TIME_WAIT TCP连接状态查看RFC-793文档，阅读了其中和TCP状态转换相关的内容 Connect StatesRFC-793中定义了一个TCP Connect在其生命周期的所有状态，原文如下 LISTEN represents waiting for a connection request from any remote TCP and port SYN-SENT represents waiting for a matching connection request after having sent a connection request SYN-RECEIVED represents waiting for a confirming connection request acknowledgment after having both received and sent a connection request ESTABLISHED represents an open connection, data received can be delivered to the user. The normal state for the data transfer phase of the connection FIN-WAIT-1 represents waiting for a connection termination request from the remote TCP, or an acknowledgment of the connection termination request previously sent FIN-WAIT-2 represents waiting for a connection termination request from the remote TCP CLOSE-WAIT represents waiting for a connection termination request from the local user CLOSING represents waiting for a connection termination request acknowledgment from the remote TCP LAST-ACK represents waiting for an acknowledgment of the connection termination request previously sent to the remote TCP (which includes an acknowledgment of its connection termination request) TIME-WAIT represents waiting for enough time to pass to be sure the remote TCP received the acknowledgment of its connection termination request CLOSED represents no connection state at all 一个TCP连接共有11种状态：LISTEN、SYN-SEND、SYN-RECEIVED、ESTABLISHED、FIN-WAIT-1、FIN-WAIT-2、CLOSE-WAIT、CLOSING、LAST-ACK、TIME-WAIT和CLOSED 重点关注和连接关闭有关的几个状态 FIN-WAIT-1 等待远端的连接终止请求，或者等待自己发送的连接终止请求被远端确认 FIN-WAIT-2 等待远端的连接终止请求 CLOSE-WAIT 等待本地用户的连接终止请求 CLOSING 等待自己发送的连接终止请求被远端确认 LAST-ACK 等待自己发送的连接终止请求被远端确认 TIME-WAIT 表示等待足够的时间以确保远端收到自己的终止请求确认 CLOSED 表示连接已经被完全关闭 只从字面意思理解，这几个状态有很多相似的地方，并不能理解到每个状态位于什么阶段 状态转移RFC-793文档中有一个TCP连接的状态转换示意图 由该图可以看出以下区别 FIN-WAIT-1 有两种情况会触发该状态： Server端在接收到Client端的连接请求后就主动关闭连接，向Client发送FIN后立即进入FIN-WAIT-1状态 建立连接后(双方都进入ESTABLISHED)，任何一方主动关闭连接，向对端发送FIN后立即进入FIN-WAIT-1状态 FIN-WAIT-2 只有处于FIN-WAIT-1的一方收到对方的FIN ACK后，立即进入该状态 CLOSE-WAIT 建立连接后，收到远端FIN后，回复FIN ACK，立即进入该状态 CLOSING 只有处于FIN-WAIT-1的一方收到对方FIN后，进入该状态 LAST-ACK 只有处于CLOSE-WAIT的一方主动关闭连接，向对方发送FIN后，进入该状态 TIME-WAIT 只有处于FIN-WAIT-12的一方收到对方FIN后，进入该状态 按照该图的状态转移流程，其实主要的转移路线有以下两条：12FIN-WAIT-1 --&gt; FIN-WAIT-2 --&gt; TIME-WAIT --&gt; CLOSEDCLOSE-WAIT --&gt; LAST-ACK --&gt; CLOSED 这两条路线的区别在于谁先发起连接终止请求，也就是谁先关闭socket MSL先关闭连接的一方最终会进入TIME-WAIT状态，由TIME-WAIT状态切换到CLOSED状态，但是中间需要等待一个超时时间2MSL。正是由于这个2MSL超时时间的存在，导致Server再次bind时发生错误 提出以下3个问题 什么是MSL，为什么需要MSL？ 先关闭的一方，接收到对端的FIN，发送FIN ACK后直接释放连接资源不行吗，为什么需要等待2个MSL时间？ 后关闭的一方，为什么不需要MSL？ 什么是MSLMSL英文全称是”Maximum Segment Lifetime”，即TCP片的最大存活时间 RFC-793原文中对于MSL的解释如下123456789Knowing When to Keep QuietTo be sure that a TCP does not create a segment that carries a sequence number which may be duplicated byan old segment remaining in the network, the TCP must keep quiet for a maximum segment lifetime (MSL) beforeassigning any sequence numbers upon starting up or recovering from a crash in which memory of sequence numbersin use was lost. For this specification the MSL is taken to be 2 minutes. This is an engineering choice, andmay be changed if experience indicates it is desirable to do so. Note that if a TCP is reinitialized in somesense, yet retains its memory of sequence numbers in use, then it need not wait at all; it must only be sureto use sequence numbers larger than those recently used. 大意为：为了确保TCP不会创建一个序列号与网络中已经存在的分片序列号重复的分片，在分配新的序列号之前必须在MSL时间内保持静默。标准规定的MSL时间为2分钟，是一个基于工程经验的选择 TIME-WAIT后为什么需要等待2个MSL？为了保证处于FIN-WAIT-2状态(记为A)时发送的最后一个ACK能够到达对端(记为B)。最后一个ACK可能在网络中丢失，使得B处于LAST-ACK状态无法进入CLOSED状态。B会超时重传这个FIN，A发送ACK丢失+B重传FIN到达A，这个时间刚好是2倍的MSL 后关闭的一方，为什么不需要2MSL？2MSL本质上是在等待最后一个ACK，后关闭的一方是FIN的发送方，等待ACK，有重传机制作保障，其状态是可控的，因此不需要其他等待超时 测试环境介绍 Server端 centos7虚拟机，IP地址172.16.79.132, python2.7，创建socket绑定localhost、9091端口 Client端 windows10，IP地址172.16.79.182，python2.7，创建socket向Server发起连接 linux 使用如下命令查看连接状态 1netstat -tn | grep '9091' windows使用如下命令查看连接状态 1netstat -tn | findstr '9091' 创建连接Server创建socket代码12345678import sockets = socket.socket()host = '0.0.0.0'port = 9091s.bind((host, port))s.listen(5)c, addr = s.accept() Client创建socket并向123import sockets = socket.socket()s.connect(('172.16.79.132', 9091)) 查看Server连接状态12[root@localhost awokezhou]# netstat -tn | grep "9091"tcp 0 0 172.16.79.132:9091 172.16.79.182:51725 ESTABLISHED 查看Client连接状态12λ netstat -tn | findstr '9091' TCP 172.16.79.182:51725 172.16.79.132:9091 ESTABLISHED InHost 双方都进入了ESTABLISHED状态，表明连接建立成功 Server先关闭socketServer关闭连接1c.close() 查看Server连接状态12[root@localhost awokezhou]# netstat -tn | grep "9091"tcp 0 0 172.16.79.132:9091 172.16.79.182:51725 FIN_WAIT2 查看Client连接状态12λ netstat -tn | findstr '9091' TCP 172.16.79.182:51725 172.16.79.132:9091 CLOSE_WAIT InHost Client关闭连接1c.close() 查看Server连接状态12[root@localhost awokezhou]# netstat -tn | grep "9091"tcp 0 0 172.16.79.132:9091 172.16.79.182:51725 TIME_WAIT 查看Client连接状态1λ netstat -tn | findstr '9091' Server最终进入了TIME-WAIT状态，Client连接已经释放。如果此时关闭并重启Server，在调用bind时就会报错 Client先关闭socketClient关闭连接1s.close() 查看Server连接状态12[root@localhost awokezhou]# netstat -tn | grep "9091"tcp 1 0 172.16.79.132:9091 172.16.79.182:51740 CLOSE_WAIT 查看Client连接状态12λ netstat -tn | findstr '9091' TCP 172.16.79.182:51740 172.16.79.132:9091 FIN_WAIT_2 InHost Server关闭连接1c.close() 查看Server连接状态1[root@localhost awokezhou]# netstat -tn | grep "9091" 查看Client连接状态12λ netstat -tn | findstr '9091' TCP 172.16.79.182:51740 172.16.79.132:9091 TIME_WAIT InHost Client再次发起连接12s = socke.socket()s.connect(('172.16.79.132', 9091)) 查看Client连接状态123λ netstat -tn | findstr '9091' TCP 172.16.79.182:51740 172.16.79.132:9091 TIME_WAIT InHost TCP 172.16.79.182:51750 172.16.79.132:9091 ESTABLISHED InHost 上一个连接还处于TIME-WAIT状态，但是又创建了一个新的连接。如果重启Client并不会报错，因为Client不用bind端口和地址 总结socket关闭后端口仍然占用的错误原因是Server端先关闭了连接，再次重启时旧连接并未释放，而是处于TIME-WAIT状态导致的。解决该问题有如下几种方法 Server端创建socket时设置端口重用SO_REUSEADDR 等待2MSL超时之后再创建socket Server端不要先关闭连接，让Client先关闭连接 ReferenceRFC-793 PDF服务器大量的fin_wait1 状态长时间存在原因分析网络的FIN_WAIT_2状态解释和分析TCP 协议（TIME_WAIT 状态TCP TIME-WAITTCP在FIN_WAIT1状态到底能持续多久以及TCP假连接问题防止linux出现大量 FIN_WAIT1,提高性能深入理解TCP(2)TCP的断开一定是四次挥手吗?FIN_WAIT_2和CLOSE_WAIT，TIME_WAIT以及LAST_ACK的细节TCP/IP中MSL详解Time-wait状态(2MSL)一些理解]]></content>
      <categories>
        <category>协议</category>
        <category>TCP</category>
      </categories>
      <tags>
        <tag>TCP</tag>
        <tag>protocol</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Raspberry-Pi-4-CPU]]></title>
    <url>%2F2019%2F07%2F25%2FRaspberry-Pi-4-CPU%2F</url>
    <content type="text"><![CDATA[本文汇总所有和树莓派4代CPU相关材料。树莓派4使用的CPU是博通公司的BCM2711，该芯片的datasheet无论是在博通官网、树莓派官网、google上都搜索不到，可能是还未公开 Reference树莓派官网发布的初步datasheet CPU Features processor: Broadcom BCM2711, Cortex-A72 (ARM v8), quad-core, 64-bit SoC @ 1.5GHz Memory： 1GB, 2GB or 4GB LPDDR4 Wi-Fi: 2.4G/5G, IEEE 802.11b/g/n/ac WLAN Ethernet: GE Bluetooth: 5.0 with BLE USB: 2×USB3.0, 2×USB2.0 GPIO: 40-pin, support multiplex(UART、I2C、SPI、SDIO、DPI、PCM、PWM、GPCLK) Video &amp; sound: 2×micro HDMI ports(up to 4Kp60 supported) 2-lane MIPI DSI display port 2-lane MIPI CSI camera port 4-pole stereo audio and composite video port Multimedia: H.265(4Kp60 decode), H.264(1080p60 decode, 1080p30 encode), OpenGL ES, 3.0 graphics SD Card: 1× Input power: 5V DC via USB-C connector(min 3A) 5V DC via GPIO header (min 3A) PoE IEEE 802.3af]]></content>
      <categories>
        <category>嵌入式</category>
        <category>玩</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>Raspberry Pi</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Raspberry-Pi-4-benchmark]]></title>
    <url>%2F2019%2F07%2F23%2FRaspberry-Pi-4-benchmark%2F</url>
    <content type="text"><![CDATA[(待续) tom’s HARDWARE 发布了一篇关于树莓派4的benchmark文章，详细介绍了对其多项测试方法和结果 CPULinpack BenchmarkCPU性能测试使用了Linpack Benchmark，Linkpack Benchmark 是对计算机浮点执行率的度量。它是通过运行一个求解密集线性方程组的计算机程序来确定的 Linkpark Q&amp;A Sysbench CPU testsysbench的cpu测试是在指定时间内，循环进行素数计算 Memory内存是做吞吐量测试，读写1M的内存块，看每秒的读写速度 文件压缩分别查看单进程和多进程下的压缩速度 GPUopenArena BenchmarkopenArena是一款需要GPU渲染的游戏，通过固定分辨率下查看平均帧率，来评估GPU性能 Storage查看吞吐量，每秒的读写能力 NetworkingEthernet吞吐量测试 Wi-Fi吞吐量测试 PowerPower Draw benchmark分别看负载和空闲状态下的功耗 Thermal Throttling Benchmark4KFFmpeg TestWeb SurfingJetstream 1.1Web HostingMachine LearningCompiling Code]]></content>
      <categories>
        <category>嵌入式</category>
        <category>玩</category>
        <category>树莓派</category>
      </categories>
      <tags>
        <tag>Raspberry Pi</tag>
        <tag>performance</tag>
        <tag>benchmark</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ReverseLinkList]]></title>
    <url>%2F2019%2F07%2F16%2FReverseLinkList%2F</url>
    <content type="text"><![CDATA[Reverse Link List题目单向链表反向，输入为 1-&gt;2-&gt;3-&gt;4-&gt;5-&gt;NULL，输出为5-&gt;4-&gt;3-&gt;2-&gt;1-&gt;NULL 分析遍历一次，单次循环中将遍历的元素next指向上一个元素 关键点：需要3个指针，一个前向指针prev用于指向当前遍历元素的上一个元素；当前遍历元素指针；由于修改了当前元素的next指针，因此需要一个后向指针next，提前保存下一个元素地址 特殊情况：当链表元素只有头或者2个元素时 C语言实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687typedef struct _list_node &#123; int x; struct _list_node *next;&#125; list_node;static inline bool list_node_empty(struct _list_node *head)&#123; return (!head-&gt;next);&#125;static inline void list_node_init(struct _list_node *head, int x)&#123; head-&gt;x = x; head-&gt;next = NULL;&#125;static inline struct _list_node *list_node_tail(struct _list_node *head)&#123; struct _list_node *p = head; while (p-&gt;next != NULL) &#123; p = p-&gt;next; &#125; return p;&#125;static inline void list_node_append(struct _list_node *new, struct _list_node *head)&#123; struct _list_node *tail; tail = list_node_tail(head); tail-&gt;next = new; new-&gt;next = NULL;&#125;#define list_node_foreach(pos, head) \ for (pos = (head); \ pos != NULL; \ pos = pos-&gt;next) \static inline struct _list_node *list_node_revert(struct _list_node *head)&#123; struct _list_node *p_prev=head, *p, *p_next; if (!head || !head-&gt;next) return head; p = head-&gt;next; p_prev-&gt;next = NULL; while (p != NULL) &#123; p_next = p-&gt;next; p-&gt;next = p_prev; p_prev = p; p = p_next; &#125; return p_prev;&#125;int main(int argc, char *argv)&#123; list_node *p, *revert; list_node head, node1, node2, node3, node4; list_node_init(&amp;head, 1); list_node_init(&amp;node1, 2); list_node_init(&amp;node2, 3); list_node_init(&amp;node3, 4); list_node_init(&amp;node4, 5); list_node_append(&amp;node1, &amp;head); list_node_append(&amp;node2, &amp;head); list_node_append(&amp;node3, &amp;head); list_node_append(&amp;node4, &amp;head); list_node_foreach(p, &amp;head) &#123; log_debug("data:%d", p-&gt;x); &#125; revert = list_node_revert(&amp;head); list_node_foreach(p, revert) &#123; log_debug("data:%d", p-&gt;x); &#125; &#125; Python实现123456789101112131415161718192021222324252627282930313233343536373839404142class ListNode(object): def __init__(self, x): self.val = x self.next = Nonedef reverseList(head): cur, pre = head, None while cur: cur.next, pre, cur = pre, cur, cur.next return pre def getListTail(head): curr = head while (curr.next): curr = curr.next return currdef appendList(item, head): tail = getListTail(head) tail.next, item.next = item, Nonelist_head = ListNode(1)data1 = ListNode(2)data2 = ListNode(3)data3 = ListNode(4)data4 = ListNode(5)appendList(data1, list_head)appendList(data2, list_head)appendList(data3, list_head)appendList(data4, list_head)curr = list_headwhile (curr): print("data:&#123;&#125;".format(curr.val)) curr = curr.nextreverse = reverseList(list_head)curr = reversewhile (curr): print("data:&#123;&#125;".format(curr.val)) curr = curr.next python代码可利用一个赋值语句对多个对象赋值特性，省略后向指针]]></content>
      <categories>
        <category>算法与数据结构</category>
        <category>LeetCode</category>
      </categories>
      <tags>
        <tag>LinkList</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[构建gcc交叉编译链]]></title>
    <url>%2F2019%2F07%2F05%2F%E6%9E%84%E5%BB%BAgcc%E4%BA%A4%E5%8F%89%E7%BC%96%E8%AF%91%E9%93%BE%2F</url>
    <content type="text"><![CDATA[构建gcc交叉编译链本文翻译“Cross-compilation Tool Making” download Binutils: Index of /gnu/binutils gcc: Index of /gnu/gcc/ Glibc &amp; Glibc-ports: Index of /gnu/glibc mpfr-2.4.2: wget ftp://gcc.gnu.org/pub/gcc/infrastructure/mpfr-2.4.2.tar.bz2 gmp-4.3.2: wget ftp://gcc.gnu.org/pub/gcc/infrastructure/gmp-4.3.2.tar.bz2 mpc-0.8.1: wget ftp://gcc.gnu.org/pub/gcc/infrastructure/mpc-0.8.1.tar.gz dir setup-dir: download files src-dir: unzip files Binutils install1./configure --target=$TARGET --prefix=$PREFIX gccLinux ARM交叉编译工具链制作过程 - w36680130的博客 - CSDN博客 Build a GCC-based cross compiler for LinuxSection 1. 开始之前关于这篇教程有时候，你正在开发的平台和使用的计算机不匹配。例如，你可能想要在你的x86/Linux笔记本电脑上构建一个PowerPC/Linux应用程序。使用GNU工具包中的gcc、gas和lds工具，您可以指定并构建一个交叉编译器，它将使您能够在您的机器上为其他目标构建程序。只需多做一点工作，您甚至可以设置一个能为各种不同的目标构建应用程序的环境。在本教程中，我将介绍在你的系统上构建交叉编译器所需的整个过程。还将讨论为一系列目标构建一个完整的环境，向您展示如何与distcc和ccache工具集成，并介绍更新你的新开发平台以及更新最新版本的方法。 要构建一个交叉编译器，您需要了解典型UNIX开源项目的构建过程的基本知识、一些基本的shell技能和足够的耐心。 预备知识要构建一个交叉编译器，您需要一个可工作的C编译器(gcc通常是一个好主意)。大多数基于Linux/unix的操作系统都提供了C编译器。您还需要用于构建交叉编译器的各种工具的源代码。您可以从GNU(http://www.gnu.org)下载GNU工具。 除了这些工具之外，还需要目标平台的头文件副本。对于Linux目标，使用Kernel.org(http://www.kernel.org)提供的通用Linux内核头文件。 Section 2. 交叉编译为什么需要交叉编译并不总是在同一个平台上编写和构建应用程序。例如，对于许多嵌入式环境，用于RAM和存储的空间较小，通常小于256MB，甚至可能小于64MB。一个合理的C编译器、相关的工具和所需的C库都不适合这么小的空间，更不用说运行了。 实际上，在这样的环境下开发显然更加困难。如果您使用键盘显示器访问和使用系统，使用成熟的编辑器(例如emacs)或成熟的开发环境(IDEA)是不可能的。许多嵌入式解决方案甚至没有网络访问的能力。 跨编译器使您能够在一个具有开发能力的平台(主机)上进行开发，而实际构建另一个目标系统的程序。目标计算机并不需要可用，您所需要的只是一个编译器，它知道如何为您的目标平台编写机器码。交叉编译器在其他情况下也很有用。我曾经不得不在一台没有安装C编译器的计算机上工作，而且我没有获得预编译二进制文件的简单方法。但是，我确实拥有GNU编译器集合(GCC)、C库(newlib)和计算机上的二进制实用程序的必要源代码，而计算机上确实有C编译器。使用这些工具，我可以先为目标计算机构建一个交叉编译器，然后再为目标计算机构建一个本机编译器，我可以跨目标计算机复制并直接使用它。 当您的计算机速度较慢且速度快得多，并且希望在几分钟内而不是几小时或几天内构建时，交叉编译器也很方便。我曾使用这种方法在一台计算机上执行升级到新软件版本的任务，在此过程中，需要花费2-3天的时间来重建所有组件——而此时计算机正在执行其已经非常重要的服务器任务。 在我向您展示构建交叉编译器的细节之前，让我们仔细看看编译是如何工作的，以便您更好地理解为什么——更重要的是——交叉编译是如何工作的。 交叉编译如何工作编译器的工作方式很简单。几个不同的组件一起工作， 最终目标是生成目标CPU使用的字节码。当您可以生成组装好的字节码时，您已经成功地交叉编译了。 任何编译器的主要组件包括: 解析器：解析器将原始语言源代码转换为汇编语言，因为要从一种格式转换为另一种格式(C语言转换为汇编)，所以解析器需要知道目标汇编语言 汇编器：汇编程序将汇编语言代码转换成CPU执行的字节码 链接器：链接器将汇编程序生成的单个目标文件组合到可执行应用程序中。不同的操作系统和CPU组合通常使用不同的封装机制和标准。链接器需要知道该目标格式才能工作 标准C库：核心C函数(例如printf)在C库中提供。假设应用程序中使用了来自C库的函数，那么这个库将与链接器和源代码结合使用，生成最终的可执行文件 在标准的、基于主机的C编译器中，每个组件都被设计成生成相应的汇编代码、字节码和主机本身的目标执行格式。在交叉编译器中，虽然构建应用程序是为了在主机上执行，但汇编语言、链接器和C库都是为目标平台和处理器设计的。例如，基于intel的在Linux机器上，您可以交叉编译一个应用程序，以便汇编语言和最终的应用程序都适用于基于solarisis的SPARC主机。 因此，构建交叉编译器依赖于构建C编译器套件的替代版本，该版本为目标主机生成并链接应用程序。幸运的是，因为可以编译GCC和相关的工具，所以可以构建自己的交叉编译器。 交叉编译器构建过程GNU工具集(即GCC)，包括C编译器、二进制实用程序和C库，都有一些好处，其中最重要的是它们是免费的、开放源码的，并且易于编译。从跨编译器的角度来看，一个更大的好处是，由于GCC已经移植到许多平台上，代码支持几种不同的CPU和平台类型。然而，也有一些限制。GCC并不支持所有处理器类型(尽管它生成的处理器类型最多)，也并不支持所有平台。配置工具会在执行时警告您这一点。 要构建一个交叉编译器，您需要从GNU套件构建三个组件： binutils：binutils包包括基本的二进制实用程序，如汇编程序、链接器和相关工具，如Size和Strip。二进制实用程序包含用于构建应用程序的核心组件和用于构建和操作目标执行格式的工具。例如，Strip程序从目标文件或应用程序中删除符号表、调试和其他“无用”信息，但要做到这一点，实用程序需要知道目标格式，以便不删除错误的信息 gcc：gcc是编译过程的主要组件。Gcc包含C预处理器(cpp)和转换器，后者将C代码转换为目标CPU汇编语言。Gcc还充当整个流程的接口，相应地调用cpp、翻译程序、汇编程序和链接器 newlib/glibc：这个库是标准的C库。Newlib是通过Redhat开发的，在为嵌入式目标设计的交叉编译器中，它对用户稍微友好一些。您也可以使用GNU库(glibc)，但我在本教程中主要使用newlib 您还需要目标操作系统的头文件，这是必需的，以便您能够访问构建应用程序所需的所有操作系统级函数和系统调用。使用Linux，您可以相当容易地获得头文件。对于其他操作系统，您可以复制一组现有的头文件。稍后我将更详细地说明头文件。 您可以选择为目标主机构建GNU调试器(gdb)。您不能构建一个调试器来在主机上运行时为目标执行代码，因为这样做需要仿真。不过，您可以为目标主机构建一个gdb可执行文件。 Section 3. 准备目的地和共存在开始配置过程之前，需要确定将在何处安装编译器和相关工具。您有两个选择:要么将它们安装到一个完全独立的目录中，要么将它们作为现有安装的一部分安装。 GNU工具集的许多好处之一是内置在安装结构中的设计，它使不同目标平台的工具和组件能够共存。安装软件后，您提供的安装前缀将根据正常布局进行组织，并为特定于目标的工具添加一个目标目录。例如，下面的结构取自我为泛型安装了交叉编译器的系统PowerPC / Linux平台: drwxrwxrwx 2 root root 4096 Nov 16 16:48 bin/drwxrwxrwx 2 root root 4096 Nov 17 12:53 info/drwxrwxrwx 2 root root 4096 Nov 17 12:53 lib/drwxrwxrwx 3 root root 4096 Nov 16 16:44 man/drwxrwxrwx 4 root root 4096 Nov 16 16:48 ppc-linux/drwxrwxrwx 3 root root 4096 Nov 16 16:43 share/ 如果你查看bin目录，你会看到每个主要的二进制实用程序都有你的build-target前缀: -rwxr-xr-x 1 root root 2108536 Nov 16 16:46 ppc-linux-addr2line-rwxr-xr-x 2 root root 2157815 Nov 16 16:45 ppc-linux-ar-rwxr-xr-x 2 root root 3398961 Nov 16 16:48 ppc-linux-as-rwxr-xr-x 1 root root 2062804 Nov 16 16:47 ppc-linux-c++filt-rwxr-xr-x 2 root root 2907348 Nov 16 16:48 ppc-linux-ld-rwxr-xr-x 2 root root 2140893 Nov 16 16:46 ppc-linux-nm-rwxr-xr-x 1 root root 2552661 Nov 16 16:46 ppc-linux-objcopy-rwxr-xr-x 1 root root 2708801 Nov 16 16:45 ppc-linux-objdump-rwxr-xr-x 2 root root 2157810 Nov 16 16:46 ppc-linux-ranlib-rwxr-xr-x 1 root root 371010 Nov 16 16:46 ppc-linux-readelf-rwxr-xr-x 1 root root 2008330 Nov 16 16:45 ppc-linux-size-rwxr-xr-x 1 root root 1982880 Nov 16 16:46 ppc-linux-strings-rwxr-xr-x 2 root root 2552660 Nov 16 16:46 ppc-linux-strip* 主要工具(如gcc)只是执行编译的后台工具的包装器，因此gcc可以在为不同平台构建时确定使用哪个工具。只要您继续将gcc用于您的构建需求，以及您构建的其他库和组件使用GNU configure结构，您应该能够在标准工具集旁边安装交叉编译工具。在大多数Linux平台上，这个位置都是/usr/local。我喜欢把我的交叉编译器和宿主编译器分开，这样我就可以保持不同版本的宿主和跨编译器工具集。 确定您的目标平台准备交叉编译的下一步是确定目标平台。GNU系统中的目标有一个特定的格式，这个信息在整个构建过程中被用来识别各种工具的正确版本。因此，当您使用特定的目标运行GCC时，GCC会在目录路径中查找包含该目标规范的应用程序路径。 GNU目标规范的格式是CPU-PLATFORM-OS。x86的Solaris 8是i386-pc-solaris2.8，而Macintosh OS X是powerpc-apple-darwin7.6.0。PC上的Linux具有目标i686-pc-linux-gnu。本例中的-gnu标记表示Linux操作系统使用的是gnu风格的环境。 有许多方法可以识别目标，包括简单地了解或猜测目标规范。例如，大多数Linux目标可以根据它们的CPU来指定；因此，PowerPC/Linux是ppc-linux。然而，最好的方法是使用config.guess脚本，它随任何脚本一起提供GNU套件，包括我在本教程中使用的那些。要使用这个脚本，只需从一个shell运行它: 12$ config.guessi686-pc-linux-gnu 对于那些无法运行此脚本的系统，有必要检查该文件以确定一些可能的目标。只要在目标CPU上使用grep，就可以了解所需的目标规范。出于本教程的目的，我将为i386-linux平台创建一个交叉编译器，这是一个公共目标，也是受支持程度最高的平台之一。 设置构建环境开始构建之前的最后一个阶段是创建一个合适的环境。您只需要创建一个简单的目录集，您可以使用它来构建不同的组件。 首先，创建一个构建目录:1$ mkdir crossbuild 接下来，获取gcc、binutils、gdb和newlib的最新版本，并将它们提取到目录中:12345$ bunzip2 -c gcc-3.3.2.tar.bz2|tar xf -$ bunzip2 -c binutils-2.14.tar.bz2 |tar xf -$ bunzip2 -c linux-2.6.9.tar.bz2 |tar xf -$ bunzip2 -c gdb-6.3.tar.bz2|tar xf -$ bunzip2 -c glibc-2.3.tar.bz2|tar xf - 如果正在为Linux目标构建，还需要解包Linux -threads包(如果目标平台支持它)。 现在，创建实际构建软件的目录。基本布局是为每个组件创建一个目录:1$ mkdir build-binutils build-gcc build-glibc build-gdb 如果要创建几个不同的交叉编译器，可以考虑为每个目标创建单独的目录，然后在每个目标目录中创建上面的目录。 准备工作完成后，就可以配置和构建每个工具了。 Section 4. 配置和构建设置环境变量重新输入所有东西是令人沮丧的，你可能会输入错误的东西。为了简化工作，我创建了几个环境变量来节省输入。我假设下面有一个类似伯恩的壳；如果使用csh、tcsh或类似的shell，可能需要使用特定于shell的技术。 1234export TARGET=i386pcexport PREFIX=/usr/local/crossgccexport TARGET_PREFIX=$PREFIX/$TARGETexport PATH=$PATH:$PREFIX/bin 获取操作系统头文件操作系统头对于获取编译器需要的信息是必要的，这些信息用于目标平台支持的系统函数调用。对于Linux目标，获得头文件的最佳方法是下载适当内核的最新副本。您需要对内核进行基本配置，以便生成正确的头文件供您复制，但是，您不需要构建或编译内核。对于我的示例目标i386-linux，必要的步骤如下：12$ cd linux-2.6.9$ make ARCH=i386 CROSS_COMPILE=i386-linux- menuconfig 注意，上面后面的连字符不是拼写错误。通过运行上面的命令，将提示您为内核配置组件。因为您不需要过多地担心内核本身的内容，所以您只需要使用默认选项，保存配置，然后退出工具。 现在，您需要将标题复制到目标目录:1234$ mkdir -p $TARGET_PREFIX/include$ cp -r include/linux $TARGET_PREFIX/include$ cp -r include/asm-i386 $TARGET_PREFIX/include/asm$ cp -r include/asm-generic $TARGET_PREFIX/include/ 显然，我在上面的代码中基于示例目标做了一些假设。如果您正在为PowerPC目标构建，您应该已经从asm-ppc复制了文件。 现在可以开始构建实用程序了。 构建binutilsbinutils实用程序是整个系统的核心。它提供了系统其余部分所需的基本汇编器和链接器功能。每种情况的第一个阶段都是为另一种目标平台配置每个包。构建的第一步：123$ cd build-binutils$ ../binutils-2.14/configure --target=$TARGET --prefix=$PREFIX --disable-nls -v$ make all 目标和前缀应该是显而易见的。命令禁用国家语言支持(NLS)。无论如何，许多嵌入式平台都不能支持必要的表。对于大多数交叉编译器，NLS的有用性是有争议的，因为目标(通常是嵌入式设备)不能在内存中保存必要的NLS表。 构建的这个阶段将需要一些时间。因为您仍然在使用主机编译器和工具进行构建，所以可以使用ccache和distcc工具来帮助加速这个过程。有关这些工具的更多信息，请参见参考资料(# Resources)部分。 现在，您已经准备好构建GCC，它稍微复杂一些。 构建第一阶段GCCGCC比binutils更复杂，这仅仅是因为构建GCC的标准方法构建两个编译器。GCC使用GNU工具来构建一个主程序(即，第一阶段，或引导)编译器，可以构建和解析基本代码。然后GCC使用目标的可用库和头文件来构建完整的编译器。构建GCC第一阶段编译器需要对配置脚本的选项进行一些较小的更改，这样您就可以在没有适当头文件的情况下构建第一阶段编译器。严格地说，只有在构建了库之后才会有头文件。with-newlib命令并不一定意味着您正在使用newlib:它只是告诉配置脚本不要担心头文件。 12345$ cd build-gcc$ ../gcc-3.3.2/configure --target=$TARGET --prefix=$PREFIX \--without-headers --with-newlib -v$ make all-gcc$ make install-gcc 与构建binutils一样，这个阶段需要一段时间才能完成。时间长短取决于您的主机，但是可以期望从一个小时(即使是在高速机器上)到慢速或繁忙的主机上最多5或6个小时。 构建newlib您可以使用glibc的newlib。c.总的来说，newlib在嵌入式平台上表现得更好，因为newlib的设计初衷就是支持嵌入式平台。Glibc更适合linux风格的主机。 在构建newlib时，需要使用目标编译器和工具构建库。当然，该库应采用目标CPU和平台的格式和语言，以便用于构建依赖于库组件的应用程序:1234$ cd build-newlib$ CC=$&#123;TARGET&#125;-gcc ../newlib-1.12.0/configure --host=$TARGET --prefix=$PREFIX$ make all$ make install 构建了newlib之后，您可以基于此代码创建最终的GCC来创建最终的编译器。或者，您可以使用glibc，我将在下一个面板中介绍它。 构建glibcglibc包构建起来很简单；与以前版本的主要区别在于，与newlib一样，现在开始使用刚才构建的引导跨编译器。您还需要告诉configure脚本操作系统的头文件保存在哪里。最后——这是一个很大的区别——定义构建的主机而不是目标。这是因为您已经构建的GCC和二进制实用程序意味着这台机器是您的开发主机；您指定的GCC将为您生成必要的目标代码。123$ CC=$&#123;TARGET&#125;-gcc ../glibc-2.3/configure --target=$TARGET \--prefix=$PREFIX --with-headers=$&#123;TARGET_PREFIX&#125;/include$ make all 如果您想包含Linux threads选项，需要在configure脚本中添加--enable-add-ons选项。同样，这个过程需要一些时间来完成。从来没有人说构建交叉编译器是快速的。 要安装glibc，您仍然使用make，但是您显式地设置了安装根并清空了前缀(否则，这两者将被连接起来，这不是您想要的)：1$ make install_root=$&#123;TARGET_PREFIX&#125; prefix="" install 最后，您可以构建gcc的最终版本，它现在使用上面的库和头信息。 构建最终的GCC最后的GCC使用您刚刚编译的头文件和库(使用您选择的目标)构建完整的gcc系统。不幸的是，这意味着在构建完整的gcc版本时，等待的时间更长。我只使用gcc构建了一个C编译器，而不是构建完整的套件。 您不需要担心旧的gcc构建，因此您可以删除该内容，然后在build-gcc目录中重新启动。配置与前面的示例一样。注意，因为您正在构建一个将在这个平台上执行的工具，所以您将回到使用宿主GCC编译器，而不是之前构建的引导GCC编译器:12345$ cd build-gcc$ rm -rf *$ ../gcc-3.3.2/configure --enable-languages=c --target=$TARGET --prefix=$PREFIX$ make all$ make install 因为您正在为目标平台构建完整的newlib和gcc组件，如果不在distcc主机列表中的其他机器上安装这些组件，就不可能使用distcc。即使使用distcc，也需要一些时间，即使是在高速机器上。如果可能的话，我倾向于让这些构建在夜间运行——部分原因是时间，但部分原因是它增加了构建机器(或多个机器)的负载，如果您的机器同时用于其他目的，这可能会很烦人。 不过，当构建完成后，就可以开始使用交叉编译器来构建目标平台所需的其他应用程序和库了。 Section 5. 安装并使用交叉编译器使用你的交叉编译器实际上使用交叉编译器很容易。要编译一个简单的C文件，只需直接调用交叉编译器：1$ i386-linux-gcc myapp.c -o myapp GCC交叉编译器的工作原理与本地版本一样:它只是为另一个平台创建了不同类型的可执行文件。这意味着您可以使用相同的命令行选项,如头和库位置、优化和调试。(请记住,您不能将不同目标平台的库连接起来,并期望它们能够工作。)在下一个面板中,我将向您展示如何使用新的跨编译器构建库和扩展。 对于具有Makefile的应用程序和项目，请在要生成的命令行上指定交叉编译器。例如：1$ make CC=i386-linux-gcc 或者，更改Makefile中的CC定义。 编译其他工具和库您可能希望在目标平台上使用的任何库都需要使用交叉编译器进行编译，以便它们能够工作。对于大多数库，应用与构建自己的项目相同的基本规则。如果一个系统使用一个简单的Makefile,使用: 1$ make CC=i386-linux-gcc 或者，如果使用configure脚本，在configure命令前面加上重新定义CC环境变量的前缀：1CC=i386-linux-gcc ./configure 使用gnu风格的配置脚本，您可能还需要指定主机：1$ ./configure --host=i386-linux 记住，对于库，您需要指定目标前缀，就像您在构建glibc或newlib时所做的那样。]]></content>
      <categories>
        <category>嵌入式</category>
        <category>编译</category>
      </categories>
      <tags>
        <tag>译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201907]]></title>
    <url>%2F2019%2F07%2F04%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201907%2F</url>
    <content type="text"><![CDATA[技术 PoEPoE(Power over Ethernet，有源以太网)，利用以太网传输数据的同时，为设备供电。IEEE 802.3af标准规定了该技术的各项指标 autojs一个利用Android系统无障碍开发服务的框架，可以在不root的情况下实现录制屏幕、触发按键、启动APP、捕获UI、弹出对话框、监听事件、多线程、定时器等功能 frp一个快速的反向代理，可以让你本地的服务穿过NAT或者防火墙，暴露到外网。支持TCP、UDP，为HTTP、HTTPS提供了额外的支持 tracking.js一个在浏览器环境实现计算机视觉算法的js库，可实现颜色跟踪、人脸检测等 iframe劫持分析网页iframe劫持原理，并介绍如何避免 树莓派4 benchmarktom’s HEADWARE 发布了一篇详细分析树莓派4板子性能的文章，包括了CPU运算能力、内存读写能力、文件压缩能力、GPU能力、存储读写能力、网络吞吐量、功耗和温度等，一方面可以了解到树莓派的性能到底处于什么水平，另一方面也可以学习一下性能测试有哪些关注点、测试方法有哪些 工具 tableconvert一个在线将表格转换为Markdown、JSON、Latex等格式的页面 Nullboard一个开源的本地看板工具 新闻 阿里巴巴旗下[平头哥]发布首枚芯片:玄铁910平头哥是阿里巴巴旗下独立的芯片公司，于2018年9月成立，整合了中天微系统有限公司和达摩院。玄铁910采用高性能RISC-V(开源)架构，采用 12nm制程，主频2.5GHz，7.1Coremark/MHZ。该颗芯片适用于用在5G、网络通讯、人工智能、自动将驾驶领域，可嵌入CPU、SOC芯片中。 华为年薪百万的应届博士到底有多厉害关注一下最新的前沿研究方向 钟钊(博士)年薪制方案：182-201万人民币/年学校：中国科学院大学2014级硕士生、2016级博士生研究方向：模式识别与智能系统 秦通(博士)年薪制方案，182-201万人民币/年学校：香港科技大学机器人研究所四年级博士生研究方向：机器视觉SLAM，视觉惯导融合，多传感器定位 李屹(博士)年薪制方案，140.5-156.5万人民币/年学校：北大数学学院，硕博连读生代表文章：《R-FCN:基于区域的全卷积网络进行的目标检测》《Coordination and Composition: From Reo to Mediator》 管高扬(博士)年薪制方案：140.5-156.5万人民币/年学校：浙江大学研发中心研究方向：物联网和边缘计算 贾许亚(博士)年薪制方案：89.6-100.8万入民币/年学校：清华大学科学与技术代表文章：《Intelligent path control for energy-saving in hybrid SDN networks》 王承珂(博士)年薪制方案：89.6-100.8万人民币/年学校：北京大学信息与技术学院，本科直博研究方向：功耗控制 林晗(博士)年薪制方案，89.6-100.8万人民币/年学校：中国科技大学计算机科学与技术学院研究方向：大数据 何睿(博士)年薪制方案：89.6-100.8万人民币/年学校：中国科学院，数学与系统科学研究院研究方向：计算数学 科学家成功在DNA上运行SQL 原论文 PDF使用DNA存储数据乍一听很奇怪，实际上大有意义。现在研究人员已取得了重大突破，他们因而能够将DNA存储整合到PostgreSQL这种流行的开源数据库中 为什么要使用DNA作为存储介质？信息、数据的产生速度越来越快，而传统存储系统的存储能力和更新速率已经远远落后了，需要一种存储能力更强大、成本更低的介质 DNA作为存储介质的特性DNA是一种极其密集的三维存储介质，具有在1克中存储455艾字节的理论能力，是现代硬盘存储密度的300亿倍。保存时间长，在常温,无水,无氧的条件下, 可以保存几个世纪；而磁盘和磁带的寿命只有5年和30年。复制起来容易, 快速和便宜 面临的问题成本问题，主要是完整基因组测序工作成本很高。读写速度比不上传统硬盘，只有4M每秒。准确率问题，科学家声称的DNA存储技术的数据准确率大约在99%，但是这样会对生物体基因带来不可估量的问题]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[make编译]]></title>
    <url>%2F2019%2F06%2F27%2Fmake%E7%BC%96%E8%AF%91%2F</url>
    <content type="text"><![CDATA[make命令的一些解释make -j 并行编译在多核CPU上利用”-j”参数并行编译，以加快编译速度 1make -j4 /* 4个线程同时执行 */ -Werror编译过程中出现告警视为错误，立刻停止编译 输出调试$(warning)利用$(warning)内置函数可以调试Makefile中的变量 12OBJS = main.o test.o debug.o$(warning $(OBJS)) @echo如果在Makefile中打印一些提示过程字符串，需要使用echo，但是如果直接用echo，执行过程中会把这一行命令也打印出来 1234567%.o:%.c: echo "compile start" ......# 实际会打印# echo "compile start"# compile start 命令前加”@”可以只输出命令执行结果，不输出命令本身 123456%.o:%.c: @echo "compile start" ......# 实际会打印# compile start 输出编译辅助信息sizegcc编译工具”arm-linux-eabi-size”可以输出统计所有目标文件的占用大小 12345$(OBJSIZE) $(OBJS) &gt;&gt; test.sizecat libtensorflow-microlite.size text data bss dec hex filename......]]></content>
      <categories>
        <category>嵌入式</category>
        <category>编译</category>
      </categories>
      <tags>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Elasticsearch条件query]]></title>
    <url>%2F2019%2F06%2F25%2FElasticsearch%E6%9D%A1%E4%BB%B6query%2F</url>
    <content type="text"><![CDATA[按照时间升序降序query1234567891011121314151617181920212223242526def query_byid(self, id, size, reverse_order=False): if reverse_order == True: _order = 'desc' else: _order = 'asc' dsl = &#123; "query": &#123; "bool":&#123; "must": [ &#123;"match":&#123;"id":id&#125;&#125;, ], &#125; &#125;, "sort": [ &#123; "service.data.time" : &#123;"order" : _order&#125;&#125;, "_score" ], "_source": [ "service.data", "deviceId" ], "size":size, &#125; raw_data = self.query(dsl) raw_list = raw_data["hits"]["hits"] return raw_list]]></content>
      <categories>
        <category>数据库</category>
        <category>Elasticsearch</category>
      </categories>
      <tags>
        <tag>Elasticsearch</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[好点子.md]]></title>
    <url>%2F2019%2F06%2F25%2F%E5%A5%BD%E7%82%B9%E5%AD%90%2F</url>
    <content type="text"><![CDATA[好点子记录一些自己想到的、或者与别人聊天发现的好玩的创意点，将来可以做着玩玩 移动的Wi-Fi放大器利用一些可移动的智能硬件，例如小车、扫地机器人，将Wi-Fi放大器放入其中，对其移动逻辑进行自定义编程，实现人在哪里，移动Wi-Fi放大器就在哪里该放大器作为家庭主Wi-Fi的中继 定制化的家庭网关将路由器刷机，或者用开发板定制自己的网关逻辑，可实现以下功能 广告过滤：内嵌广告过滤算法，识别出广告数据包，并从视频流中剔除 远程监控：内置一个可供外网访问的服务，与手机APP交互，实现远程视频监控 自动下载：自动爬取电影或者电视剧资源列表，下载到移动硬盘，可提前创建预约任务 3D模型扫描生成工具最近新房装修，研究了几款平面设计的软件，这些软件大都免费安装，但是在设计布局的时候，需要很多的家具模板都需要付费，或者根本没有可在手机APP上开发一款使用摄像头扫描真实物体生成3D模型的软件 基于机器学习的喷子识别现在网络上到处都充斥着一群喷子，可以把混迹6年的知乎大佬喷自闭，能否开发一个基于机器学习的喷子识别程序，对某平台所有用户的言论进行分析识别，当其大部分言论中都充斥着对别人的否定、侮辱、谩骂词汇，或者语义时，认为该用户是一个喷子 基于机器学习的垃圾分类采集多个已分类好的垃圾照片，利用神经网络进行学习，可在移动端制作垃圾分类助手APP实现随拍识别垃圾是哪一类 基于神经网络的摄影后期学习某类摄影作品的风格，并批量应用于自己拍摄的图像]]></content>
      <categories>
        <category>嵌入式</category>
        <category>玩</category>
      </categories>
      <tags>
        <tag>idea</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[每月见闻201906]]></title>
    <url>%2F2019%2F06%2F20%2F%E6%AF%8F%E6%9C%88%E8%A7%81%E9%97%BB201906%2F</url>
    <content type="text"><![CDATA[新闻 Facebook libraFacebook推出了libra，一个全球化稳定加密货币。它基于区块链技术，以真实的资产兑换为担保，希望建立一个全球数字化的金融系统。以下几个问题值得思考 区块链技术并不是一个新鲜的东西，这一概念从诞生到现在，为什么一直没有实际的发展，难以落地的原因是什么？ libra和比特币有什么不同？ libra如果做大，对全球的交易行为、金融生态有什么影响，对我们有什么影响？ Facebook为什么可以做出libra，而不是别的公司？ 树莓派第四代树莓派发布了第4代开发板，性能强劲，价格35$起，ARM Cortex-A72的CPU、4G DDR4 SDRAM、802.11 A/C Wi-Fi、蓝牙5.0、USB 3.0、4K 60帧硬解码 文章 Benchmarks Game常用编程语言（27种）的两两性能对比，发现哪种语言最快 视频 vim使用、配置及插件介绍 视频中对vim的常用用法做了比较详细的介绍，满满的都是干货。例如一些常用的设置、键位映射、块操作、动作录制、颜色方案、分屏操作、文件目录、Markdown支持等 Deep Work卡尔·纽波特写的《Deep Work》这本书中介绍了什么是深度工作，什么是浅度工作，以及如何进入和训练深度工作。他认为深度工作是一个高效的、高质量的、创造性的、能够提升学术和专业能力的过程。但是由于现代公司的组织结构，人们很难进入深度工作状态，也更倾向于看起来非常忙碌的浅度工作状态(一心多用、重复性的，不能创造什么价值，可替代性高)。深度工作中，人需要保持高度专注，更加关注自己的能力发挥、有意识地提高自己的效率、思考如何做到高质量，高质量工作的公式为：\begin{equation}High-Quality Work = Time * Intensity\end{equation}高质量的工作等于时间乘以专注程度 人物 Machine Learning MasteryJason Brownlee, PhD.(杰森·布朗利博士)创建的一个社区，帮助开发者学习机器学习相关内容，里面有很多方面的内容。杰森·布朗利本人在机器学习领誉造诣很深 Mariana学霸Mariana(玛丽安娜)是一位葡萄牙的学霸妹子，经常在youtube(Mariana’s Study Corner)上发一些和学习习惯、技巧相关的视频，除了看学霸是怎么学习的，顺便练练听力也不错]]></content>
      <categories>
        <category>每月见闻</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[一些链接]]></title>
    <url>%2F2019%2F05%2F29%2F%E4%B8%80%E4%BA%9B%E9%93%BE%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[嵌入式开发ARM Developer Document Python Pandas官方文档 pythontutor一个将python代码转化为可视化动画的网站 PywaveletsPython小波变换库官方网站，包括安装使用说明，以及API介绍 Machine Learning FloydHub Blog一个深度学习、AI、云GPU的博客，里面的文章很前沿 Jason Brownlee PhD一个机器学习博主，写了很多机器学习、数据分析相关文章 CS231n斯坦福大学机器学习公开课笔记 其他 Medium一个类似简书的阅读写作平台，支持start、follow功能，内容质量较高]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[杭州市地铁乘客流量预测]]></title>
    <url>%2F2019%2F05%2F28%2F%E6%9D%AD%E5%B7%9E%E5%B8%82%E5%9C%B0%E9%93%81%E4%B9%98%E5%AE%A2%E6%B5%81%E9%87%8F%E9%A2%84%E6%B5%8B%2F</url>
    <content type="text"><![CDATA[(待续) 天池竞赛城市计算AI挑战赛以“杭州市地铁乘客流量预测”为题，提供了杭州市2019年1月1日到25日的地铁刷卡流量数据，要求预测出未来时间的客流变化情况 赛题简介赛题提供了以下文件 Metro_train.zip，包含25天的刷卡数据，每天为一个单独的.csv文件 testA_record_2019-01-28.csv，测试集，用来测试模型性能 Metro_roadMap.csv，各地铁站点连接关系 刷卡数据格式为 列名 类型 说明 示例 time String 刷卡发生时间 2019-01-02 00:30:53 lineID String 地铁线路ID C stationID int 地铁站ID 15 deviceID int 刷卡设备ID 2992 status int 进出站状态，0表示出站，1表示进站 1 userID String 用户ID C21b87e232a083b3e7d6d45e2ff933e31 payType int 刷卡类型 0 Metro_roadMap.csv文件中是一个2维矩阵，首行和首列是地铁站ID，roadMap[i][j]为1表示stationID为i的地铁站和为j的地铁站相连，为0表示不相连 数据分析初步分析使用pandas对数据做一个初步分析 整体情况首先读取一个数据文件，查看结构 123456789&gt;&gt;&gt; data = pd.read_csv(path+'/record_2019-01-01.csv')&gt;&gt;&gt; data.head() time lineID stationID deviceID status userID payType0 2019-01-01 02:00:05 B 27 1354 0 D13f76f42c9a677c4add94d9e480fb5c5 31 2019-01-01 02:01:40 B 5 200 1 D9a337d37d9512184b8e3fd477934b293 32 2019-01-01 02:01:53 B 5 247 0 Dc9e179298617f40b782490c1f3e2346c 33 2019-01-01 02:02:38 B 5 235 0 D9a337d37d9512184b8e3fd477934b293 34 2019-01-01 02:03:42 B 23 1198 0 Dd1cde61886c23fdb7ef1fdb76c9b1234 3&gt;&gt;&gt; 可以看到，文件结构与描述一致，从左到右，列依次为time、lineID、stationID、deviceID、status、userID和payType 1234567891011121314&gt;&gt;&gt; data.info()&lt;class 'pandas.core.frame.DataFrame'&gt;RangeIndex: 2539592 entries, 0 to 2539591Data columns (total 7 columns):time objectlineID objectstationID int64deviceID int64status int64userID objectpayType int64dtypes: int64(4), object(3)memory usage: 106.6+ MB&gt;&gt;&gt; 除了首行，数据共2539592条，占用106MB内存 查看描述信息 1234567891011&gt;&gt;&gt; data.describe() stationID deviceID status payTypecount 2.539592e+06 2.539592e+06 2.539592e+06 2.539592e+06mean 2.885803e+01 1.395870e+03 5.002957e-01 1.432034e+00std 2.190865e+01 1.002121e+03 5.000000e-01 8.313080e-01min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+0025% 1.100000e+01 5.890000e+02 0.000000e+00 1.000000e+0050% 2.000000e+01 1.056000e+03 1.000000e+00 1.000000e+0075% 4.600000e+01 2.196000e+03 1.000000e+00 2.000000e+00max 8.000000e+01 3.638000e+03 1.000000e+00 3.000000e+00&gt;&gt;&gt; 由描述信息可看出 共有253万次刷卡数据 地铁站ID从0到80，共81条地铁线 刷卡设备ID从0到3638，共3639个刷卡设备 payType从0到3，共4种刷卡方式 基本统计查看地铁线路统计信息 12345678&gt;&gt;&gt; data.lineID.nunique()3&gt;&gt;&gt; data.lineID.value_counts()B 1677014C 646253A 216325Name: lineID, dtype: int64&gt;&gt;&gt; 共3条地铁线，分别为A、B、C，其中地铁线路B流量最大为167万，C次之为64万，A最小为21万 查看地铁站统计信息 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263&gt;&gt;&gt; data.stationID.value_counts()15 3570919 1674487 8573711 7576010 7291320 708248 6496324 6177422 5370225 5165233 5151412 483864 4581913 3867414 3624916 3350729 332272 3228951 3181946 2998238 2987078 2949657 2927156 269746 2674758 2632876 2491639 2471463 2449355 24465 ...61 1953527 1933962 1927943 1825250 1814470 1775141 1759648 1752834 1742232 1616667 1565645 1383766 1364471 1359073 1304136 127861 1247744 1234940 1051921 1048879 1032831 1003317 948164 917675 796428 793935 746672 722380 675274 4642&gt;&gt;&gt; 站点15的流量最大，站点9次之 用户统计分析 123456789101112&gt;&gt;&gt; user_counts = data.userID.value_counts()&gt;&gt;&gt; user_counts.shape(916414,)&gt;&gt;&gt; user_counts[user_counts&gt;10].shape(1092,)&gt;&gt;&gt; user_counts[user_counts&gt;5].shape(46683,)&gt;&gt;&gt; user_counts[user_counts==4].shape(241819,)&gt;&gt;&gt; user_counts[user_counts==2].shape(613793,)&gt;&gt;&gt; 由用户统计分析可看出 总的地铁出行用户数为91万人 只有4.6万人刷卡次数超过5次，1万人刷卡次数超过10次，60万人刷卡次数为2，24万人刷卡次数为4次 挖掘规律由以上基本分析可得出几个关键结论 从地铁线路来看，流量大小依次为B&gt;C&gt;A 从地铁站来看，15站点流量极高，9站点次之 从用户信息来看，大量用户刷卡次数在5次以下，其中66%的用户是单程 由以上结论，提出以下问题 为什么B线流量大？ 为什么15站和9站流量大？ 为什么单程用户流量大？单程的起始和终点集中在哪些线路和站点？往返用户占多少？ 为什么B线流量大？猜测B线流量大，可能是因为B线站点数量多，B线存在交通枢纽站点，或者B线上某些站点周边覆盖人口密度大 查看各线路站点数 1234567&gt;&gt;&gt; data[data.lineID=='A'].stationID.nunique()14&gt;&gt;&gt; data[data.lineID=='B'].stationID.nunique()34&gt;&gt;&gt; data[data.lineID=='C'].stationID.nunique()32&gt;&gt;&gt; 可看到B线站点数量为34最多，C线32站，A线最少，只有14站 查看各地铁线站点流量情况 12345678910111213141516171819202122232425262728293031&gt;&gt;&gt; data[data.lineID=='A'].stationID.value_counts().describe()count 14.000000mean 15451.785714std 7684.061288min 4642.00000025% 8555.00000050% 14623.00000075% 21201.000000max 29496.000000Name: stationID, dtype: float64&gt;&gt;&gt; data[data.lineID=='B'].stationID.value_counts().describe()count 34.000000mean 49323.941176std 62542.487342min 7939.00000025% 21330.25000050% 32758.00000075% 53189.500000max 357091.000000Name: stationID, dtype: float64&gt;&gt;&gt; data[data.lineID=='C'].stationID.value_counts().describe()count 32.000000mean 20195.406250std 6149.949336min 7466.00000025% 17501.50000050% 20390.50000075% 24472.000000max 31819.000000Name: stationID, dtype: float64&gt;&gt;&gt; 可看到地铁线A最大站点流量才2.9万，平均在1.5万；B线最大站点流量为35万，平均为4.9万；C线最大流量站点为3万，平均为2万；因此，B线大部分站点流量都较高，例如15站、9站等高流量站点，都属于B线，所以导致B线整体流量较大 为什么15站和9站流量大？仅仅从训练数据种，无法得出15站和9站的其他信息，站点流量大小很大程度上取决于其周边设施以及本站是否为换乘站 根据赛题给出的站点连接关系，总结出以下交汇点 站点1 站点2 28 20 51 9 50 10 51 10 52 10 74 5 75 5 80 15 77 46 78 46 根据站点交汇关系，以及各地铁线路的总站点数，再结合杭州市地铁图，可确定 B线为1号线、C线为2号线，A线为4号线 B线站点范围为0~33，其中0为湘湖，5为近江，10为凤起路，15为火车站，20为客运中心，27为下沙滨江，33为临平 C线站点范围为34~66(54不存在)，其中34为朝阳，46为钱江路，51为凤起路 A线站点范围为67~80，74为甬江路，75为城星路，77为江锦路，78为景芳 15站为火车站，由于刷卡数据为2019年1月份，临近春节，因此猜测15站流量大是因为春运流量9站为龙翔桥，经百度地图查看，该站点临近西湖，且周边为商业繁华地带，而读取的数据文件是1月1日元旦，因此流量大 为什么单程用户流量大？查看单车用户集中在哪些站点 1234567891011121314151617181920212223242526272829303132333435363738394041424344&gt;&gt;&gt; user_counts = data.userID.value_counts()&gt;&gt;&gt; oneway_users = user_counts[user_counts==2].index&gt;&gt;&gt; oneway_usersIndex([u'Da777d480eb319974721ba1395276b9b9', u'Dbb272e5468e0f8f3119affa3548cb2d8', u'Ba5b3c0b2fdcc02009044e1745f786a40', u'B5be311471b553fa8a1dae68ae2e98e25', u'B5c8c3ea1fe6fdb0f70cc264a16a43349', u'Dde0d323a4dd433975277717c57525cef', u'B55870174fd27626952cb0960a679ba6c', u'Ddf91d65e630c39ac4705d1a9789b1dbd', u'B3487dabebed8c675e2d72fcaea059ce7', u'B775172740ccc61a1856568a09baaa9dc', ... u'Be838ec43cc032ee398531caa5fd82bdb', u'B54815c3bbb7135547c0407d252d1562f', u'B703db1dc0f0a9600d7929754ca79e600', u'B8fa611fb11119fe7ee4a11d79cd3f6b3', u'Dc989f13b188864668cc0e7d9347d946c', u'B8f8d236356b772eb4f143796334d2981', u'Be37e7163697a8caacdb938546306eec0', u'A55804f3d7bf1b47eee1f26e77f7f9acb', u'D117ad973c40f4e4a154604f382923fb7', u'B3ddd6a228d019312e7ce53fdb4476301'], dtype='object', length=613793)&gt;&gt;&gt;&gt;&gt;&gt; data[data.userID.isin(oneway_users)].stationID.value_counts()15 2624849 607017 5087120 3977311 3738324 3520825 3086333 2776110 261018 2560812 2035522 2030213 202824 20269 ...Name: stationID, Length: 80, dtype: int64&gt;&gt;&gt; 单程用户613793人，那么总流量为613793*2=1227586次。单程流量集中在15、9、7、20、11、24、25，15为火车站，9为龙翔桥，7为客运站，20为客运中心，11为商圈和住宅中心，24、25是大学城，因此可判断单程流量主要是由于临近春运的各大车站出入人口，以及元旦假期的出入人口 可视化编写代码，以天为单位，绘制总流量图由上图可看出，总流量以周为单位，呈周期性变化，规律是周内工作日流量较高，周5到达峰值，周末降到峰谷(除了1月1日的元旦假期)总的趋势是无论周内周日，随着时间临近春节，整体流量有上升趋势 以时刻为单位，绘制3条地铁线的某天实时流量图，发现所有周内情况与下图类似整天的流量呈现双峰现象，分别集中在早8点和晚6点，是上下班流量 周末的流量与下图类似 1月1日元旦的流量图如下 周内分析查看数据摘要 1234567891011&gt;&gt;&gt; data = pd.read_csv(path+'record_2019-01-02.csv')&gt;&gt;&gt; data.describe() stationID deviceID status payTypecount 2.376462e+06 2.376462e+06 2.376462e+06 2.376462e+06mean 3.282597e+01 1.574050e+03 4.998182e-01 1.371589e+00std 2.340102e+01 1.069709e+03 5.000001e-01 8.458438e-01min 0.000000e+00 0.000000e+00 0.000000e+00 0.000000e+0025% 1.300000e+01 6.770000e+02 0.000000e+00 1.000000e+0050% 2.600000e+01 1.315000e+03 0.000000e+00 1.000000e+0075% 5.300000e+01 2.565000e+03 1.000000e+00 2.000000e+00max 8.000000e+01 3.638000e+03 1.000000e+00 3.000000e+00 可看到1月2日总流量为2376462 123456789&gt;&gt;&gt; user_counts = data.userID.value_counts()&gt;&gt;&gt; user_counts.shape(764188,)&gt;&gt;&gt; user_counts[user_counts&gt;4].shape(45376,)&gt;&gt;&gt; user_counts[user_counts==2].shape(394674,)&gt;&gt;&gt; user_counts[user_counts==4].shape(315658,) 总刷卡人数为74万，刷卡数主要集中在单程和往返 查看单程车站分布123456789101112&gt;&gt;&gt; user_counts = data.userID.value_counts()&gt;&gt;&gt; oneway_users = user_counts[user_counts==2].index&gt;&gt;&gt; data[data.userID.isin(oneway_users)].stationID.value_counts()15 1323027 325339 272784 2053020 2042911 1918433 1702313 14934 ... 可以看到，单程流量较高的车站有 15：杭州东站 7：城站 9：龙翔桥 4：江陵路 20：客运中心 11：武林广场 33：临平15、7和20客流量大是因为虽然是周内，依然是春运高峰期，或者元旦假期结尾的返程客流；9龙翔桥站靠近西湖景区，且周边有很多商业热点，猜测可能是经常性大流量；其余站点流量大还需要分析原因 查看杭州东站的人流方向首先查看从杭州东站进的人主要去往哪些站点 12345678910&gt;&gt;&gt; s15_in_users = data[data.stationID==15][data.status==1].userID.values&gt;&gt;&gt; s15_out_users = data[data.stationID==15][data.status==0].userID.values&gt;&gt;&gt; &gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s15_in_users)) &amp; (data.stationID!=15)].stationID.value_counts()[0:5]11 370524 359425 34537 33229 3303Name: stationID, dtype: int64&gt;&gt;&gt; 可看到，从15站进入的人，主要去往了11、24、25、7、9这几个站点，24、25是大学城区域，猜测是元旦假期返程人流；7是靠近杭州火车站，猜测是从东站换乘的春运人流；9站更加可以说明是从火车站来西湖游玩的日常流量；7站比9站里西湖稍远，但是也属于靠近西湖的站点，且周边住宅、商业热点众多，因此猜测也是和到西湖游玩有关 查看去往杭州东站的人主要来自哪些站点 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s15_out_users)) &amp; (data.stationID!=15)].stationID.value_counts()[0:5]9 34767 244211 229620 191033 1864Name: stationID, dtype: int64&gt;&gt;&gt; 可看到，从15站出站乘坐高铁的人，主要来自9、7、11、20、33几个站点 9：西湖游玩返程的人 7：春运换乘上高铁 11：西湖游玩返程的人 20：春运换乘上高铁 33：此站点远离市区，猜测可能房价偏低，人口密度大，也是属于春运客流 查看城站的人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s7_in_users)) &amp; (data.stationID!=7)].stationID.value_counts()[0:5]15 24424 10372 72133 6010 572Name: stationID, dtype: int64&gt;&gt;&gt; 可看到从城站进地铁的人流主要去往了15和4站点，15站是因为春运换乘，4站点是因为居住区 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s7_out_users)) &amp; (data.stationID!=7)].stationID.value_counts()[0:5]15 33220 7404 7089 64733 625Name: stationID, dtype: int64&gt;&gt;&gt; 可看到去往城站的人流，主要来自15站，因为春运换乘 查看龙翔桥的人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s9_in_users)) &amp; (data.stationID!=9)].stationID.value_counts()[0:5]15 34767 64714 46613 45333 442Name: stationID, dtype: int64&gt;&gt;&gt; 从龙翔桥出发的人流主要去往了15站，是从西湖游玩返程的 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s9_out_users)) &amp; (data.stationID!=9)].stationID.value_counts()[0:5]15 33037 56414 53233 51813 502Name: stationID, dtype: int64&gt;&gt;&gt; 去往龙翔桥的人流主要来自15站，是去西湖游玩的 查看江陵路人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s4_in_users)) &amp; (data.stationID!=4)].stationID.value_counts()[0:5]15 14727 7082 4949 4263 341Name: stationID, dtype: int64&gt;&gt;&gt; 江陵路人流主要去往15和7站点，是春运出发 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s4_out_users)) &amp; (data.stationID!=4)].stationID.value_counts()[0:5]15 22877 10372 4369 3950 393Name: stationID, dtype: int64&gt;&gt;&gt; 江陵路人流主要来自15和7站点，是春运返程 查看客运中心人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s20_in_users)) &amp; (data.stationID!=20)].stationID.value_counts()[0:5]15 191025 5177 48133 47024 464Name: stationID, dtype: int64&gt;&gt;&gt; 客运中心人流主要去往15站点，春运 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s20_out_users)) &amp; (data.stationID!=20)].stationID.value_counts()[0:5]15 194833 4849 3907 36129 302Name: stationID, dtype: int64&gt;&gt;&gt; 客运中心人流主要来自15站点，春运 查看武林广场人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s11_in_users)) &amp; (data.stationID!=11)].stationID.value_counts()[0:5]15 22967 4574 3879 38214 323Name: stationID, dtype: int64&gt;&gt;&gt; 武林广场人流主要来自15站点，春运和去西湖游玩 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s11_out_users)) &amp; (data.stationID!=11)].stationID.value_counts()[0:5]15 37057 51120 4569 39133 354Name: stationID, dtype: int64&gt;&gt;&gt; 武林广场人流主要去往15、7站点，春运和西湖游玩返程 查看临平人流方向12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s33_in_users)) &amp; (data.stationID!=33)].stationID.value_counts()[0:5]15 18647 6259 51820 48431 392Name: stationID, dtype: int64&gt;&gt;&gt; 临平人流主要来自15站点，春运 12345678&gt;&gt;&gt; data[(data.userID.isin(oneway_users)) &amp; (data.userID.isin(s33_out_users)) &amp; (data.stationID!=33)].stationID.value_counts()[0:5]15 18957 60120 4709 44231 309Name: stationID, dtype: int64&gt;&gt;&gt; 临平人流主要去往15站点，春运 查看返程车站分布12345678910111213&gt;&gt;&gt; data[data.userID.isin(twiceway_users)].stationID.value_counts()[0:10]4 4569115 448269 4275612 291877 2800116 2782510 2603311 2588222 254705 25369Name: stationID, dtype: int64&gt;&gt;&gt; 返程站点流量前10位为4、15、9、12、7、16、10、11、22、5，查看这些站点附近热点 4，市公安局、滨江区政府、武警医院、江锦汽车有限公司、中财、中化、海康威视、吉利集团、百得利 9，市第一人民医院、浙大附属产科医院、银泰、天长小学、思鑫坊、胜利剧院、国贸中心、浙江省中医院、惠星中学 12，环球中心、科技馆、清园小区、通盛嘉苑、武林府、中山北园、西子花园、河东社区 7，建国中路小区、长明寺巷社区、第二中学、葵巷社区、新东方、浙大附属第一医院、银联、国贸、国税局、中闽大厦、国家电网、第六中学、三益里小区、第三人民医院 16，宇威德信、三花国际、港龙城、新和嘉苑、德信东望、明月嘉苑、明月嘉苑、夏衍小学 10，换乘站、皇亲苑社区、皇后公园、第十四中学、长寿社区、锦绣天地、杭州嘉里中心、建德路小区、凤麟社区、镜瑞弄、国都公寓、麒麟公寓、儿童医院、竹竿巷社区 11，国大城市广场、杭州百货大楼、浙信大厦、国信大厦、元通大厦、广发大厦、下城区人民医院、南都天水苑、仓桥社区、天水阳光家园、长江实验小学、天巢花苑、杭州大厦购物城、文化会堂浙江展览馆、浙江展览馆南广场、武林广场、电信大楼、中国电信、天水小学 22，经济技术开发区、龙湖滟澜山、龙湖时代金沙天街、德信中外公寓、名城湖左岸、新元社区、金沙城、金沙湖、和达御观邸、上沙锦湖家园、金沙湖公园 5，换乘站、杭州市公安局、望江公园、杭州市建兰中学分校、近江东园社区、近江家园、天福花园、滨江新苑、崇文实验学校、胜利小学、开元中学、妇产科医院、万泰城、文华苑、林风花园 123456789101112&gt;&gt;&gt; user_counts = data.userID.value_counts()&gt;&gt;&gt; twiceway_users = user_counts[user_counts==4].index&gt;&gt;&gt; data[data.userID.isin(twiceway_users)].stationID.value_counts()15 1323027 325339 272784 2053020 2042911 1918433 1702313 14934 ... s13_in_users = data[data.stationID==13][data.status==1].userID.valuess13_out_users = data[data.stationID==13][data.status==0].userID.values 模型选择和评估预测结果]]></content>
      <categories>
        <category>ML</category>
        <category>project</category>
      </categories>
      <tags>
        <tag>ML</tag>
        <tag>Python</tag>
        <tag>天池竞赛</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[panda基本操作]]></title>
    <url>%2F2019%2F05%2F27%2Fpanda%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[DataFrame基本操作删除删除行或列使用的函数都是drop 删除列1df.drop('col_name', axis=1) col_name 是列名标识 axis用来标识删除的是行还是列 删除行1df.drop([row_nr1, row_nr2, ...]) 要删除的行号以list形式传入 时间序列数据处理将字典列表转化为DataFrame12345678import pandas as pdx = [&#123;'a':1, 'b':2, 'c':'2000-01-01'&#125;, &#123;'a':2, 'b':3, 'c':'2000-01-02'&#125;, &#123;'a':3, 'b':4, 'c':'2000-01-03'&#125;, &#123;'a':4, 'b':5, 'c':'2000-01-04'&#125;, &#123;'a':5, 'b':6, 'c':'2000-01-05'&#125;, &#123;'a':6, 'b':7, 'c':'2000-01-06'&#125;,]df = pd.DataFrame(x, columns=['a', 'b', 'c']) 查看数据12345678&gt;&gt;&gt; df a b c0 1 2 2000-01-011 2 3 2000-01-022 3 4 2000-01-033 4 5 2000-01-044 5 6 2000-01-055 6 7 2000-01-06 修改索引为日期1df.set_index('c') 输出123456789&gt;&gt;&gt; df.set_index('c') a bc2000-01-01 1 22000-01-02 2 32000-01-03 3 42000-01-04 4 52000-01-05 5 62000-01-06 6 7 DataFrame转化为csv文件]]></content>
      <categories>
        <category>Program Language</category>
        <category>Python</category>
        <category>Pandas</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AES-CBC]]></title>
    <url>%2F2019%2F05%2F23%2FAES-CBC%2F</url>
    <content type="text"><![CDATA[开源项目mbedtls是一个专用于嵌入式arm平台的加解密方案，包含很多算法 github仓库地址https://github.com/ARMmbed/mbedtls官网地址https://tls.mbed.org/ AES算法原理AES(Advanced Encryption Standard，高级加密标准)算法属于对称加密算法，几个关键概念 分组密码体制 Padding 密钥 初始向量 加密模式 分组密码体制首先把明文拆分成一个个小段，对每个小段单独加密，最后再把每个加过密的小段拼起来，就是密文。拆分小段需要遵循字节对齐，每个小段长度必须为128bit(16字节)、196bit(24字节)或者256bit(32字节) Padding把明文末尾不满足字节对齐的最后一小段进行填充，有PKCS5、PKCS7和NOPADDING三种方式。PKCS5指分组缺少几个字节，就在数据末尾填充几个几，例如缺少3个字节，就在末尾填充3个3；PKCS7是指分组数据缺少几个字节，就在数据的末尾填充几个字节的0；NoPadding是指不需要填充，认为数据是填充好的 密钥密钥长度与分组长度一致，位数越高，安全性越好，但是效率越低 初始向量初始向量可以使得加密更加安全，需要通信双方固定好，初始向量在分组加密过程中重复被利用，初始向量首先和第一个分组进行计算，结果将作为下一个分组的初始向量。其长度规定为16字节 加密模式加密模式有5种 电码本模式(Electronic Codebook Boo, ECB) 密码分组链接模式(Cipher Block Chaining, CBC) 计算器模式(Counter, CTR) 密码反馈模式(Cipher FeedBack, CFB) 输出反馈模式(Output FeedBack, OFB) 加密流程加密的核心步骤是4个操作：密钥扩展、初始轮、重复轮和最终轮 密钥扩展密钥扩展是指根据初始密钥生成后面10轮密钥的操作；AES通过一个简单地快速混合，生成10轮密钥，加上原本的密钥，一共是11轮密钥，每一轮都是依据前一轮生成的，其4个步骤为：排列、置换、与轮常量异或 初始轮将第一个分组放在一个4*4的矩阵种，称为状态矩阵将状态矩阵和初始密钥进行亦或操作 重复轮把字节混淆、行移位、列混乱、加轮密钥这四个操作重复执行。重复轮重复的轮数取决于密钥的长度，128位16字节的密钥重复轮推荐重复执行9次，192位密钥重复轮推荐重复执行11次，256位密钥重复轮推荐重复执行13次重复轮每轮重复的操作包括：字节混淆、行移位、列混乱、加轮密钥 字节混淆把初始轮得到的状态矩阵经过一个置换盒，输出一个新的矩阵，叫它为字节混淆矩阵 行移位对字节混淆矩阵进行行移位，然后再按照图中的方式重新放一下字节，这样行移位就算完成，得到的新矩阵，称之为行移位矩阵 列混乱用模不可约多项式将每列混乱，得到一个新的矩阵，称之为列混乱矩阵 加轮密钥在每一轮结束的时候，把列混乱矩阵和下一轮的密钥做一下异或操作，得到一个新的矩阵，称之为加轮秘钥矩阵 最终轮最终轮的操作和重复轮类似，只是没有列混乱的操作， 总结每执行一次AES加密，其实内部一共进行了11轮加密，包括1个初始轮，9个拥有4个操作的重复轮，1个拥有3个操作的最终轮，才算得到密文解密意味着加密的逆过程，只需要把加密的每个步骤倒着顺序执行就能完成解密了 注意点 双方必须使用一样的密钥和初始向量IV 双方必须使用一样的加密模式 双方必须使用一样的Padding模式 实现分别以C语言和Java编写测试代码，测试字符串和字节序列的加密解密 C语言mbedtls项目已实现，只需要做一些额外的封装 aes_string_padding()函数用于对字符串进行填充，参数s是输入字符串，c是填充内容，align是对齐长度，rlen为返回的填充后长度，函数返回值为一个新的字符串123456789101112131415161718192021222324252627uint8_t *aes_string_padding(uint8_t *s, char c, int align, int *rlen)&#123; int i; uint8_t *p; uint8_t *new_s; size_t len; size_t align_len; if ((!s) || (align &lt;= 0)) return NULL; len = strlen(s); if (len==align) align_len = align; else align_len = ((len/align)+1)*align; new_s = mem_alloc(align_len); p = new_s; memcpy(p, s, len); p += len; for (i=0; i&lt;align_len-len; i++) &#123; buf_push_byte(c, p); &#125; *rlen = align_len; return new_s;&#125; 函数aes_byte_padding()用于对字节序列进行填充1234567891011121314151617181920212223uint8_t *aes_byte_padding(uint8_t *buf, int buf_len, uint8_t c, int align, int *rlen)&#123; int i; uint8_t *p; uint8_t *new_buf; size_t align_len; if ((!buf) || (align &lt;= 0)) return NULL; if (buf_len==align) align_len = align; else align_len = ((buf_len/align)+1)*align; new_buf = mem_alloc(align_len); p = new_buf; memcpy(p, buf, buf_len); p += buf_len; for (i=0; i&lt;align_len-buf_len; i++) &#123; buf_push_byte(c, p); &#125; *rlen = align_len; return new_buf;&#125; 函数aes_cbc_enc_string()是字符串加密函数，参数： key，密钥 inv，初始向量 key_len，固定密钥长度，用于填充密钥和明文 str，字符串形式的明文 out，返回的密文 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889err_type aes_cbc_enc_string(uint8_t *key, uint8_t *inv, int key_len, uint8_t *str, uint8_t **out)&#123; int len; int safe_key_len; int safe_inv_len; int safe_str_len; uint8_t *safe_key; uint8_t *safe_inv; uint8_t *safe_str; uint8_t *safe_enc; mbedtls_aes_context ctx; //加密结构 if (!key || !inv || !str || key_len&lt;=0) return et_aes_enc; len = strlen(key); if (len &gt; key_len) return et_aes_enc; len = strlen(inv); if (len &gt; key_len) return et_aes_enc; log_debug("aes_cbc_enc_string in"); // 密钥可能不满足字节对齐，填充key safe_key = aes_string_padding(key, ' ', key_len, &amp;safe_key_len); if (!safe_key) goto err; // 初始向量可能不满足字节对齐，填充key safe_inv = aes_string_padding(inv, ' ', key_len, &amp;safe_inv_len); if (!safe_inv) goto err; // 明文可能不满足字节对齐，填充key safe_str = aes_string_padding(str, ' ', key_len, &amp;safe_str_len); if (!safe_str) goto err; mbedtls_aes_init(&amp;ctx); safe_enc = mem_alloc_z(safe_str_len); printf("key:%s, len:%d\n", key, strlen(key)); buf_dump(key, strlen(key)); printf("\n"); printf("safe_key:%s, len:%d\n", safe_key, strlen(safe_key)); buf_dump(safe_key, safe_key_len); printf("\n"); printf("inv:%s, len:%d\n", inv, strlen(inv)); buf_dump(inv, strlen(inv)); printf("\n"); printf("safe_inv:%s, len:%d\n", safe_inv, strlen(safe_inv)); buf_dump(safe_inv, safe_inv_len); printf("\n"); printf("plain:%s, len:%d\n", str, strlen(str)); buf_dump(str, strlen(str)); printf("\n"); printf("safe plain:%s, len:%d\n", safe_str, strlen(safe_str)); buf_dump(safe_str, safe_str_len); printf("\n"); mbedtls_aes_setkey_enc(&amp;ctx, safe_key, 128); // 设置密钥 mbedtls_aes_crypt_cbc(&amp;ctx, MBEDTLS_AES_ENCRYPT, safe_str_len, // 加密函数 safe_inv, safe_str, safe_enc); // 打印密文内容 printf("cipher:%s, len:%d\n", safe_enc, strlen(safe_enc)); printf("0x%2x, cipher len %d\n", safe_enc[15], safe_str_len); buf_dump(safe_enc, safe_str_len); printf("\n"); if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_str) mem_free(safe_str); *out = safe_enc; log_debug("aes_cbc_enc_string ok"); return et_ok;err: if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_str) mem_free(safe_str); if (safe_enc) mem_free(safe_enc); return et_aes_enc;&#125; 函数aes_cbc_dec_string()用于字符串解密，参数 key，密钥 inv，初始向量 key_len，规定的密钥长度 str，密文 out，解密后的明文12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758err_type aes_cbc_dec_string(uint8_t *key, uint8_t *inv, int key_len, uint8_t *str, uint8_t **out)&#123; int len; int safe_key_len; int safe_inv_len; int safe_str_len; uint8_t *safe_key; uint8_t *safe_inv; uint8_t *safe_dec; mbedtls_aes_context ctx; if (!key || !inv || !str || key_len&lt;=0) return et_aes_enc; len = strlen(key); if (len &gt; key_len) return et_aes_enc; len = strlen(inv); if (len &gt; key_len) return et_aes_enc; log_debug("aes_cbc_dec_string in"); safe_key = aes_string_padding(key, ' ', key_len, &amp;safe_key_len); if (!safe_key) goto err; safe_inv = aes_string_padding(inv, ' ', key_len, &amp;safe_inv_len); if (!safe_inv) goto err; mbedtls_aes_init(&amp;ctx); safe_dec = mem_alloc_z(strlen(str)); printf("cipher:%s, len:%d\n", str, strlen(str)); buf_dump(str, strlen(str)); printf("\n"); mbedtls_aes_setkey_enc(&amp;ctx, safe_key, 128); mbedtls_aes_crypt_cbc(&amp;ctx, MBEDTLS_AES_DECRYPT, strlen(str), safe_inv, str, safe_dec); printf("plain:%s, len:%d\n", safe_dec, strlen(safe_dec)); buf_dump(safe_dec, strlen(safe_dec)); printf("\n"); if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); *out = safe_dec; log_debug("aes_cbc_dec_string ok"); return et_ok;err: if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_dec) mem_free(safe_dec); return et_aes_enc;&#125; 函数aes_cbc_enc_byte()用于字节序列加密，参数 key，密钥 key_len，密钥长度 inv，初始向量 inv_len，初始向量长度 align，规定字节对齐长度 buf，明文 buf_len，明文长度 out，输出密文 out_len，输出密文长度1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283err_type aes_cbc_enc_byte(uint8_t *key, int key_len, uint8_t *inv, int inv_len, int align, uint8_t *buf, int buf_len, uint8_t **out, int *out_len)&#123; int len; int safe_key_len; int safe_inv_len; int safe_buf_len; uint8_t *safe_key; uint8_t *safe_inv; uint8_t *safe_buf; uint8_t *safe_out; mbedtls_aes_context ctx; if (!key || !inv || !buf || key_len&lt;=0) return et_aes_enc; if (key_len &gt; align) return et_aes_enc; if (inv_len &gt; align) return et_aes_enc; log_debug("aes_cbc_enc_byte in"); mbedtls_aes_init(&amp;ctx); safe_key = aes_byte_padding(key, key_len, 0x0, align, &amp;safe_key_len); if (!safe_key) goto err; safe_inv = aes_byte_padding(inv, inv_len, 0x0, align, &amp;safe_inv_len); if (!safe_inv) goto err; safe_buf = aes_byte_padding(buf, buf_len, 0x0, align, &amp;safe_buf_len); if (!safe_buf) goto err; safe_out = mem_alloc_z(safe_buf_len); printf("key len %d, byte:\n", key_len); buf_dump(key, key_len); printf("\n"); printf("safe key len %d, byte:\n", safe_key_len); buf_dump(safe_key, safe_key_len); printf("\n"); printf("inv len %d, byte:\n", inv_len); buf_dump(inv, inv_len); printf("\n"); printf("safe inv len %d, byte:\n", safe_inv_len); buf_dump(safe_inv, safe_inv_len); printf("\n"); printf("plain len %d, byte:\n", buf_len); buf_dump(buf, buf_len); printf("\n"); printf("safe plain len %d, byte:\n", safe_buf_len); buf_dump(safe_buf, safe_buf_len); printf("\n"); mbedtls_aes_setkey_enc(&amp;ctx, safe_key, 128); mbedtls_aes_crypt_cbc(&amp;ctx, MBEDTLS_AES_ENCRYPT, safe_buf_len, safe_inv, safe_buf, safe_out); printf("cipher len %d, byte:\n", safe_buf_len); buf_dump(safe_out, safe_buf_len); printf("\n"); if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_buf) mem_free(safe_buf); *out = safe_out; *out_len = safe_buf_len; log_debug("aes_cbc_enc_byte ok"); printf("\n"); return et_ok;err: if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_buf) mem_free(safe_buf); return et_ok;&#125; 函数aes_cbc_dec_byte()用于字节序列解密123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960err_type aes_cbc_dec_byte(uint8_t *key, int key_len, uint8_t *inv, int inv_len, int align, uint8_t *buf, int buf_len, uint8_t **out, int *out_len)&#123; int len; int safe_key_len; int safe_inv_len; uint8_t *safe_key; uint8_t *safe_inv; uint8_t *safe_out; mbedtls_aes_context ctx; if (!key || !inv || !buf || key_len&lt;=0) return et_aes_enc; if (key_len &gt; align) return et_aes_enc; if (inv_len &gt; align) return et_aes_enc; log_debug("aes_cbc_dec_byte in"); mbedtls_aes_init(&amp;ctx); safe_key = aes_byte_padding(key, key_len, 0x0, align, &amp;safe_key_len); if (!safe_key) goto err; safe_inv = aes_byte_padding(inv, inv_len, 0x0, align, &amp;safe_inv_len); if (!safe_inv) goto err; safe_out = mem_alloc_z(buf_len); printf("cipher len %d, byte:\n", buf_len); buf_dump(buf, buf_len); printf("\n"); mbedtls_aes_setkey_enc(&amp;ctx, safe_key, 128); mbedtls_aes_crypt_cbc(&amp;ctx, MBEDTLS_AES_DECRYPT, buf_len, safe_inv, buf, safe_out); printf("plain len %d, byte:\n", buf_len); buf_dump(safe_out, buf_len); printf("\n"); if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); *out = safe_out; *out_len = buf_len; log_debug("aes_cbc_dec_byte ok"); return et_ok;err: if (safe_key) mem_free(safe_key); if (safe_inv) mem_free(safe_inv); if (safe_out) mem_free(safe_out); return et_aes_enc;&#125; 以明文”hello world”进行加密解密测试123456789101112131415161718int main(int argc, char *argv[])&#123; int enc_len; uint8_t *enc, *dec; log_level(LOG_DBG); log_mode(LOG_TEST); uint8_t *str_key = "xadcdgde"; uint8_t *str_inv = "1234567890123456"; aes_cbc_enc_string(str_key, str_inv, KEY_LEN, str_plain, &amp;enc); aes_cbc_dec_string(str_key, str_inv, KEY_LEN, enc, &amp;dec); mem_free(enc); mem_free(dec); return 0;&#125; 编译运行程序，打印如下1234567891011121314151617181920212223242526272829303132333435363738[root@localhost aes_test]# ./bin/aes_test [2019/05/24 00:50:19] [DEBUG] [aes_cbc_enc_string:711 ] aes_cbc_enc_string inkey:xadcdgde, len:878 61 64 63 64 67 64 65safe_key:xadcdgde , len:16 # 密钥长度被填充到1678 61 64 63 64 67 64 65 20 20 20 20 20 20 20 20inv:1234567890123456, len:1631 32 33 34 35 36 37 38 39 30 31 32 33 34 35 36safe_inv:1234567890123456, len:16 # 初始向量不变31 32 33 34 35 36 37 38 39 30 31 32 33 34 35 36plain:hello world, len:11 68 65 6c 6c 6f 20 77 6f 72 6c 64safe plain:hello world , len:16 # 明文长度被填充到1668 65 6c 6c 6f 20 77 6f 72 6c 64 20 20 20 20 20cipher:'flen:160x4a, cipher len 16de 1d f9 6b 27 82 19 2f 79 88 5f 8b 95 66 be 4a # 密文字节 [2019/05/24 00:50:19] [DEBUG] [aes_cbc_enc_string:763 ] aes_cbc_enc_string ok [2019/05/24 00:50:19] [DEBUG] [aes_cbc_dec_string:797 ] aes_cbc_dec_string incipher:'flen:16de 1d f9 6b 27 82 19 2f 79 88 5f 8b 95 66 be 4aplain:hello world , len:1668 65 6c 6c 6f 20 77 6f 72 6c 64 20 20 20 20 20 # 密文被正确解密 [2019/05/24 00:50:19] [DEBUG] [aes_cbc_dec_string:825 ] aes_cbc_dec_string ok[root@localhost aes_test]# 以明文``进行加密解密测试12345678910111213141516171819202122232425262728293031int main(int argc, char *argv[])&#123; int enc_len; uint8_t *enc, *dec; log_level(LOG_DBG); log_mode(LOG_TEST); uint8_t key[KEY_LEN] = &#123; 0x25,0xdd,0xff,0xee,0x33,0x4c,0xfe,0x30,0x09,0xaa,0x55,0xa5,0x0c,0x97,0x66,0x13 &#125;; uint8_t inv[KEY_LEN] = &#123; 0x78,0x97,0xee,0xea,0xa3,0x3c,0xcf,0xf8,0x8f,0xfb,0xb6,0x65,0x5c,0xc3,0xc7,0x76 &#125;; uint8_t plain[10] = &#123; 0x03, 0x05, 0x2d, 0x1c, 0xaa, 0xe3, 0xff, 0x23, 0x17, 0x89 &#125;; aes_cbc_enc_byte(key, array_size(key), inv, array_size(inv), KEY_LEN, plain, array_size(plain), &amp;enc, &amp;enc_len); aes_cbc_dec_byte(key, array_size(key), inv, array_size(inv), KEY_LEN, enc, enc_len, &amp;dec, &amp;enc_len); mem_free(enc); mem_free(dec); return 0;&#125; 编译运行程序，打印如下123456789101112131415161718192021222324252627282930313233343536[root@localhost aes_test]# ./bin/aes_test [2019/05/24 00:54:38] [DEBUG] [aes_cbc_enc_byte:857 ] aes_cbc_enc_byte inkey len 16, byte:25 dd ff ee 33 4c fe 30 9 aa 55 a5 c 97 66 13safe key len 16, byte:25 dd ff ee 33 4c fe 30 9 aa 55 a5 c 97 66 13inv len 16, byte:78 97 ee ea a3 3c cf f8 8f fb b6 65 5c c3 c7 76safe inv len 16, byte:78 97 ee ea a3 3c cf f8 8f fb b6 65 5c c3 c7 76plain len 10, byte: 3 5 2d 1c aa e3 ff 23 17 89safe plain len 16, byte: # 明文长度被填充到16 3 5 2d 1c aa e3 ff 23 17 89 0 0 0 0 0 0cipher len 16, byte: 8 ce a0 f0 65 ec e7 25 d8 8b d7 ae 0 ac 8a 73 [2019/05/24 00:54:38] [DEBUG] [aes_cbc_enc_byte:908 ] aes_cbc_enc_byte ok [2019/05/24 00:54:38] [DEBUG] [aes_cbc_dec_byte:940 ] aes_cbc_dec_byte incipher len 16, byte: 8 ce a0 f0 65 ec e7 25 d8 8b d7 ae 0 ac 8a 73plain len 16, byte: # 密文被正确解密 3 5 2d 1c aa e3 ff 23 17 89 0 0 0 0 0 0 [2019/05/24 00:54:38] [DEBUG] [aes_cbc_dec_byte:970 ] aes_cbc_dec_byte ok[root@localhost aes_test]# Java123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168package com.awokezhou.aestest;import org.apache.commons.codec.binary.Base64;import javax.crypto.Cipher;import javax.crypto.spec.IvParameterSpec;import javax.crypto.spec.SecretKeySpec;/** * AES加密算法util * Created by steadyjack on 2018/4/21. */public class AESUtils &#123; private static final String EncryptAlg ="AES"; //private static final String Cipher_Mode="AES/ECB/PKCS7Padding"; //private static final String Cipher_Mode="AES/CBC/PKCS5Padding"; private static final String Cipher_Mode="AES/CBC/NoPadding"; private static final String Encode="UTF-8"; private static final int Secret_Key_Size=16; private static final byte PaddingChar = ' '; private static final String Key_Encode="UTF-8"; private static String ivParameter="1234567890123456"; public static String bytesToHexString(byte[] src)&#123; StringBuilder stringBuilder = new StringBuilder(""); if (src == null || src.length &lt;= 0) &#123; return null; &#125; for (int i = 0; i &lt; src.length; i++) &#123; int v = src[i] &amp; 0xFF; String hv = Integer.toHexString(v); if (hv.length() &lt; 2) &#123; stringBuilder.append(0); &#125; stringBuilder.append(hv); &#125; return stringBuilder.toString(); &#125; /** * AES/ECB/PKCS7Padding 加密 * @param content * @param key 密钥 * @return aes加密后 转base64 * @throws Exception */ public static String aesPKCS7PaddingEncrypt(String content, String key, String ivParameter) throws Exception &#123; try &#123; //Security.addProvider(new org.bouncycastle.jce.provider.BouncyCastleProvider()); Cipher cipher = Cipher.getInstance(Cipher_Mode); byte[] realKey = getSecretKey(key); System.out.printf("密钥16进制:%s\n", bytesToHexString(realKey)); SecretKeySpec keySpec = new SecretKeySpec(realKey, EncryptAlg); IvParameterSpec iv = new IvParameterSpec(ivParameter.getBytes());//使用CBC模式，需要一个向量iv，可增加加密算法的强度 cipher.init(Cipher.ENCRYPT_MODE, keySpec, iv); byte[] data = cipher.doFinal(content.getBytes(Encode)); System.out.println(String.format("密文:%s, length:%d", data, data.length)); System.out.printf("密文16进制:%s\n", bytesToHexString(data)); String result = new Base64().encodeToString(data); return result; &#125; catch (Exception e) &#123; e.printStackTrace(); throw new Exception("AES加密失败：content=" +content +" key="+key); &#125; &#125; /** * AES/ECB/PKCS7Padding 解密 * @param content * @param key 密钥 * @return 先转base64 再解密 * @throws Exception */ public static String aesPKCS7PaddingDecrypt(String content, String key, String ivParameter) throws Exception &#123; try &#123; //Security.addProvider(new org.bouncycastle.jce.provider.BouncyCastleProvider()); byte[] decodeBytes = new Base64().decodeBase64(content); Cipher cipher = Cipher.getInstance(Cipher_Mode); byte[] realKey = getSecretKey(key); SecretKeySpec keySpec = new SecretKeySpec(realKey, EncryptAlg); IvParameterSpec iv = new IvParameterSpec(ivParameter.getBytes()); cipher.init(Cipher.DECRYPT_MODE, keySpec, iv); byte[] realBytes=cipher.doFinal(decodeBytes); return new String(realBytes, Encode); &#125; catch (Exception e) &#123; e.printStackTrace(); throw new Exception("AES解密失败：Aescontent = " +e.fillInStackTrace(),e); &#125; &#125; /** * 对密钥key进行处理：如密钥长度不够位数的则 以指定paddingChar 进行填充； * * 此处用空格字符填充，也可以 0 填充，具体可根据实际项目需求做变更 * @param key * @return * @throws Exception */ public static byte[] getSecretKey(String key) throws Exception&#123; final byte paddingChar=' '; byte[] realKey = new byte[Secret_Key_Size]; byte[] byteKey = key.getBytes(Key_Encode); for (int i =0;i&lt;realKey.length;i++)&#123; if (i&lt;byteKey.length)&#123; realKey[i] = byteKey[i]; &#125;else &#123; realKey[i] = paddingChar; &#125; &#125; return realKey; &#125; public static byte[] keyPadding(String key, byte c, int size, String encode) throws Exception&#123; byte[] paddingKey = new byte[size]; byte[] bytesKey = key.getBytes(encode); for (int i=0; i&lt;paddingKey.length; i++)&#123; if (i&lt;bytesKey.length)&#123; paddingKey[i] = bytesKey[i]; &#125; else &#123; paddingKey[i] = c; &#125; &#125; return paddingKey; &#125; public static String textPadding(String text, byte c, int size, String encode) throws Exception&#123; int len = text.length(); byte[] paddingText = new byte[size]; byte[] bytesText = text.getBytes(encode); for (int i=0; i&lt;paddingText.length; i++)&#123; if (i&lt;bytesText.length)&#123; paddingText[i] = bytesText[i]; &#125; else &#123; paddingText[i] = c; &#125; &#125; String result = new String(paddingText); return result; &#125; public static void main(String[] args) throws Exception &#123; //密钥 加密内容(对象序列化后的内容-json格式字符串) String key = "xadcdgde"; String encPlainText = "hello world"; System.out.println(String.format("明文:%s, length:%d", encPlainText, encPlainText.length())); encPlainText = textPadding(encPlainText, PaddingChar, Secret_Key_Size, Encode); System.out.printf("明文16进制:%s\n", bytesToHexString(encPlainText.getBytes())); String cipherText = aesPKCS7PaddingEncrypt(encPlainText, key, ivParameter); String decPlainText = aesPKCS7PaddingDecrypt(cipherText, key, ivParameter); System.out.println(String.format("解密:%s, length:%d", decPlainText, decPlainText.length())); &#125;&#125; 运行代码，打印为123456明文:hello world, length:11明文16进制:68656c6c6f20776f726c642020202020密钥16进制:78616463646764652020202020202020密文:[B@2b2948e2, length:16密文16进制:de1df96b2782192f79885f8b9566be4a解密:hello world , length:16 java密文与C语言密文16进制内容一致]]></content>
      <categories>
        <category>协议</category>
        <category>安全</category>
        <category>加密算法</category>
      </categories>
      <tags>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[csv导入cassandra大量row丢失]]></title>
    <url>%2F2019%2F05%2F17%2Fcsv%E5%AF%BC%E5%85%A5cassandra%E5%A4%A7%E9%87%8Frow%E4%B8%A2%E5%A4%B1%2F</url>
    <content type="text"><![CDATA[在测试cassandra数据库导出/导入csv文件功能时，发现从csv文件导入所有数据到cassandra的表中后，大量行丢失。经查资料和排查发现，是因为创建表时的主键设置问题 问题描述csv文件名为record_2019-01-01.csv，列结构为[time,lineID,stationID,deviceID,status,userID,payType]，一共7列，行数为253959312345[root@eca977993d07 import]# cat record_2019-01-01.csv | head -n 1time,lineID,stationID,deviceID,status,userID,payType[root@eca977993d07 import]# [root@eca977993d07 import]# wc -l record_2019-01-01.csv2539593 record_2019-01-01.csv 执行cqlsh连接到本地的cassandra数据库，在名为open_metro的keysapce中，创建了名为metro_train的表12345678910111213[root@eca977993d07 import]# cqlshConnected to Test Cluster at 127.0.0.1:9042.[cqlsh 5.0.1 | Cassandra 3.11.4 | CQL spec 3.4.4 | Native protocol v4]Use HELP for help.cqlsh&gt; use open_metro ;cqlsh:open_metro&gt; CREATE TABLE metro_train ( time text PRIMARY KEY, lineID varchar, stationID varchar, deviceID text, status varint, userID text, payType varint ); cqlsh:open_metro&gt; SELECT * FROM metro_train ; time | deviceid | lineid | paytype | stationid | status | userid------+----------+--------+---------+-----------+--------+--------(0 rows)cqlsh:open_metro&gt; 从csv文件中导入数据到metro_train表中12345678910111213141516171819cqlsh:open_metro&gt; COPY metro_train (time, lineID, stationID, deviceID, status, userID, payType) FROM &apos;record_2019-01-01.csv&apos; WITH HEADER = true; Using 1 child processesStarting copy of open_metro.metro_train with columns [time, lineid, stationid, deviceid, status, userid, paytype].Processed: 2539592 rows; Rate: 6755 rows/s; Avg. rate: 12327 rows/s2539592 rows imported from 1 files in 3 minutes and 26.019 seconds (0 skipped).cqlsh:open_metro&gt; cqlsh:open_metro&gt; SELECT COUNT(*) FROM metro_train ; count------- 65790(1 rows)Warnings :Aggregation query used without partition keycqlsh:open_metro&gt; 由打印可以看出，的确处理了2539592行数据，和原文件行数一致，但是查看表的行数，只有65790行 问题排查在StackOverflow里找到了类似的问题，Cassandra is missing data when loading a csv with cassandra-loader 原因在于表中的主键不唯一，cassandra的COPY操作做的是插入更新操作，因为主键是time，而后续的数据中有重复的时间数据，后一个就会覆盖前一个，因此导致最终导入表中的数据行数少了很多 问题解决删除metro_train表，重新创建该表，并设置为多个主键123456789101112cqlsh:open_metro&gt; CREATE TABLE metro_train ( time text, lineID varchar, stationID varchar, deviceID text, status varint, userID text, payType varint, PRIMARY KEY(time, lineID, stationID, deviceID, status, userID, payType));cqlsh:open_metro&gt; COPY metro_train (time, lineID, stationID, deviceID, status, userID, payType) FROM '/usr/local/apache-cassandra-3.11.4/import/record_2019-01-01.csv'... 等待所有行都从文件加载好，查看一下表大小123cqlsh:open_metro&gt; SELECT COUNT(*) FROM metro_train;OperationTimedOut: errors=&#123;'127.0.0.1': 'Client request timeout. See Session.execute[_async](timeout)'&#125;, last_host=127.0.0.1cqlsh:open_metro&gt; 这里出现超时，是因为cassandra设置的超时，关闭cqlsh，重新运行，并携带超时参数1cqlsh --request-timeout 60000 重新查看表大小，为253906012345678910111213cqlsh&gt; use open_metro ; cqlsh:open_metro&gt; SELECT COUNT(*) FROM metro_train; count--------- 2539060(1 rows)Warnings :Aggregation query used without partition keycqlsh:open_metro&gt; 这次数据量大致与文件行数差不多，但是还差了533行，为什么会差这么多行呢？猜测可能是因为文件本身有一些重复的行 使用sort命令查看文件重复的行1234[root@eca977993d07 import]# sort record_2019-01-01.csv |uniq -d &gt; repeat.txt[root@eca977993d07 import]# wc -l repeat.txt532 repeat.txt[root@eca977993d07 import]# 可以看到，record_2019-01-01.csv文件本身有532行是重复的，再加上第一行行首，刚好是cassandra的metro_train表大小和文件行数的差距]]></content>
      <categories>
        <category>数据库</category>
        <category>cassandra</category>
      </categories>
      <tags>
        <tag>Cassandra</tag>
        <tag>csv</tag>
        <tag>问题解决</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker网络配置]]></title>
    <url>%2F2019%2F05%2F16%2Fdocker%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[4种网络模式docker支持4种网络模式 none 容器除了本地接口’lo’，没有其他网络接口 host 容器的网络配置与宿主机完全相同 bridge 容器的网络接口连接到宿主机建立的bridge上 默认情况下，docker本来会创建3个网络：bridge、none和host，通过以下命令可查看docker中所有网络12345[root@localhost install]# docker network lsdocker network lsNETWORK ID NAME DRIVER SCOPE0f4a6eea9183 bridge bridge local8a9064b404c3 host host localb00d9a442061 none null local 指定容器的网络模式通过docker run命令从镜像创建容器时可通过--net参数指定网络模式123docker run --net=none image_testdocker run --net=host image_testdocker run --net=bridge image_test 如果不指定网络模式，默认是bridge模式，docker默认会在宿主机上创建一个名为docker0的bridge，通过ifconfig可看到12345678docker0: flags=4099&lt;UP,BROADCAST,MULTICAST&gt; mtu 1500 inet 172.17.0.1 netmask 255.255.0.0 broadcast 172.17.255.255 inet6 fe80::42:80ff:fea1:9eca prefixlen 64 scopeid 0x20&lt;link&gt; ether 02:42:80:a1:9e:ca txqueuelen 0 (Ethernet) RX packets 0 bytes 0 (0.0 B) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 16 bytes 1970 (1.9 KiB) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 该bridge的地址可通过配置文件修改，默认创建的镜像会将其网络接口绑定到docker0上 创建自定义docker网络一般不使用docker0绑定容器，而是自定义网络1docker network create --driver bridge net_name 然后将容器加入网络1docker run --net=net_name ... 容器访问外网默认情况下，容器可访问外网，其原理是在宿主机上做了NAT转换，iptables -t nat --nvL查看iptables的nat表123456789101112131415161718192021222324252627Chain PREROUTING (policy ACCEPT 1455 packets, 213K bytes) pkts bytes target prot opt in out source destination 5 324 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCALChain INPUT (policy ACCEPT 1131 packets, 192K bytes) pkts bytes target prot opt in out source destination Chain OUTPUT (policy ACCEPT 17 packets, 1703 bytes) pkts bytes target prot opt in out source destination 0 0 DOCKER all -- * * 0.0.0.0/0 !127.0.0.0/8 ADDRTYPE match dst-type LOCALChain POSTROUTING (policy ACCEPT 17 packets, 1703 bytes) pkts bytes target prot opt in out source destination 73 4596 MASQUERADE all -- * !br-0e12e16b15a1 172.19.0.0/16 0.0.0.0/0 0 0 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0 365 23169 MASQUERADE all -- * !br-ba1cdf022332 172.18.0.0/16 0.0.0.0/0 2 321 RETURN all -- * * 192.168.122.0/24 224.0.0.0/24 0 0 RETURN all -- * * 192.168.122.0/24 255.255.255.255 0 0 MASQUERADE tcp -- * * 192.168.122.0/24 !192.168.122.0/24 masq ports: 1024-65535 0 0 MASQUERADE udp -- * * 192.168.122.0/24 !192.168.122.0/24 masq ports: 1024-65535 0 0 MASQUERADE all -- * * 192.168.122.0/24 !192.168.122.0/24 Chain DOCKER (2 references) pkts bytes target prot opt in out source destination 0 0 RETURN all -- br-0e12e16b15a1 * 0.0.0.0/0 0.0.0.0/0 0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0 2 168 RETURN all -- br-ba1cdf022332 * 0.0.0.0/0 0.0.0.0/0 可以看到在POSTROUTING链上对源地址是172.17.0.0/16的数据流做了MASQUERADE操作，因此如果要禁用容器访问外网，则将其DROP掉1iptables -t nat -I POSTROUTING -s 172.18.0.0/16 -j RETURN 或者使用filter表的FORWARD链，直接禁止其转发1iptables -I FORWARD -s 172.18.0.0/16 -j DROP]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>网络</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法分析]]></title>
    <url>%2F2019%2F05%2F15%2F%E7%AE%97%E6%B3%95%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[课程内容主要分为两个部分：算法分析和算法设计。算法分析是理论研究，研究计算机程序的性能和资源利用，尤其关注性能，也考量其他方面如通信、存储(内存、磁盘)等 排序算法例子输入一组序列 a_{1},a_{2},...,a_{n}，按照从小到大的顺序排列后输出 使用插入排序算法来完成，伪代码如下1234567For j=2 to n key = A[j] i = j-1 while i&gt;0 and A[i]&gt;key A[i+1] = A[i] i -= 1 A[i+1] = key 该算法有两层循环，外层循环j从2到n递增，内层循环i从j-1到0递减。任意一次循环前的情况如图所示关键点如下 将数组j位置的元素提取出来，记作key 0到j-1位置的元素为已排列sorted 循环的目的就是使sorted部分增长1 循环步骤如图所示做法是一步步将前一个元素拷贝到后一个位置，直到找到key值的合适位置，插入进去 排序算法分析运行时间算法的运行时间取决于很多因素 输入本身 如果输入本身已经是排好序的或者部分排好序的，剩余的工作就很少；而最坏的情况是逆序，所有的元素都要重新调整 输入规模 将运行时间看作数据规模的函数 运行时间的上界 运行时间不会超过的某个值，这代表对用户的一种承诺；如果关注运行时间最快是多少是没有意义的，因为一个很烂的算法在输入本身已经排好序的情况下也可以很快 各种类型的分析 最坏情况分析 通常最关注这个，定义$T(n)$为输入规模为$n$时的最长运行时间， 平均情况分析 这里$T(n)$是输入规模$n$之下所有可能输入的期望时间，而计算期望，必须假定或者给出输入的统计分布(如均匀分布、正态分布等) 最好情况分析 这是一种假象，没啥用 那么，插入排序的最坏情况消耗多少时间？首先取决于运行该算法的计算机，计算能力是多少。当比较算法时，比较的是相对速度，即在相同的计算机上作比较。当然，也会关心其绝对速度，一个算法，无论在什么样的机器上，其能力都可以用一种形式表示，那就是算法的大局观——渐进分析 算法大局观——渐进分析渐进分析的基本观点 忽略掉依赖于机器的常量， 不去检查实际的运行时间 关注运行时间的增长 定义$\Theta$符号，表示弃去低阶项，忽略常数因子。例如公式为$3n^{3}+90n^{2}-5n+6046$，最高阶是3阶，因此2阶项和1阶项都去掉，常数项也去掉，则该公式为$\Theta(n^{3})$ 当$n \to +\infty$，可知总是存在一个$n$，使得$\Theta(n^{2})$的算法优于$\Theta(n^{3})$的算法，无论其他低阶项和常数项是什么；即使在一台慢速机器上运行$\Theta(n^{2})$，而在快速机器上运行$\Theta(n^{3})$，也不影响$\Theta(n^{2})$超过$\Theta(n^{3})$ $\Theta$符号的好处在于可以满足对相对速度和绝对速度的双重比较要求，因为无论在什么计算机平台上都能实现这一点，在不同平台上也许只差一个常数因子，但随着输入规模变大，渐进结果是准确的，如图总会存在一个$n{0}$，当$n &gt; n{0}$时，$\Theta(n^{2})$比$\Theta(n^{3})$的时间开销更小。但是有些时候，$n_{0}$的值过大，大到计算机无法运行该算法，这也是某些情况下对相对低速的算法也感兴趣的原因，因为低速算法尽管用渐进的视角来看，最终可能比较慢，但是让然可以在合理输入范围内运行更快。因此需要在数学理论和工程直觉之间做权衡 排序算法的最坏情况最坏情况是输入是逆向排序，每次循环都要把所有元素移动一次。假设每一条代码指令或原子操作都耗费某固定常数时间，而这个常数是多少无关紧要。外层循环从$j=2$到$n$，对于j的每一次取值，循环体的执行耗时应该是$j$乘以某个常数，则耗时为$\Theta(j)$。因此排序算法的总时间消耗为 \begin{equation}T(n) = \sum_{i=1}^{n} \Theta(j)\end{equation} 如何简化该公式呢？这是一个求和表达式，具体来说是等差数列求和，等差数列求和公式为 \begin{equation}S(n) = na_{1} + \frac{n(n-1)}{2} d\end{equation} 最高阶次为2次，省略掉常数项和低阶项，等于$\Theta(n^{2})$，即\begin{equation}T(n) = \sum_{i=1}^{n} \Theta(j) = \Theta(n^{2})\end{equation} 因此，对于较小的$n$，插入排序的速度还可以，但是$n$足够大时，插入排序效率就比较低了 并归排序对于数组A[1...n]的并归排序，有3个步骤 如果n=1，认为已经排好序了 递归地对A[1到n/2向上取整]和A[n/2向下取整到n]这两部分排序 把排好序的两个表并归 关键的子程序在于并归，下图中是两个已经排好序的数组，首先判断两张表中最小元素是多少，最小元素一定在两张表之首；然后将最小的元素拿出来重复以上步骤，每次从两个表头取出最小的元素排在新数组的后面 分析性能并归算法的$T(n)$是多少？该算法的3个步骤中，第1步只需要常数时间；第2步可以描述成\begin{equation}T(n) = T(n/2向下取整) + T(n/2向上取整) = 2T(n/2)\end{equation} 第3步并归两张已排序好的表，耗时为$\Theta(n)$ 综上，并归排序的总耗时$T(n)$为 T(n)=\left\{ \begin{aligned} &\Theta(1), &if \quad n=1 \\ &2T(n/2) + \Theta(n), &if \quad n>1\\ \end{aligned} \right.该表达式用递归树方法分析 最终可以分解为一个递归树树高为$\log{2}n$，叶子节点最大为$n$，将树上所有层级耗时相加，总和为\begin{equation}C{n} \log{2}n + \Theta(n) = \Theta(n\log{2}n)\end{equation} $\Theta(n\log_{2}n)$比$\Theta(n^{2})$快]]></content>
      <categories>
        <category>公开课</category>
        <category>MIT 算法导论</category>
      </categories>
      <tags>
        <tag>公开课</tag>
        <tag>algorithm</tag>
        <tag>sort</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[github多账户设置]]></title>
    <url>%2F2019%2F05%2F15%2Fgithub%E5%A4%9A%E8%B4%A6%E6%88%B7%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[如果有两个github账户，要在windows下同时操作它们，和单账户的配置有些不同 假设 两个github的用户名分别为test1、test2 两个github账户绑定的邮箱分别为test1@example.com和test2@example.com github与windows下的用户名一致 创建ssh key首先进入ssh目录1cd ~/.ssh 为test1创建ssk key1ssh-keygen -t rsa -b 4096 -C "test1@example.com" 输入test1保存的密钥文件名称12Enter a file in which to save the key (/Users/you/.ssh/id_rsa): [Press enter]# 这里输入 test1_id_rsa 输入密码12Enter passphrase (empty for no passphrase): [Type a passphrase]Enter same passphrase again: [Type passphrase again] test2的ssk key创建步骤同上，保存的密钥文件为test2_id_rsa ssh key 添加到ssh-agentssh-agent在后台启动1eval "$(ssh-agent -s)" 将两个密钥文件添加到ssh-agent12ssh-add ~/.ssh/test1_id_rsassh-add ~/.ssh/test2_id_rsa 配置文件config配置或者创建并配置config文件(还是在~/.ssh/路径下)123456789Host test1.github.com HostName github.com AddKeysToAgent yes IdentityFile ~/.ssh/test1_id_rsaHost test2.github.com HostName github.com AddKeysToAgent yes IdentityFile ~/.ssh/test2_id_rsa ssh key 添加到github在test1 github页面的Settings—&gt;SSH keys and GPG keys中，点击New SSH key，将test1_id_rsa.pub文件中的所有内容复制粘贴过去 test2同上 测试测试test1、test212ssh -T git@test1.github.comssh -T git@test2.github.com clone项目到本地例如test1的远程仓库中有project11git clone git@test1.github.com:test1/project1.git 第一个test1是该账户在windows上配置的config文件中的HostName，后一个test1是github上的用户名]]></content>
      <tags>
        <tag>github</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[linecache处理大文件时内存溢出]]></title>
    <url>%2F2019%2F05%2F09%2Flinecache%E5%A4%84%E7%90%86%E5%A4%A7%E6%96%87%E4%BB%B6%E6%97%B6%E5%86%85%E5%AD%98%E6%BA%A2%E5%87%BA%2F</url>
    <content type="text"><![CDATA[最近在研究天池竞赛的城市计算AI挑战赛题目，在编写代码过程中发现一个非常诡异的事情，最后经过一系列排查，最终定位到是在使用linecache模块处理大文件时发生了内存溢出 问题描述赛题给出了杭州市2019年1月1日到1月25日的3条地铁线路81个站点的刷卡数据，每天一个文件，25天共25个文件。文件类型为.csv文件，文件中以行为单位记录了每个刷卡事件，包括时间、地铁线、站点、用户、刷卡类型等，每个文件大小160M左右，行数在250万行左右 为什么使用linecache？按理说.csv文件，python直接使用csv模块来操作就行了，为啥还要用linecache呢？因为效率！ csv读文件的方式为cvs.reader(fileobj)，该函数返回一个可迭代对象，代码中使用for循环来遍历每一行，然后做操作。但是当我对25天的数据做统计分析时，依次读取25个文件，按行遍历，结果是要将近2个小时才能处理完，太慢了！ 经过查阅资料结合自己分析，单次循环我的操作复杂度并不高，所以性能的瓶颈应该在csv的I/O操作上，而这种I/O操作又不能用并发的方式去做，大文件的处理应该使用批量化 网上有人说可以用linecache读取文件的指定行，所以我封装了一个简单的批量化操作文件的类Batch，代码入下12345678910111213141516171819202122232425262728293031323334353637class Batcher(object): def __init__(self, filename, batch_size=2000, offset=0, end=None): self.filename = filename try: self._n_lines = lines(self.filename) except Exception as e: print e if batch_size &gt; self._n_lines: self._size = self._n_lines self._n_batchs = 1 print 'batch_size too big, adjust to file lines &#123;&#125;'.format(self._n_lines) else: self._size = batch_size self._n_batchs = (self._n_lines/self._size) + 1 self._offset = offset self._end = end self.getlines = linecache.getlines self.clearcache = linecache.clearcache def _batch(self): if self._end and self._offset &gt;= self._end: return None offset = self._offset size = self._size end = offset+size batch = self.getlines(self.filename)[offset:end] self._offset += size return batch def __iter__(self): while True: batch = self._batch() if not batch: break yield batch 思路是利用yield将Batch做成一个可遍历的生成器，每次返回一个小批量的数据，而小批量数据的获得就通过linecache来做，最后在一个循环中来使用就行了，例如：123batcher = Batcher(filename, 2000)for batch in batcher: ...... 实践证明该方法确实将时间压缩了很多，单个文件处理速度5秒左右，处理25个文件时间只花了2分43秒1234567&gt;&gt;&gt; from analysis import DataAnalysis&gt;&gt;&gt; da = DataAnalysis()&gt;&gt;&gt; flows = da.subway_line_analysis('2019-01-01')one file calculating: 100%|██████████| 127/127 [00:05&lt;00:00, 26.54it/s]&gt;&gt;&gt; flows = da.subway_line_analysis()subway line calculating: 100%|██████████| 25/25 [02:43&lt;00:00, 6.90s/it]&gt;&gt;&gt; 问题处理单个文件时并没有发生异常情况，而在处理多个文件时，每个文件会返回一个统计列表，其中几个统计列表会是空的，代码逻辑类似1234567891011121314def process_one(file): list = [] batcher = Batch(file, 20000) for batch in batcher: for line in csv.reader(batch) # 处理每一行 ...... list.append(xxx) return listdef process(): for f in files: list = process_one(f) ...... 处理单个文件的函数process_one的逻辑经检查是正确的，只要按代码执行顺序读到文件的每一行，返回的list一定不为空，但现象就是有几个文件的处理结果返回值为空 问题排查排查问题并没有想太多，猜测到什么情况，就立即实现验证 文件内容不对？最先想到的可能原因就是这些异常的文件本身内容有误，因此我单独调用process_one来处理单个文件，结果发现单独处理这些文件是能够返回正常值的 接着我又在process函数中将文件列表乱序，结果发现出现问题的文件并不固定，而是处理了8到9个文件之后一定发生一次问题，然后再处理了4到5个之后再发生一次问题… 打印调试因为排除了文件本身的问题，那只能是程序逻辑执行上的问题了。我在process_one函数逻辑中加入了很多调试信息，尝试将出现错误的执行情况过程中运行逻辑打印出来，我在函数头部、list追加操作前后以及函数返回前都加入了调试信息 结果很奇怪，居然有两种情况 list追加操作前后有打印信息 list追加操作前后没有打印信息第一种情况运行是正常的，返回值是有的。第二种是不正常现象，因为没有进行追加操作，因此list为空 但是按照程序逻辑执行，如果读每一行都正常，list是不可能为空的，怀疑是我的Batch()操作或者linecache或者csv.reader()可能存在使用或者执行上的问题 超出自己知识范围？排查到这个地步，我觉得问题已经超出了我的知识范围，该怎么排查？我在网络上查阅csv、linecache与运行异常相关问题，没啥结果 突然想到能不能用资源管理器看出来什么门道?我一边运行程序，一边对着看资源管理器的CPU、内存等视图，结果发现每次出问题时，内存的走势都是一个断崖式下跌，如图 内存占用走势是阶梯式上升，然后突然下跌，再阶梯式上升，下跌，且下跌的时间刚好是出问题的循环 哪里占用内存？很明显，是内存占用过高，python解释器或者系统强制把内存释放了，但是程序中哪里会占用内存呢？我怀疑是我的Batch或者列表操作。我在代码中加入了一些del操作，把可能占用内存的变量在使用后都释放掉，再运行程序看内存变化，结果并没有任何变化。又陷入了困境 分析解决观察内存的上升，发现每次上升的的幅度差不多，感觉很像C语言的malloc向系统很有规律的申请了一次次内存块，程序中哪里占用内存最多呢？ 我在网上查到可以用memory_profiler模块来分析内存，但是使用后程序运行太过缓慢，而且这个模块是必须将程序运行完后才分析显示出来，我放弃了 主要的操作是读文件，这块会比较消耗内存，文件是读取到内存中，再访问的，难道是每次读完一个文件后，内存中的内容没释放吗？我想到了linecache，cache不是缓存的意思吗？我怀疑可能是linecache这里操作有什么问题。我在网上查linecache和内存占用相关问题，结果发现有一篇文章恰好提到这个问题 参考连接：python linecache读取过程 该文章中作者也是发现linecache操作导致高内存占用，他读了源码发现其实际上也是调用readlines函数，将文件内容预先缓存在内存中，用户才能调用lines = readlines(file)[x:y]获取指定行数。而linecache提供了clearcache方法来清理缓存 linecache源码分析123456789101112131415# The cachecache = &#123;&#125; # The cachedef getlines(filename, module_globals=None): """Get the lines for a file from the cache. Update the cache if it doesn't contain an entry for this file already.""" if filename in cache: return cache[filename][2] try: return updatecache(filename, module_globals) except MemoryError: clearcache() return [] getlines函数会优先从cache中获取文件内容，如果cache不存在，才会从updatecache函数获取 12345678910111213141516171819202122232425def updatecache(filename, module_globals=None): """Update a cache entry and return its list of lines. If something's wrong, print a message, discard the cache entry, and return an empty list.""" if filename in cache: del cache[filename] if not filename or (filename.startswith('&lt;') and filename.endswith('&gt;')): return [] fullname = filename try: stat = os.stat(fullname) except Exception: ...... try: with open(fullname, 'rU') as fp: lines = fp.readlines() except IOError: return [] if lines and not lines[-1].endswith('\n'): lines[-1] += '\n' size, mtime = stat.st_size, stat.st_mtime cache[filename] = size, mtime, lines, fullname return lines 由最后一行可以看到，文件内容被放在cache字典结构的第3个元素中，cache结构如图 12345def clearcache(): """Clear the cache entirely.""" global cache cache = &#123;&#125; 清理缓存函数会将cache清空，因为我的操作里没有清空缓存，每读一次文件，cache里都会新增一个文件的全部内容，当经过几个文件处理之后，cache结构如下 验证既然是因为缓存没有清理导致的，我修改Batch类，在其__del__方法上调用clearcache来释放内存，这样每次操作完一个文件后，缓存清空一次，应该就不会一直占用内存了 12def __del__(self): self.clearcache() 修改代码，运行程序，结果竟然没有发生异常！内存视图如下每读一次文件，在操作结束后都会将cache清空，因此内存趋势是上下起伏]]></content>
      <categories>
        <category>Program Language</category>
        <category>Python</category>
        <category>大文件处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>问题解决</tag>
        <tag>性能</tag>
        <tag>内存</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python-kafka抓包分析]]></title>
    <url>%2F2019%2F05%2F07%2Fpython-kafka%E6%8A%93%E5%8C%85%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[看源码看了一段时间了，发现很多协议上有些概念和流程理解的不到位，因此搭建环境来抓包分析一下整个消息交互的过程是怎么样的 环境介绍kafka的broker(只有一个)服务运行于虚拟机上的docker容器(dk-kafka-test-v2)中，虚拟机和容器都是centos7系统，宿主和容器之间网络模式为Bridge模式。环境示意图如下容器连接到宿主的docker0桥上，宿主地址172.18.0.1，容器地址172.18.0.2，在容器中使用tcpdump抓包 注意环境安装问题需要安装以下内容 jdk zookeeper kafka docker的hostname问题在docker中运行kafka服务后，宿主运行load_example.py，会报DNS lookup failed错误，因为docker的hostname默认是一串数字，调用socket.getaddrinfo()函数解析不了该hostname，需手动修改/etc/hosts12345678127.0.0.1 localhost::1 localhost ip6-localhost ip6-loopbackfe00::0 ip6-localnetff00::0 ip6-mcastprefixff02::1 ip6-allnodesff02::2 ip6-allrouters172.17.0.2 172.17.0.2172.18.0.2 172.18.0.2 具体操作broker启动容器中已经编写好shell脚本启动zookeeper和broker12345678kafka_path='kafka_broker'zookeeper_path='zookeeper'source /etc/profile./$zookeeper_path/bin/zkServer.sh start./$kafka_path/bin/kafka-server-start.sh $kafka_path/config/server.properties 执行脚本，broker启动运行1234567891011121314[root@3b86889153b0 local]# ./kafka_broker_start.sh ZooKeeper JMX enabled by defaultUsing config: /usr/local/zookeeper/bin/../conf/zoo.cfgStarting zookeeper ... already running as process 84.[2019-05-07 12:33:13,714] INFO Registered kafka:type=kafka.Log4jController MBean (kafka.utils.Log4jControllerRegistration$)[2019-05-07 12:33:14,630] INFO starting (kafka.server.KafkaServer)[2019-05-07 12:33:14,632] INFO Connecting to zookeeper on localhost:2181 (kafka.server.KafkaServer)[2019-05-07 12:33:14,676] INFO [ZooKeeperClient] Initializing a new session to localhost:2181. (kafka.zookeeper.ZooKeeperClient)[2019-05-07 12:33:14,683] INFO Client environment:zookeeper.version=3.4.13-2d71af4dbe22557fda74f9a9b4309b15a7487f03, built on 06/29/2018 00:39 GMT (org.apache.zookeeper.ZooKeeper)[2019-05-07 12:33:14,684] INFO Client environment:host.name=3b86889153b0 (org.apache.zookeeper.ZooKeeper)[2019-05-07 12:33:14,684] INFO Client environment:java.version=12.0.1 (org.apache.zookeeper.ZooKeeper)[2019-05-07 12:33:14,684] INFO Client environment:java.vendor=Oracle Corporation (org.apache.zookeeper.ZooKeeper)[2019-05-07 12:33:14,684] INFO Client environment:java.home=/usr/local/java (org.apache.zookeeper.ZooKeeper)... 启动抓包抓包使用tcpdump工具，执行以下命令，指定截获eth1接口的端口号为9092的tcp包，保存到capture.cap文件1tcpdump -i eth1 tcp port 9092 -w capture.cap 宿主运行load_example.py宿主上运行python-kafka的load_example.py测试脚本，该脚本会启动一个生产者和一个消费者，生产者循环生产数据，消费者循环消费数据，需要先修改生产者和消费者配置的broker地址1234567891011121314151617181920212223242526class Producer(threading.Thread): big_msg = b'1' * msg_size def run(self): producer = KafkaProducer(bootstrap_servers='172.18.0.2:9092') self.sent = 0 while not producer_stop.is_set(): producer.send('my-topic', self.big_msg) self.sent += 1 producer.flush()......class Consumer(threading.Thread): def run(self): consumer = KafkaConsumer(bootstrap_servers='172.18.0.2:9092', auto_offset_reset='earliest') consumer.subscribe(['my-topic']) self.valid = 0 self.invalid = 0 for message in consumer: if len(message.value) == msg_size: self.valid += 1 else: self.invalid += 1 执行脚本后，开始数据传输，打开了日志的DEBUG模式1234567891011121314151617181920[root@localhost kafka-python]# python benchmarks/load_example.py 2019-05-07 22:34:07,437.437.673091888:kafka.producer.kafka:139872394159872:DEBUG:32192:Starting the Kafka producer2019-05-07 22:34:07,438.438.35401535:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name connections-closed2019-05-07 22:34:07,438.438.606977463:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name connections-created2019-05-07 22:34:07,438.438.797950745:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name select-time2019-05-07 22:34:07,439.439.13602829:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name io-time2019-05-07 22:34:07,439.439.702033997:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name connections-closed2019-05-07 22:34:07,440.440.099954605:kafka.client:139872394159872:INFO:32192:Bootstrapping cluster metadata from [(&apos;172.18.0.2&apos;, 9092, 2)]2019-05-07 22:34:07,440.440.378904343:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name connections-created2019-05-07 22:34:07,440.440.948963165:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name select-time2019-05-07 22:34:07,440.440.732955933:kafka.client:139872394159872:DEBUG:32192:Attempting to bootstrap via node at 172.18.0.2:90922019-05-07 22:34:07,441.441.575050354:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name bytes-sent-received2019-05-07 22:34:07,441.441.804885864:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name bytes-sent2019-05-07 22:34:07,442.442.168951035:kafka.metrics.metrics:139872394159872:DEBUG:32192:Added sensor with name bytes-received2019-05-07 22:34:07,442.442.622900009:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name io-time2019-05-07 22:34:07,443.443.308115005:kafka.client:139872385505024:INFO:32192:Bootstrapping cluster metadata from [(&apos;172.18.0.2&apos;, 9092, 2)]2019-05-07 22:34:07,443.443.564891815:kafka.client:139872385505024:DEBUG:32192:Attempting to bootstrap via node at 172.18.0.2:90922019-05-07 22:34:07,444.444.015979767:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name bytes-sent-received2019-05-07 22:34:07,444.444.26202774:kafka.metrics.metrics:139872385505024:DEBUG:32192:Added sensor with name bytes-sent...... wireshark打开抓包文件宿主上执行以下命令，将容器中的抓包文件拷贝到共享文件夹下1docker cp dk-kafka-v2:/usr/local/capture.cap /home/share/ wireshark(3.0.0)打开该文件，内容如下会发现其中有很多SMPP类型的报文，这其实就是kafka协议报文，wireshark是支持kafka协议的(版本过低可能解析不了，因为kafka协议也是最新的) 选中一包SMPP数据，右击选择解码为 在弹出的对话框中将当前值选择为kafka 确认后，显示的协议类型就是kafka了 报文分析 抓包文件 capture.cap load_example.py运行日志 load_example_log.txt 建立连接可看到前6个报文分别是生产者和消费者与broker的tcp三次握手，生产者源端口号为38212，消费者源端口号为38214 metadata请求连接建立之后，生产者和消费者都向broker发起了metadata请求，报文结构如下图该请求中API Key为3，还包含了API Version、Correlation ID等字段 那么什么是metadata呢？由于kafka的broker是分布式集群，metadata记录了一些和集群有关的映射信息 broker.id与node的对应关系 topic与partition的对应关系 node与partition的对应关系 详细的metadata解释见Kafka 源码之生产者 Metadata 更新机制（二） broker接收到该请求后，首先会回一个ack，然后回复一个metadata响应，报文结构如图 返回的结构中，主要包含两大块信息：Broker Metadata和Topic Metadata。Broker Metadata中只有一个Broker，包含该broker的Node ID、Host和Port，因为本环境中只有一个broker，就是在容器中的broker。Topic Metadata共有3个，分别是my-topic、hellotopic和__consumer_offsets，因为load_example.py脚本创建的topic是my-topic，另外两个是做测试时创建的 每隔topic结构中包含该topic的名字和一个partition，什么是partition呢？一个topic可以有多个partition，消息会被发送到不同的partition，每个partition其实就是broker下的一个log文件，存放消息。多个partition会有一个leader，负责备份和写入，Replicas就是备份信息 ApiVersion请求猜测可能各个版本的api有些差异，因此要对齐api版本 请求报文结构如图，key为18 响应报文结构如图broker回返回自己支持的各部分api版本，包括Produce、Fetch、Offsets、Metadata、LeaderAndIsr、StopReplica等等 Offset请求消费者会先请求一次Offset，Offset记录了broker当前消费位置信息 请求报文结构如图，key为2该报文中包含了要请求的topic和partition 响应报文结构如图可以看出my-topic的partition1的offset为12705 Produce请求Porduce请求是生产者向broker发送的生产数据请求 请求报文如图，key为0其中Message Set Size字段告知了数据大小为15556，Message Set-&gt;Message是消息实体，包含了CRC校验头、Magic Byte等内容 响应报文如图返回的报文中包含一个offset Fetch请求Fetch请求是消费者向broker发出的请求，向broker申请消费数据 请求报文结构如图，key为1其中告知了请求的topic、partition、offset和最大字节数Max bytes 响应报文如图响应报文不是kafka协议报文，而是直接tcp流，将数据发过来 总结消息交过流程如图]]></content>
      <categories>
        <category>源码分析</category>
        <category>python-kafka</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Docker</tag>
        <tag>Kafka</tag>
        <tag>抓包</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker基本操作]]></title>
    <url>%2F2019%2F05%2F07%2Fdocker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[记录一些常用的docker命令 基本查看命令查看所有正在运行的容器 1docker ps 查看所有容器 1docker ps -a 查看所有镜像 1docker images 容器基本操作从镜像文件创建一个容器 12345docker run [options] image# -d 后台运行# -it ... /bin/bash 终端连接# --name 设置容器名称# --net=host/bridege... 设置网络 启动某个已存在的容器 1docker start id/name 停止某个容器 1docker stop id/name 删除某个容器 1docker rm id/name 停止所有容器 1docker stop $(docker ps -a -q) 删除所有容器 1docker rm $(docker ps -a -q) 连接容器 1docker exec -it id/name /bin/bash 保存容器的修改，提交到镜像 1docker commit id name 保存容器为文件 12docker export id &gt; name# docker export 3b86889153b0 &gt; dk-test.tar 加载容器文件 12docker import name newname# docker import dk-test.tar dk-test-v2 镜像操作加载镜像 1docker load file_name &lt; image_name 保存镜像到文件 1docker save id &gt; name 删除镜像 1docker rmi id]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用socketpair唤醒selector]]></title>
    <url>%2F2019%2F05%2F05%2F%E5%88%A9%E7%94%A8socketpair%E5%94%A4%E9%86%92selector%2F</url>
    <content type="text"><![CDATA[在分析kafka-python源码时发现生产者经常调用其子对象Client的一个wakeup()方法，它利用了socketpair管道发送一个数据来快速唤醒selector，快速中断selector的轮询。上下文关系如图所示程序运行中有两个线程，主线程’main loop’和客户端线程’client thread’。主线程定时利用client发送数据到server，而client线程本身要调用selector()函数以超时等待的方式轮询server的数据。当主线程要发送数据时，先调用一个wakeup()函数唤醒client，然后发送 wakeup函数代码如下123456789def wakeup(self): with self._wake_lock: try: self._wake_w.sendall(b'x') except socket.timeout: log.warning('Timeout to send to wakeup socket!') raise Errors.KafkaTimeoutError() except socket.error: log.warning('Unable to send to wakeup socket!') 其中_wake_lock是通过threading.Lock()创建的线程锁，_wake_w是由socket.socketpair()函数创建的socket管道写端 我猜测这样的用法是因为直接调用send()函数发送数据最终会调用client.send()，这里会请求lock，但是如果selector的轮训线程在等待超时之前已经申请了lock，导致send阻塞，无法发送数据，直到selector超时结束并释放了lock之后，send才能发送。wakeup的作用是让selector收到数据，并立刻释放lock，使得send可以立即申请到lock并发送数据 验证编写的测试脚本如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394import timeimport socketimport threadingtry: import selectors # Python2.7不支持selectors模块，找的源码except ImportError: from vendor import selectors34 as selectorsclass Poller(threading.Thread): def __init__(self): super(Poller, self).__init__() self._selector = selectors.DefaultSelector() self._wake_r, self._wake_w = socket.socketpair() # 利用socketpair创建唤醒消息通道 self._wake_r.setblocking(False) self._wake_w.settimeout(3) self._wake_lock = threading.Lock() self._lock = threading.RLock() self._selector.register(self._wake_r, selectors.EVENT_READ) self.stop_event = threading.Event() # 线程结束控制信号 def stop(self): # 控制线程结束方法 with self._lock: self.stop_event.set() def _clear_wake_fd(self): # 清理唤醒通道的数据 while True: try: self._wake_r.recv(1024) except socket.error: break def close(self): self._wake_r.close() self._wake_w.close() self._selector.close() def wakeup(self): # 唤醒方法 with self._wake_lock: try: self._wake_w.sendall(b'x') except socket.timeout: print 'Timeout to send to wakeup socket!' except socket.error: print 'Unable to send to wakeup socket!' def run(self): while not self.stop_event.is_set(): try: self.poll() except Exception as e: print 'Exception &#123;&#125;'.format(e) self.stop() try: self.close() except Exception as e: print e def poll(self): with self._lock: print 'selecting...' start_select = time.time() ready = self._selector.select(10) end_select = time.time() print 'select time &#123;&#125;'.format(end_select-start_select) for key, events in ready: if key.fileobj is self._wake_r: print 'is wakeup event' self._clear_wake_fd() continue elif not (events &amp; selectors.EVENT_READ): continue try: data = key.fileobj.recv(1024) print data except Exception as e: print e if __name__ == "__main__": p = Poller() p.start() time.sleep(60) # 注释这里以启用唤醒测试 # 注释这里以关闭唤醒测试 ''' count = 0 while count &lt; 5: time.sleep(3) p.wakeup() count += 1 ''' p.stop() 主循环定时每3秒调用wakeup方法唤醒线程selector，线程中select()函数结束后打印等待时间 测试结果不启用唤醒主函数代码为123456if __name__ == "__main__": p = Poller() p.start() time.sleep(60) # 注释这里以启用唤醒测试 p.stop() 运行程序，结果为123456789101112131415&gt;&gt; python selector_sockpair_wakeup.pyselecting...select time 10.0130000114selecting...select time 10.0150001049selecting...select time 10.004999876selecting...select time 10.0050001144selecting...select time 10.004999876selecting...select time 10.0090000629selecting...select time 10.0120000839 程序正常运行，每10秒select发生超时 启用唤醒主函数代码为123456789if __name__ == "__main__": p = Poller() p.start() count = 0 while count &lt; 5: time.sleep(3) p.wakeup() count += 1 p.stop() 运行程序，结果为123456789101112131415161718&gt;&gt; python selector_sockpair_wakeup.pyselecting...select time 3.01499986649is wakeup eventselecting...select time 3.0150001049is wakeup eventselecting...select time 3.0150001049is wakeup eventselecting...select time 3.00499987602is wakeup eventselecting...select time 3.00200009346is wakeup eventselecting...select time 10.003000021 由于主循环中调用了wakeup()方法，select收到数据，因此select时间是3秒]]></content>
      <categories>
        <category>源码分析</category>
        <category>python-kafka</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[引用、拷贝和垃圾回收]]></title>
    <url>%2F2019%2F04%2F29%2F%E5%BC%95%E7%94%A8%E3%80%81%E6%8B%B7%E8%B4%9D%E5%92%8C%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6%2F</url>
    <content type="text"><![CDATA[在分析kafka-python源码时发现有个回调函数的处理怎么看也看不懂，生产者创建了一个client对象实例，client有一个方法_conn_state_change()。当建立连接时，使用conn实例来描述和管理一个连接，conn需要用到client的_conn_state_change()方法，因此在创建conn实例时要传入该方法，大致的上下文逻辑如图代码如下123456cb = WeakMethod(self._conn_state_change)conn = BrokerConnection(host, broker.port, afi, state_change_callback=cb, node_id=node_id, **self.config)self._conns[node_id] = conn 为什么不能直接将该函数作为参数传入，而要通过另外一个类WeakMethod来封装一下，将返回的对象传入呢？ Weakmethod类的定义如下12345678910111213141516171819202122232425262728293031323334class WeakMethod(object): """ Callable that weakly references a method and the object it is bound to. It is based on https://stackoverflow.com/a/24287465. Arguments: object_dot_method: A bound instance method (i.e. 'object.method'). """ def __init__(self, object_dot_method): try: self.target = weakref.ref(object_dot_method.__self__) except AttributeError: self.target = weakref.ref(object_dot_method.im_self) self._target_id = id(self.target()) try: self.method = weakref.ref(object_dot_method.__func__) except AttributeError: self.method = weakref.ref(object_dot_method.im_func) self._method_id = id(self.method()) def __call__(self, *args, **kwargs): """ Calls the method on target with args and kwargs. """ return self.method()(self.target(), *args, **kwargs) def __hash__(self): return hash(self.target) ^ hash(self.method) def __eq__(self, other): if not isinstance(other, WeakMethod): return False return self._target_id == other._target_id and self._method_id == other._method_id 发现WeakMethod的定义中使用了weakref，通过查阅资料，才了解到这是python的弱引用。又经过一番查资料和写测试代码，才对python中的引用、拷贝和垃圾回收有了一定的理解 引用式变量首先必须明确的一点是，类似Python或者Java语言中，变量只是一个标注，而不是对象本身。以一个例子来证明12345&gt;&gt;&gt; a = [1, 2, 3]&gt;&gt;&gt; b = a&gt;&gt;&gt; a.append(4)&gt;&gt;&gt; b[1, 2, 3, 4] 对列表a的改动，结果b也发生了同样的改动，原因是a、b都只是同一个列表的引用，对引用的操作实质上是对对象本身的操作，如下图 默认做浅拷贝这里有一个将代码运行转换成交互式动画的网站Python Tutor可以方便查看代码运行过程中到底发生了什么 Python默认的拷贝方式是浅拷贝，即只复制最外层容器，以下例子用来说明12345678910&gt;&gt;&gt; l1 = [3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l2 = list(l1)&gt;&gt;&gt; l2[3, [55, 44], (7, 8, 9)]&gt;&gt;&gt; l1.append(100)&gt;&gt;&gt; l1[1].remove(55) &gt;&gt;&gt; print('l1:', l1)('l1:', [3, [44], (7, 8, 9), 100])&gt;&gt;&gt; print('l2:', l2)('l2:', [3, [44], (7, 8, 9)]) l2是l1的浅拷贝，当直接对l1进行改变时(插入100)，l2不会发生变化。但是对l1的第2个成员[55, 44]删除元素是，其变化会影响到l2，原理如图 深拷贝如果要做深拷贝，使用copy模块的deepcopy()方法 弱引用如果对引用操作不当，会导致对象无法被回收，内存泄漏，例子如下1234567891011121314class ReferenceTest(object): def __init__(self, name): print 'Object &#123;&#125; created'.format(self.name) def __del__(self): print 'Object &#123;&#125; destroy'.format(self.name)def ref_test(): a = ReferenceTest('obj_a') b = ReferenceTest('obj_b') a.b = b b.a = aif __name__ == "__main__": ref_test() 运行以上代码，结果是这样的123&gt;&gt;&gt; python referenct_test.pyObject obj_a createdObject obj_b created 正常来说，创建ReferenceTest类的两个实例a、b，当程序运行结束时，它们的引用计数变为0，会被Python的垃圾回收器自动销毁，而在销毁时会调用类的__del__方法，打印函数中定义的内容，而结果却没有打印。 因为这两个实例互相引用，其引用次数不可能为0，导致无法被回收。要解决该问题，就需要使用弱引用 weakrefPython提供weakref模块来处理弱引用，修改以上代码12345678910111213141516import weakrefclass ReferenceTest(object): def __init__(self, name): print 'Object &#123;&#125; created'.format(self.name) def __del__(self): print 'Object &#123;&#125; destroy'.format(self.name)def ref_test(): a = ReferenceTest('obj_a') b = ReferenceTest('obj_b') a.b = weakref.proxy(b) b.a = weakref.proxy(a)if __name__ == "__main__": ref_test() 运行以上代码，结果是这样的12345&gt;&gt;&gt; python referenct_test.pyObject obj_a createdObject obj_b createdObject obj_a destroyObject obj_b destroy 由于垃圾回收器在只有弱引用时也会将对象清理，因此obj_a和obj_b都被清理了 WeakMethod回过头来看WeakMethod的用法，如果直接将函数_conn_state_change()传入conn中，而conn的返回实例又会被client._conns引用，形成了引用循环，因此需要创建弱引用对象，将弱引用对象传入到conn中]]></content>
      <categories>
        <category>源码分析</category>
        <category>python-kafka</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Kafka</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[加密测试]]></title>
    <url>%2F2019%2F04%2F28%2F%E5%8A%A0%E5%AF%86%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[加密测试]]></content>
      <tags>
        <tag>加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[我的blog发布啦]]></title>
    <url>%2F2019%2F04%2F28%2F%E6%88%91%E7%9A%84blog%E5%8F%91%E5%B8%83%E5%95%A6%2F</url>
    <content type="text"><![CDATA[感谢创造了github、leancloud、hexo等等优秀的开发者们，让穷困潦倒&amp;技艺拙略的我也能非常方便的搭建起一个blog，拥有一个看起来不是那么low的场所，去记录一些公开化的、属于我的东西，最重要的是：它是完完全全免费的！！！ I. 为什么要有一个自己的blog？目前我主要使用github和gitlab来做记录。github不用说，这是属于developer的世界，我的github侧重于开源project，另外也会用WiKi归类整理一些自有技术文档；gitlab和github非常相似，我觉得它的issue、milestone、labels等功能在使用和界面交互上更加舒适，我喜欢用它来记录一些生活中计划性的事情和一些专项记录，例如装修计划、病例 它们的确很好用，有着良好的设计性和交互性，支持markdown使得文档的排版看起来非常舒服，特别是使用gitlab来做计划管理，用它的milestone来管理deadline和进度，用issue来记录问题点、交换意见和讨论。但是它们都太专业化，侧重点非常明确，且不够个性化，拿它们来做项目管理和技术文档整理再好不过，却不再适合别的什么了 在查阅一些内容的时候，常常会搜索到某位hacker的blog，他们的blog有着简洁优雅的排版，里面的内容除了技术文档，更涵盖了个人生活的方方面面，旅行、摄影、美食烹饪、随笔、散文。我也希望能够有一个展现完整个人的场所，记录自己的生活、动态和思想 II. 记录些什么？目前我能够想到的，想要去记录的内容，主要是这样几点 开发 阅读 思考 兴趣 开发记录一些成体系的知识结构，主要是嵌入式和机器学习两个方向 阅读此处的阅读是广义的阅读，包含了读书、追剧、电影等内容，写一些阶段性的感受、影评或是书评 思考记录好的想法、idea，对一些感兴趣领誉的思索，如历史、经济、社会、文化、科技等等 兴趣这里是作品发布、学习干货的聚集地，放置所有我视之为兴趣的东西，可以是Dota回忆录、摄影作品展、插画展、游记、家装设计、乐器学习等]]></content>
      <categories>
        <category>其他</category>
      </categories>
  </entry>
</search>
